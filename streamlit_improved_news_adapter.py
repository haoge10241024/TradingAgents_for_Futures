#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Streamlitæ”¹è¿›ç‰ˆæ–°é—»åˆ†æé€‚é…å™¨
å°†æ”¹è¿›ç‰ˆæ–°é—»åˆ†æç³»ç»Ÿé›†æˆåˆ°Streamlitç³»ç»Ÿä¸­
æ”¯æŒä¸“ä¸šåˆ†ææŠ¥å‘Šï¼Œæ–°é—»é“¾æ¥æ ‡æ³¨ï¼Œæ— Markdownç¬¦å·
"""

import sys
from pathlib import Path
from typing import Dict, Any, Optional
import asyncio
import concurrent.futures
from datetime import datetime
import os

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
try:
    project_root = Path(__file__).parent
except NameError:
    project_root = Path.cwd()

if str(project_root) not in sys.path:
    sys.path.insert(0, str(project_root))

# å¯¼å…¥ä¸“ä¸šæŠ¥å‘Šç‰ˆæ–°é—»åˆ†æç³»ç»Ÿ
try:
    from æœŸè´§æ–°é—»AIåˆ†æç³»ç»Ÿ_ä¸“ä¸šæŠ¥å‘Šç‰ˆ import ProfessionalFuturesNewsAnalyzer
    from æœŸè´§æ–°é—»AIåˆ†æç³»ç»Ÿ_ä¸“ä¸šæŠ¥å‘Šç‰ˆ import install_and_import
except ImportError as e:
    print(f"å¯¼å…¥é”™è¯¯: {e}")
    # å¤‡ç”¨å¯¼å…¥
    class ProfessionalFuturesNewsAnalyzer:
        def __init__(self, deepseek_api_key=None):
            pass
        def comprehensive_news_search(self, commodity, target_date, days_back):
            return []
        def analyze_comprehensive_news_professional(self, akshare_df, search_news, news_citations, commodity, target_date):
            return "å¯¼å…¥å¤±è´¥", []
    def install_and_import():
        import pandas as pd
        return None, pd, None, None, None, None

class StreamlitImprovedNewsAdapter:
    """Streamlitæ”¹è¿›ç‰ˆæ–°é—»åˆ†æé€‚é…å™¨"""
    
    def __init__(self, deepseek_api_key: str = None):
        """åˆå§‹åŒ–é€‚é…å™¨"""
        try:
            # ä½¿ç”¨é»˜è®¤APIå¯†é’¥å¦‚æœæœªæä¾›
            api_key = deepseek_api_key or os.getenv('DEEPSEEK_API_KEY', 'sk-default-key')
            self.analyzer = ProfessionalFuturesNewsAnalyzer(api_key)
            self.initialized = True
            print("âœ… ä¸“ä¸šæŠ¥å‘Šç‰ˆæ–°é—»åˆ†æé€‚é…å™¨åˆå§‹åŒ–æˆåŠŸ")
        except Exception as e:
            print(f"âŒ é€‚é…å™¨åˆå§‹åŒ–å¤±è´¥: {e}")
            self.analyzer = None
            self.initialized = False
    
    def analyze_news_for_streamlit(self, commodity: str, analysis_date: str = None, 
                                 days_back: int = 7) -> Dict[str, Any]:
        """
        ä¸ºStreamlitç³»ç»Ÿæä¾›æ”¹è¿›ç‰ˆæ–°é—»åˆ†æ
        
        Args:
            commodity: å“ç§åç§°ï¼ˆä¸­æ–‡ï¼‰
            analysis_date: åˆ†ææ—¥æœŸ
            days_back: æœç´¢å¤©æ•°
            
        Returns:
            Dict: åˆ†æç»“æœï¼Œå…¼å®¹Streamlitæ˜¾ç¤ºæ ¼å¼
        """
        if not self.initialized or self.analyzer is None:
            return {
                "success": False,
                "error": "æ–°é—»åˆ†æç³»ç»Ÿæœªæ­£ç¡®åˆå§‹åŒ–",
                "commodity": commodity
            }
        
        try:
            print(f"ğŸ” å¼€å§‹åˆ†æ{commodity}æ–°é—»...")
            
            # å¯¼å…¥å¿…è¦åº“
            ak, pd, requests, BeautifulSoup, feedparser, date_parser = install_and_import()
            
            # è§£æåˆ†ææ—¥æœŸ
            if analysis_date:
                try:
                    if isinstance(analysis_date, str):
                        if len(analysis_date) == 8 and analysis_date.isdigit():
                            target_date = datetime.strptime(analysis_date, "%Y%m%d").date()
                        else:
                            target_date = datetime.strptime(analysis_date, "%Y-%m-%d").date()
                    else:
                        target_date = analysis_date
                except:
                    target_date = datetime.now().date()
            else:
                target_date = datetime.now().date()
            
            # 1. è·å–akshareæ•°æ®
            akshare_news, akshare_total, akshare_citations = self.analyzer.get_akshare_news(
                commodity, target_date, days_back, ak
            )
            
            # 2. ç»¼åˆæœç´¢æ–°é—»
            search_news = self.analyzer.comprehensive_news_search(commodity, target_date, days_back)
            
            # åˆå¹¶æ‰€æœ‰å¼•ç”¨
            all_citations = akshare_citations.copy()
            for news in search_news:
                citation = {
                    'title': news['title'],
                    'source': news['source'],
                    'date': news['date'],
                    'url': news['url'],
                    'type': news.get('type', 'search')
                }
                all_citations.append(citation)
            
            total_news = len(akshare_news) + len(search_news) if not akshare_news.empty else len(search_news)
            
            if total_news == 0:
                return {
                    "success": False,
                    "error": f"æœªæ‰¾åˆ°{commodity}ç›¸å…³æ–°é—»",
                    "commodity": commodity,
                    "news_count": 0
                }
            
            # 3. ç”Ÿæˆä¸“ä¸šåˆ†æ
            analysis_result, final_citations = self.analyzer.analyze_comprehensive_news_professional(
                akshare_news, search_news, all_citations, commodity, target_date
            )
            
            if "âŒ" in analysis_result:
                print(f"âŒ {commodity} æ–°é—»åˆ†æå¤±è´¥: {analysis_result}")
                return {
                    "success": False,
                    "error": analysis_result,
                    "commodity": commodity
                }
            
            # æ„å»ºæˆåŠŸç»“æœ - æ”¯æŒä¸­æ–‡åç§°å’Œè‹±æ–‡ä»£ç 
            if commodity in self.analyzer.all_commodities:
                config = self.analyzer.all_commodities[commodity]
            else:
                # å°è¯•é€šè¿‡è‹±æ–‡ä»£ç æŸ¥æ‰¾
                config = None
                for chinese_name, cfg in self.analyzer.all_commodities.items():
                    if cfg.get("symbol") == commodity:
                        config = cfg
                        break

                if not config:
                    return {
                        "success": False,
                        "error": f"æœªæ‰¾åˆ°å“ç§é…ç½®: {commodity}",
                        "commodity": commodity
                    }

            result = {
                "success": True,
                "commodity": commodity,
                "symbol": config['symbol'],
                "exchange": config['exchange'],
                "category": config['category'],
                "analysis_date": target_date.strftime('%Y-%m-%d'),
                "analysis_content": analysis_result,
                "news_sources": final_citations,
                "news_count": total_news,
                "akshare_count": len(akshare_news) if not akshare_news.empty else 0,
                "search_count": len(search_news),
                "data_quality_score": min(0.95, total_news / 20),  # ç®€å•è´¨é‡è¯„åˆ†
                "analysis_version": "v4.0_professional"
            }
            
            print(f"âœ… {commodity} æ–°é—»åˆ†ææˆåŠŸ")
            return self._format_for_streamlit(result)
                
        except Exception as e:
            print(f"âŒ æ–°é—»åˆ†æå¼‚å¸¸: {e}")
            return {
                "success": False,
                "error": f"æ–°é—»åˆ†æè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}",
                "commodity": commodity
            }
    
    def _format_for_streamlit(self, result: Dict[str, Any]) -> Dict[str, Any]:
        """æ ¼å¼åŒ–ç»“æœä»¥é€‚é…Streamlitæ˜¾ç¤º"""
        
        # åŸºç¡€ä¿¡æ¯
        formatted_result = {
            "success": True,
            "commodity": result.get("commodity", ""),
            "symbol": result.get("symbol", ""),
            "exchange": result.get("exchange", ""),
            "category": result.get("category", ""),
            "analysis_date": result.get("analysis_date", ""),
            "news_count": result.get("news_count", 0),
            "data_quality_score": result.get("data_quality_score", 0.0),
            "analysis_version": result.get("analysis_version", "v2.0_improved")
        }
        
        # AIåˆ†æå†…å®¹ï¼ˆå·²æ¸…ç†Markdownç¬¦å·ï¼‰
        if "analysis_content" in result:
            formatted_result["ai_analysis"] = result["analysis_content"]
        
        # æ–°é—»æ¥æºå’Œå¼•ç”¨
        if "news_sources" in result:
            formatted_result["citations"] = self._format_citations(result["news_sources"])
        
        # æ·»åŠ akshareå’Œæœç´¢æ•°æ®ç»Ÿè®¡
        formatted_result["akshare_count"] = result.get("akshare_count", 0)
        formatted_result["search_count"] = result.get("search_count", 0)
        
        # åˆ†æå…ƒæ•°æ®
        formatted_result["metadata"] = {
            "analysis_timestamp": result.get("analysis_timestamp", ""),
            "data_source": "improved_news_analysis",
            "model_used": "deepseek-chat",
            "search_engines": self._extract_search_engines(result.get("news_sources", [])),
            "professional_sites_count": self._count_professional_sites(result.get("news_sources", []))
        }
        
        # ç”Ÿæˆç»“æ„åŒ–åˆ†ææ‘˜è¦
        formatted_result["structured_analysis"] = self._generate_structured_summary(result)
        
        return formatted_result
    
    def _format_citations(self, news_sources: list) -> list:
        """æ ¼å¼åŒ–å¼•ç”¨åˆ—è¡¨"""
        citations = []
        
        for i, source in enumerate(news_sources, 1):
            citation = {
                "index": i,
                "title": source.get("title", "æœªçŸ¥æ ‡é¢˜"),
                "source": source.get("source", "æœªçŸ¥æ¥æº"),
                "url": source.get("url", ""),
                "date": source.get("date", "æœªçŸ¥æ—¥æœŸ"),
                "relevance_score": source.get("relevance", 0),  # æ³¨æ„è¿™é‡Œæ˜¯relevanceè€Œä¸æ˜¯relevance_score
                "search_engine": source.get("type", "æœªçŸ¥ç±»å‹")
            }
            citations.append(citation)
        
        return citations
    
    def _extract_search_engines(self, news_sources: list) -> list:
        """æå–ä½¿ç”¨çš„æœç´¢å¼•æ“"""
        engines = set()
        for source in news_sources:
            engine = source.get("search_engine", "")
            if engine:
                engines.add(engine)
        return list(engines)
    
    def _count_professional_sites(self, news_sources: list) -> int:
        """ç»Ÿè®¡ä¸“ä¸šç½‘ç«™æ•°é‡"""
        professional_count = 0
        for source in news_sources:
            search_engine = source.get("search_engine", "")
            if "ä¸“ä¸šç½‘ç«™æœç´¢" in search_engine:
                professional_count += 1
        return professional_count
    
    def _generate_structured_summary(self, result: Dict[str, Any]) -> Dict:
        """ç”Ÿæˆç»“æ„åŒ–åˆ†ææ‘˜è¦"""
        analysis_content = result.get("analysis_content", "")
        
        summary = {
            "analysis_quality": {
                "content_length": len(analysis_content),
                "data_quality": result.get("data_quality_score", 0.0),
                "news_coverage": result.get("news_count", 0),
                "professional_level": "é«˜" if len(analysis_content) > 2000 else "ä¸­" if len(analysis_content) > 1000 else "ä½"
            },
            "key_insights": self._extract_key_insights(analysis_content),
            "risk_factors": self._extract_risk_factors(analysis_content),
            "investment_suggestions": self._extract_investment_suggestions(analysis_content)
        }
        
        return summary
    
    def _extract_key_insights(self, content: str) -> list:
        """æå–å…³é”®æ´å¯Ÿ"""
        insights = []
        
        # ç®€å•çš„å…³é”®è¯æå–é€»è¾‘
        key_phrases = [
            "æ ¸å¿ƒè§‚ç‚¹", "ä¸»è¦é©±åŠ¨", "å…³é”®å› ç´ ", "é‡è¦å˜åŒ–", "æ˜¾è‘—å½±å“",
            "æ ¸å¿ƒé€»è¾‘", "ä¸»è¦åŸå› ", "å…³é”®æ•°æ®", "é‡è¦ä¿¡å·", "æ ¸å¿ƒæ”¯æ’‘"
        ]
        
        lines = content.split('\n')
        for line in lines:
            line = line.strip()
            if any(phrase in line for phrase in key_phrases) and len(line) > 20:
                insights.append(line[:200])  # é™åˆ¶é•¿åº¦
                if len(insights) >= 5:  # æœ€å¤š5ä¸ªæ´å¯Ÿ
                    break
        
        return insights
    
    def _extract_risk_factors(self, content: str) -> list:
        """æå–é£é™©å› ç´ """
        risks = []
        
        risk_keywords = [
            "é£é™©", "ä¸ç¡®å®š", "å‹åŠ›", "ä¸‹è¡Œ", "å¨èƒ", "æŒ‘æˆ˜",
            "è­¦æƒ•", "å…³æ³¨", "è°¨æ…", "æ³¢åŠ¨", "å†²å‡»"
        ]
        
        lines = content.split('\n')
        for line in lines:
            line = line.strip()
            if any(keyword in line for keyword in risk_keywords) and len(line) > 20:
                risks.append(line[:200])
                if len(risks) >= 3:  # æœ€å¤š3ä¸ªé£é™©
                    break
        
        return risks
    
    def _extract_investment_suggestions(self, content: str) -> list:
        """æå–æŠ•èµ„å»ºè®®"""
        suggestions = []
        
        suggestion_keywords = [
            "å»ºè®®", "ç­–ç•¥", "æ“ä½œ", "é…ç½®", "å¸ƒå±€", "å…³æ³¨",
            "ä¹°å…¥", "å–å‡º", "æŒæœ‰", "å‡ä»“", "åŠ ä»“", "è§‚æœ›"
        ]
        
        lines = content.split('\n')
        for line in lines:
            line = line.strip()
            if any(keyword in line for keyword in suggestion_keywords) and len(line) > 20:
                suggestions.append(line[:200])
                if len(suggestions) >= 3:  # æœ€å¤š3ä¸ªå»ºè®®
                    break
        
        return suggestions

# åˆ›å»ºå…¨å±€é€‚é…å™¨å®ä¾‹
_improved_news_adapter = None

def get_improved_news_adapter(deepseek_api_key: str):
    """è·å–æ”¹è¿›ç‰ˆæ–°é—»åˆ†æé€‚é…å™¨å®ä¾‹"""
    global _improved_news_adapter
    if _improved_news_adapter is None:
        _improved_news_adapter = StreamlitImprovedNewsAdapter(deepseek_api_key)
    return _improved_news_adapter

def analyze_improved_news_for_streamlit(commodity: str, deepseek_api_key: str,
                                      analysis_date: str = None, days_back: int = 7) -> Dict[str, Any]:
    """
    Streamlitç³»ç»Ÿè°ƒç”¨çš„æ”¹è¿›ç‰ˆæ–°é—»åˆ†ææ¥å£
    
    Args:
        commodity: å“ç§åç§°
        deepseek_api_key: DeepSeek APIå¯†é’¥
        analysis_date: åˆ†ææ—¥æœŸ
        days_back: æœç´¢å¤©æ•°
        
    Returns:
        Dict: æ ¼å¼åŒ–çš„åˆ†æç»“æœ
    """
    adapter = get_improved_news_adapter(deepseek_api_key)
    return adapter.analyze_news_for_streamlit(commodity, analysis_date, days_back)

# æµ‹è¯•å‡½æ•°
if __name__ == "__main__":
    print("ğŸ§ª æµ‹è¯•Streamlitæ”¹è¿›ç‰ˆæ–°é—»åˆ†æé€‚é…å™¨")
    print("=" * 60)
    
    # æµ‹è¯•åˆ†æ
    test_commodity = "èºçº¹é’¢"
    test_api_key = "your_deepseek_api_key_here"
    
    if test_api_key == "your_deepseek_api_key_here":
        print("âŒ è¯·å…ˆé…ç½®DeepSeek APIå¯†é’¥è¿›è¡Œæµ‹è¯•")
    else:
        print(f"ğŸ“Š æµ‹è¯•åˆ†æå“ç§: {test_commodity}")
        
        result = analyze_improved_news_for_streamlit(
            commodity=test_commodity,
            deepseek_api_key=test_api_key,
            days_back=7
        )
        
        if result.get("success"):
            print("âœ… æµ‹è¯•æˆåŠŸ!")
            print(f"ğŸ“ˆ å“ç§: {result['commodity']} ({result['symbol']})")
            print(f"ğŸ“… åˆ†ææ—¥æœŸ: {result['analysis_date']}")
            print(f"ğŸ“Š æ–°é—»æ•°é‡: {result['news_count']}")
            print(f"ğŸ¯ æ•°æ®è´¨é‡: {result['data_quality_score']:.1%}")
            
            if "ai_analysis" in result:
                analysis_length = len(result['ai_analysis'])
                print(f"ğŸ¤– AIåˆ†æ: {analysis_length} å­—ç¬¦")
                
                # æ£€æŸ¥æ˜¯å¦åŒ…å«Markdownç¬¦å·
                markdown_symbols = ['##', '**', '*', '```', '- ', '> ']
                has_markdown = any(symbol in result['ai_analysis'] for symbol in markdown_symbols)
                if has_markdown:
                    print("âš ï¸ æ£€æµ‹åˆ°Markdownç¬¦å·ï¼Œéœ€è¦è¿›ä¸€æ­¥æ¸…ç†")
                else:
                    print("âœ… åˆ†æå†…å®¹æ— Markdownç¬¦å·")
            
            if "citations" in result:
                print(f"ğŸ”— æ–°é—»å¼•ç”¨: {len(result['citations'])} æ¡")
                
                # æ˜¾ç¤ºå‰3ä¸ªå¼•ç”¨ç¤ºä¾‹
                for i, citation in enumerate(result['citations'][:3], 1):
                    print(f"  [{i}] {citation['title'][:50]}...")
                    print(f"      æ¥æº: {citation['source']}")
                    print(f"      é“¾æ¥: {citation['url'][:50]}...")
                    
        else:
            print(f"âŒ æµ‹è¯•å¤±è´¥: {result.get('error', 'æœªçŸ¥é”™è¯¯')}")
    
    print("\nğŸ‰ æµ‹è¯•å®Œæˆ!")
