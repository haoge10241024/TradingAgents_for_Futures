# ============================================================================
# ä¸“ä¸šæœŸè´§æŒä»“æ•°æ®AIåˆ†æç³»ç»Ÿ
# å®Œå…¨åŸºäºAIçš„ä¸“ä¸šåˆ†ææ¡†æ¶ - æ•°æ®å‡†å¤‡ + AIä¸“ä¸šåˆ†æ
# ============================================================================

import pandas as pd
import numpy as np
from pathlib import Path
from datetime import datetime, timedelta
from typing import Dict, List, Tuple, Optional, Any
import json
import warnings
from dataclasses import dataclass, asdict
import httpx
import time
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import plotly.express as px

warnings.filterwarnings('ignore')
plt.rcParams['font.sans-serif'] = ['SimHei']  # æ”¯æŒä¸­æ–‡æ˜¾ç¤º
plt.rcParams['axes.unicode_minus'] = False

print("ğŸš€ ä¸“ä¸šæœŸè´§æŒä»“æ•°æ®AIåˆ†æç³»ç»Ÿ")
print("=" * 80)

# ============================================================================
# 1. é…ç½®å’Œæ•°æ®ç»“æ„
# ============================================================================

# åŸºç¡€é…ç½®
BASE_DIR = Path(r"D:\Cursor\cursoré¡¹ç›®\TradingAgent")
POSITIONING_ROOT = BASE_DIR / "qihuo" / "database" / "positioning"
OUTPUT_DIR = BASE_DIR / "qihuo" / "output"

# DeepSeek APIé…ç½®
# è¯·åœ¨é…ç½®æ–‡ä»¶ä¸­è®¾ç½®APIå¯†é’¥ï¼Œæˆ–é€šè¿‡ç¯å¢ƒå˜é‡ DEEPSEEK_API_KEY
DEEPSEEK_API_KEY = os.getenv("DEEPSEEK_API_KEY", "YOUR_DEEPSEEK_API_KEY_HERE")
DEEPSEEK_MODEL = "deepseek-chat"
DEEPSEEK_BASE_URL = "https://api.deepseek.com/v1/chat/completions"

@dataclass
class EnhancedPositionMetrics:
    """å¢å¼ºç‰ˆæŒä»“æŒ‡æ ‡æ•°æ®ç»“æ„ - ä¸ºAIåˆ†æå‡†å¤‡çš„å®Œæ•´æ•°æ®"""
    date: str
    symbol: str
    
    # åŸºç¡€æŒä»“æ•°æ®
    long_top5_total: int = 0
    long_top10_total: int = 0
    long_top20_total: int = 0
    short_top5_total: int = 0
    short_top10_total: int = 0
    short_top20_total: int = 0
    volume_top20_total: int = 0
    
    # æŒä»“å˜åŒ–æ•°æ®
    long_top5_change: int = 0
    long_top10_change: int = 0
    long_top20_change: int = 0
    short_top5_change: int = 0
    short_top10_change: int = 0
    short_top20_change: int = 0
    
    # èœ˜è››ç½‘ç­–ç•¥æ•°æ®
    spider_web_dB: int = 0  # å¤šå¤´å˜åŠ¨
    spider_web_dS: int = 0  # ç©ºå¤´å˜åŠ¨
    spider_web_signal_strength: float = 0.0  # ä¿¡å·å¼ºåº¦
    
    # èªæ˜é’±æŒ‡æ ‡
    oi_volume_ratio: float = 0.0  # æŒä»“/æˆäº¤æ¯”
    position_efficiency: float = 0.0  # æŒä»“æ•ˆç‡
    smart_money_score: float = 0.0  # èªæ˜é’±è¯„åˆ†
    
    # é›†ä¸­åº¦æŒ‡æ ‡
    long_concentration: float = 0.0  # å¤šå•é›†ä¸­åº¦(HHI)
    short_concentration: float = 0.0  # ç©ºå•é›†ä¸­åº¦(HHI)
    volume_concentration: float = 0.0  # æˆäº¤é›†ä¸­åº¦
    concentration_differential: float = 0.0  # é›†ä¸­åº¦å·®å¼‚
    
    # å¸­ä½è¡Œä¸ºæ•°æ®
    retail_net_change: int = 0  # æ•£æˆ·å‡€å˜åŒ–
    institutional_net_change: int = 0  # æœºæ„å‡€å˜åŒ–
    foreign_net_change: int = 0  # å¤–èµ„å‡€å˜åŒ–
    hedging_net_change: int = 0  # å¥—ä¿å‡€å˜åŒ–
    retail_vs_smart_money: int = 0  # æ•£æˆ·vsèªæ˜é’±å¯¹æ¯”
    
    # æ´¾ç”ŸæŠ€æœ¯æŒ‡æ ‡
    net_position_top20: int = 0
    net_change_top20: int = 0
    long_short_ratio: float = 0.0
    position_momentum: float = 0.0  # æŒä»“åŠ¨é‡
    volatility_index: float = 0.0  # æ³¢åŠ¨ç‡æŒ‡æ•°
    
    # è¶‹åŠ¿æŒ‡æ ‡(éœ€è¦å†å²æ•°æ®è®¡ç®—)
    net_change_trend_5d: float = 0.0  # 5æ—¥å‡€å˜åŒ–è¶‹åŠ¿
    position_trend_5d: float = 0.0  # 5æ—¥å‡€æŒä»“è¶‹åŠ¿
    concentration_trend_5d: float = 0.0  # 5æ—¥é›†ä¸­åº¦è¶‹åŠ¿
    
    # å¸‚åœºæƒ…ç»ªæŒ‡æ ‡
    market_sentiment_score: float = 0.0  # å¸‚åœºæƒ…ç»ªè¯„åˆ†
    consensus_level: float = 0.0  # å…±è¯†åº¦

# ============================================================================
# 2. å¸­ä½åˆ†ç±»ç³»ç»Ÿ
# ============================================================================

class SeatClassifier:
    """ä¸“ä¸šå¸­ä½åˆ†ç±»å™¨"""
    
    def __init__(self):
        self.seat_categories = {
            'retail': [
                "ä¸œæ–¹è´¢å¯Œ", "å¹³å®‰æœŸè´§", "å¾½å•†æœŸè´§", "åå®‰æœŸè´§", 
                "ç”³é“¶ä¸‡å›½", "å¹¿å‘æœŸè´§", "æ‹›å•†æœŸè´§", "å…‰å¤§æœŸè´§",
                "å›½ä¿¡æœŸè´§", "ä¸­æŠ•æœŸè´§", "æµ·é€šæœŸè´§", "å…´ä¸šæœŸè´§",
                "ä¸œå´æœŸè´§", "æ¸¤æµ·æœŸè´§", "ä¸­é‡‘è´¢å¯Œ"
            ],
            
            'top_institutional': [
                "ä¸­ä¿¡æœŸè´§", "å›½æ³°å›å®‰", "åæ³°æœŸè´§", "ä¸­é‡‘æœŸè´§",
                "é“¶æ²³æœŸè´§", "ä¸œè¯æœŸè´§", "æ°¸å®‰æœŸè´§", "å—åæœŸè´§",
                "ä¸­ä¿¡å»ºæŠ•", "ç”³ä¸‡æœŸè´§"
            ],
            
            'institutional': [
                "ä¸­ç²®æœŸè´§", "ç‰©äº§ä¸­å¤§", "æµ™å•†æœŸè´§", "ä¸­æ³°æœŸè´§", 
                "å›½æŠ•æœŸè´§", "æ–¹æ­£ä¸­æœŸ", "ä¸€å¾·æœŸè´§", "å®æºæœŸè´§", 
                "å¼˜ä¸šæœŸè´§", "ç‘è¾¾æœŸè´§", "å®åŸæœŸè´§", "äº”çŸ¿æœŸè´§"
            ],
            
            'foreign': [
                "æ‘©æ ¹å¤§é€š", "é«˜ç››", "ç‘é“¶", "èŠ±æ——", "å¾·æ„å¿—é“¶è¡Œ",
                "æ³•å›½å…´ä¸šé“¶è¡Œ", "å·´å…‹è±", "æ±‡ä¸°é“¶è¡Œ", "æ˜Ÿå±•é“¶è¡Œ",
                "æ‘©æ ¹å£«ä¸¹åˆ©", "é‡æ‘è¯åˆ¸", "ç‘å£«ä¿¡è´·"
            ],
            
            'hedging': [
                "ä¸­å‚¨ç²®", "ä¸­ç²®é›†å›¢", "å˜‰å‰", "è·¯æ˜“è¾¾å­š", "ADM",
                "å®é’¢", "æ²³é’¢", "æ²™é’¢", "å»ºé¾™", "ä¸­é“", "äº”çŸ¿",
                "ä¸­çŸ³åŒ–", "ä¸­çŸ³æ²¹", "ä¸­æµ·æ²¹", "ä¸­åŒ–é›†å›¢", "ä¸­è¾‰æœŸè´§"
            ]
        }
    
    def classify_seat(self, seat_name: str) -> str:
        """å¸­ä½åˆ†ç±»"""
        for category, seats in self.seat_categories.items():
            if any(seat in seat_name for seat in seats):
                return category
        return 'unknown'

# ============================================================================
# 3. æ•°æ®å¤„ç†å™¨ - ä¸ºAIå‡†å¤‡ç»“æ„åŒ–æ•°æ®
# ============================================================================

class ProfessionalDataProcessor:
    """ä¸“ä¸šæ•°æ®å¤„ç†å™¨ - ä¸ºAIåˆ†æå‡†å¤‡å®Œæ•´çš„ç»“æ„åŒ–æ•°æ®"""
    
    def __init__(self):
        self.top_n_seats = 20
        self.classifier = SeatClassifier()
    
    def prepare_comprehensive_data(self, symbol: str, days_back: int = 30) -> List[EnhancedPositionMetrics]:
        """ä¸ºAIåˆ†æå‡†å¤‡å®Œæ•´çš„ç»“æ„åŒ–æ•°æ®"""
        symbol_dir = POSITIONING_ROOT / symbol
        
        if not symbol_dir.exists():
            print(f"âš ï¸ {symbol}: æ•°æ®ç›®å½•ä¸å­˜åœ¨")
            return []
        
        # è¯»å–ä¸‰ä¸ªæ–‡ä»¶
        long_file = symbol_dir / "long_position_ranking.csv"
        short_file = symbol_dir / "short_position_ranking.csv" 
        volume_file = symbol_dir / "volume_ranking.csv"
        
        if not all(f.exists() for f in [long_file, short_file, volume_file]):
            print(f"âš ï¸ {symbol}: æ•°æ®æ–‡ä»¶ä¸å®Œæ•´")
            return []
        
        long_df = pd.read_csv(long_file)
        short_df = pd.read_csv(short_file)
        volume_df = pd.read_csv(volume_file)
        
        metrics_list = []
        
        # æŒ‰æ—¥æœŸåˆ†ç»„å¤„ç†
        dates = sorted(long_df['date'].unique())[-days_back:]
        
        for i, date in enumerate(dates):
            long_day = long_df[long_df['date'] == date].head(self.top_n_seats)
            short_day = short_df[short_df['date'] == date].head(self.top_n_seats)
            volume_day = volume_df[volume_df['date'] == date].head(self.top_n_seats)
            
            if long_day.empty or short_day.empty:
                continue
                
            metrics = self._calculate_comprehensive_metrics(symbol, str(date), long_day, short_day, volume_day)
            metrics_list.append(metrics)
        
        # è®¡ç®—è¶‹åŠ¿æŒ‡æ ‡
        self._calculate_trend_indicators(metrics_list)
        
        return metrics_list
    
    def _calculate_comprehensive_metrics(self, symbol: str, date: str, long_df: pd.DataFrame, 
                                       short_df: pd.DataFrame, volume_df: pd.DataFrame) -> EnhancedPositionMetrics:
        """è®¡ç®—å…¨é¢çš„æŒä»“æŒ‡æ ‡"""
        
        metrics = EnhancedPositionMetrics(date=date, symbol=symbol)
        
        # åŸºç¡€æŒä»“é‡ç»Ÿè®¡
        if not long_df.empty:
            metrics.long_top5_total = int(long_df.head(5)['æŒä»“é‡'].sum())
            metrics.long_top10_total = int(long_df.head(10)['æŒä»“é‡'].sum())
            metrics.long_top20_total = int(long_df['æŒä»“é‡'].sum())
            metrics.long_top5_change = int(long_df.head(5)['æ¯”ä¸Šäº¤æ˜“å¢å‡'].sum())
            metrics.long_top10_change = int(long_df.head(10)['æ¯”ä¸Šäº¤æ˜“å¢å‡'].sum())
            metrics.long_top20_change = int(long_df['æ¯”ä¸Šäº¤æ˜“å¢å‡'].sum())
            metrics.long_concentration = self._calculate_hhi(long_df['æŒä»“é‡'])
        
        if not short_df.empty:
            metrics.short_top5_total = int(short_df.head(5)['æŒä»“é‡'].sum())
            metrics.short_top10_total = int(short_df.head(10)['æŒä»“é‡'].sum())
            metrics.short_top20_total = int(short_df['æŒä»“é‡'].sum())
            metrics.short_top5_change = int(short_df.head(5)['æ¯”ä¸Šäº¤æ˜“å¢å‡'].sum())
            metrics.short_top10_change = int(short_df.head(10)['æ¯”ä¸Šäº¤æ˜“å¢å‡'].sum())
            metrics.short_top20_change = int(short_df['æ¯”ä¸Šäº¤æ˜“å¢å‡'].sum())
            metrics.short_concentration = self._calculate_hhi(short_df['æŒä»“é‡'])
        
        if not volume_df.empty:
            metrics.volume_top20_total = int(volume_df['æŒä»“é‡'].sum())
            metrics.volume_concentration = self._calculate_hhi(volume_df['æŒä»“é‡'])
        
        # èœ˜è››ç½‘ç­–ç•¥æ•°æ®
        metrics.spider_web_dB = metrics.long_top20_change
        metrics.spider_web_dS = metrics.short_top20_change
        metrics.spider_web_signal_strength = abs(metrics.spider_web_dB) + abs(metrics.spider_web_dS)
        
        # èªæ˜é’±æŒ‡æ ‡
        if metrics.volume_top20_total > 0:
            metrics.oi_volume_ratio = (metrics.long_top20_total + metrics.short_top20_total) / metrics.volume_top20_total
            metrics.position_efficiency = abs(metrics.long_top20_change - metrics.short_top20_change) / metrics.volume_top20_total
            metrics.smart_money_score = metrics.oi_volume_ratio * metrics.position_efficiency
        
        # é›†ä¸­åº¦æŒ‡æ ‡
        metrics.concentration_differential = metrics.long_concentration - metrics.short_concentration
        
        # å¸­ä½è¡Œä¸ºåˆ†æ
        seat_behavior = self._analyze_seat_behavior_detailed(symbol, date, long_df, short_df)
        metrics.retail_net_change = seat_behavior['retail_net']
        metrics.institutional_net_change = seat_behavior['institutional_net']
        metrics.foreign_net_change = seat_behavior['foreign_net']
        metrics.hedging_net_change = seat_behavior['hedging_net']
        metrics.retail_vs_smart_money = seat_behavior['retail_net'] - (seat_behavior['institutional_net'] + seat_behavior['foreign_net'])
        
        # æ´¾ç”ŸæŒ‡æ ‡
        metrics.net_position_top20 = metrics.long_top20_total - metrics.short_top20_total
        metrics.net_change_top20 = metrics.long_top20_change - metrics.short_top20_change
        
        if metrics.short_top20_total > 0:
            metrics.long_short_ratio = metrics.long_top20_total / metrics.short_top20_total
        
        # æŒä»“åŠ¨é‡
        if metrics.long_top20_total + metrics.short_top20_total > 0:
            metrics.position_momentum = abs(metrics.net_change_top20) / (metrics.long_top20_total + metrics.short_top20_total)
        
        # å¸‚åœºæƒ…ç»ªè¯„åˆ†
        metrics.market_sentiment_score = self._calculate_sentiment_score(metrics)
        metrics.consensus_level = self._calculate_consensus_level(long_df, short_df)
        
        return metrics
    
    def _calculate_hhi(self, positions: pd.Series) -> float:
        """è®¡ç®—èµ«èŠ¬è¾¾å°”-èµ«å¸Œæ›¼æŒ‡æ•°(HHI)"""
        if positions.empty or positions.sum() == 0:
            return 0.0
        
        shares = positions / positions.sum()
        return float((shares ** 2).sum())
    
    def _analyze_seat_behavior_detailed(self, symbol: str, date: str, long_df: pd.DataFrame, short_df: pd.DataFrame) -> Dict:
        """è¯¦ç»†åˆ†æå¸­ä½è¡Œä¸º"""
        category_stats = {
            'retail_net': 0, 'institutional_net': 0, 'foreign_net': 0, 'hedging_net': 0
        }
        
        # ç»Ÿè®¡å¤šå•å˜åŒ–
        for _, row in long_df.iterrows():
            seat_name = row['ä¼šå‘˜ç®€ç§°']
            category = self.classifier.classify_seat(seat_name)
            change = row['æ¯”ä¸Šäº¤æ˜“å¢å‡']
            
            if category == 'retail':
                category_stats['retail_net'] += change
            elif category in ['top_institutional', 'institutional']:
                category_stats['institutional_net'] += change
            elif category == 'foreign':
                category_stats['foreign_net'] += change
            elif category == 'hedging':
                category_stats['hedging_net'] += change
        
        # ç»Ÿè®¡ç©ºå•å˜åŒ–
        for _, row in short_df.iterrows():
            seat_name = row['ä¼šå‘˜ç®€ç§°']
            category = self.classifier.classify_seat(seat_name)
            change = row['æ¯”ä¸Šäº¤æ˜“å¢å‡']
            
            if category == 'retail':
                category_stats['retail_net'] -= change
            elif category in ['top_institutional', 'institutional']:
                category_stats['institutional_net'] -= change
            elif category == 'foreign':
                category_stats['foreign_net'] -= change
            elif category == 'hedging':
                category_stats['hedging_net'] -= change
        
        return category_stats
    
    def _calculate_sentiment_score(self, metrics: EnhancedPositionMetrics) -> float:
        """è®¡ç®—å¸‚åœºæƒ…ç»ªè¯„åˆ†"""
        # åŸºäºå¤šä¸ªç»´åº¦è®¡ç®—æƒ…ç»ªè¯„åˆ†
        score = 0.0
        
        # å‡€æŒä»“æ–¹å‘
        if metrics.net_position_top20 > 0:
            score += 0.3
        elif metrics.net_position_top20 < 0:
            score -= 0.3
        
        # å‡€å˜åŒ–æ–¹å‘
        if metrics.net_change_top20 > 0:
            score += 0.2
        elif metrics.net_change_top20 < 0:
            score -= 0.2
        
        # é›†ä¸­åº¦å·®å¼‚
        score += metrics.concentration_differential * 0.5
        
        # èªæ˜é’±å€¾å‘
        if metrics.smart_money_score > 0.1:
            if metrics.net_change_top20 > 0:
                score += 0.2
            else:
                score -= 0.2
        
        return max(-1.0, min(1.0, score))
    
    def _calculate_consensus_level(self, long_df: pd.DataFrame, short_df: pd.DataFrame) -> float:
        """è®¡ç®—å¸‚åœºå…±è¯†åº¦"""
        # åŸºäºæŒä»“å˜åŒ–çš„ä¸€è‡´æ€§è®¡ç®—å…±è¯†åº¦
        long_changes = long_df['æ¯”ä¸Šäº¤æ˜“å¢å‡'].values
        short_changes = short_df['æ¯”ä¸Šäº¤æ˜“å¢å‡'].values
        
        # è®¡ç®—å˜åŒ–æ–¹å‘çš„ä¸€è‡´æ€§
        long_positive_ratio = (long_changes > 0).mean()
        short_positive_ratio = (short_changes > 0).mean()
        
        # å…±è¯†åº¦ = 1 - åˆ†æ­§åº¦
        consensus = 1.0 - abs(0.5 - max(long_positive_ratio, 1 - long_positive_ratio, 
                                       short_positive_ratio, 1 - short_positive_ratio))
        
        return float(consensus)
    
    def _calculate_trend_indicators(self, metrics_list: List[EnhancedPositionMetrics]):
        """è®¡ç®—è¶‹åŠ¿æŒ‡æ ‡"""
        if len(metrics_list) < 5:
            return
        
        for i in range(4, len(metrics_list)):
            current = metrics_list[i]
            recent_5 = metrics_list[i-4:i+1]
            
            # 5æ—¥è¶‹åŠ¿
            net_changes = [m.net_change_top20 for m in recent_5]
            net_positions = [m.net_position_top20 for m in recent_5]
            concentrations = [m.long_concentration - m.short_concentration for m in recent_5]
            
            # è®¡ç®—ç›¸å…³æ€§ä½œä¸ºè¶‹åŠ¿æŒ‡æ ‡
            x = list(range(5))
            if len(set(net_changes)) > 1:
                current.net_change_trend_5d = np.corrcoef(x, net_changes)[0, 1]
            if len(set(net_positions)) > 1:
                current.position_trend_5d = np.corrcoef(x, net_positions)[0, 1]
            if len(set(concentrations)) > 1:
                current.concentration_trend_5d = np.corrcoef(x, concentrations)[0, 1]
            
            # æ³¢åŠ¨ç‡æŒ‡æ•°
            if i >= 9:  # éœ€è¦10å¤©æ•°æ®
                recent_10_changes = [m.net_change_top20 for m in metrics_list[i-9:i+1]]
                current.volatility_index = float(np.std(recent_10_changes))

# ============================================================================
# 4. ä¸“ä¸šAIåˆ†æå®¢æˆ·ç«¯
# ============================================================================

class ProfessionalAIAnalyst:
    """ä¸“ä¸šAIåˆ†æå¸ˆ - å®Œå…¨åŸºäºAIçš„æœŸè´§æŒä»“åˆ†æ"""
    
    def __init__(self, api_key: str = DEEPSEEK_API_KEY):
        self.api_key = api_key
        self.model = DEEPSEEK_MODEL
        self.base_url = DEEPSEEK_BASE_URL
    
    def comprehensive_positioning_analysis(self, symbol: str, metrics_data: List[EnhancedPositionMetrics]) -> Optional[Dict]:
        """ç»¼åˆæŒä»“æ•°æ®AIåˆ†æ"""
        
        if not self.api_key or not metrics_data:
            return None
        
        # æ„å»ºä¸“ä¸šåˆ†æprompt
        system_prompt = self._build_professional_analysis_prompt()
        
        # å‡†å¤‡æ•°æ®
        analysis_data = self._prepare_analysis_data(symbol, metrics_data)
        
        headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json"
        }
        
        request_data = {
            "model": self.model,
            "messages": [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": json.dumps(analysis_data, ensure_ascii=False)}
            ],
            "temperature": 0.2,
            "max_tokens": 2000
        }
        
        try:
            with httpx.Client(timeout=60) as client:
                response = client.post(self.base_url, headers=headers, json=request_data)
                response.raise_for_status()
                
                data = response.json()
                content = data.get("choices", [{}])[0].get("message", {}).get("content", "")
                
                # å°è¯•è§£æJSON
                try:
                    result = json.loads(content)
                    return result if isinstance(result, dict) else None
                except json.JSONDecodeError:
                    # å°è¯•æå–JSONéƒ¨åˆ†
                    import re
                    json_match = re.search(r'\{[\s\S]*\}', content)
                    if json_match:
                        try:
                            return json.loads(json_match.group(0))
                        except json.JSONDecodeError:
                            pass
                    
                    # è¿”å›åŸå§‹åˆ†æ
                    return {"raw_analysis": content, "parsing_error": True}
                    
        except Exception as e:
            return {"error": str(e), "analysis_failed": True}
    
    def _build_professional_analysis_prompt(self) -> str:
        """æ„å»ºä¸“ä¸šåˆ†æprompt"""
        return """
ä½ æ˜¯èµ„æ·±çš„å•†å“æœŸè´§æŒä»“æ•°æ®åˆ†æä¸“å®¶ï¼Œæ‹¥æœ‰15å¹´çš„æœŸè´§å¸‚åœºåˆ†æç»éªŒï¼Œä¸“ç²¾äºé€šè¿‡æŒä»“æ•°æ®é¢„åˆ¤å¸‚åœºèµ°åŠ¿ã€‚

## åˆ†ææ–¹æ³•è®º

### 1. èœ˜è››ç½‘ç­–ç•¥åˆ†æ (Spider Web Strategy)
**æ ¸å¿ƒåŸç†**: åŸºäºæœŸè´§äº¤æ˜“æ‰€å…¬å¸ƒçš„å‰20åä¼šå‘˜æŒä»“å˜åŠ¨æ•°æ®ï¼Œæ•æ‰"èªæ˜èµ„é‡‘"åŠ¨å‘
**å…³é”®æŒ‡æ ‡**: 
- dB (å¤šå¤´å˜åŠ¨): spider_web_dBï¼Œåæ˜ å¤šå¤´èµ„é‡‘å¢å‡
- dS (ç©ºå¤´å˜åŠ¨): spider_web_dSï¼Œåæ˜ ç©ºå¤´èµ„é‡‘å¢å‡
**ä¿¡å·åˆ¤æ–­**:
- dB > 0 ä¸” dS < 0: å¤šå¤´å¢ä»“+ç©ºå¤´å‡ä»“ â†’ å¼ºçƒˆçœ‹å¤šä¿¡å·
- dB < 0 ä¸” dS > 0: å¤šå¤´å‡ä»“+ç©ºå¤´å¢ä»“ â†’ å¼ºçƒˆçœ‹ç©ºä¿¡å·  
- dB å’Œ dS åŒå‘: å¯èƒ½å­˜åœ¨åˆ†æ­§æˆ–å¥—åˆ©è¡Œä¸ºï¼Œéœ€ç»“åˆå…¶ä»–æŒ‡æ ‡
**ä¿¡å·å¼ºåº¦**: spider_web_signal_strength = |dB| + |dS|ï¼Œæ•°å€¼è¶Šå¤§ä¿¡å·è¶Šå¼º

### 2. èªæ˜é’±åˆ†æ (Smart Money Analysis)
**æ ¸å¿ƒåŸç†**: é€šè¿‡æŒä»“/æˆäº¤æ¯”è¯†åˆ«çŸ¥æƒ…èµ„é‡‘ï¼Œèªæ˜é’±é€šå¸¸æŒä»“æ—¶é—´é•¿ã€äº¤æ˜“é¢‘ç‡ä½
**å…³é”®æŒ‡æ ‡**:
- oi_volume_ratio: æŒä»“/æˆäº¤æ¯”ï¼Œ>2.0 è¡¨ç¤ºèªæ˜é’±æ´»è·ƒ
- position_efficiency: æŒä»“æ•ˆç‡ï¼Œ>0.1 è¡¨ç¤ºèµ„é‡‘ä½¿ç”¨é«˜æ•ˆ
- smart_money_score: èªæ˜é’±è¯„åˆ† = oi_volume_ratio Ã— position_efficiency
**åˆ¤æ–­é€»è¾‘**: å½“èªæ˜é’±æ´»è·ƒä¸”å‡€æŒä»“å˜åŒ–æ˜æ˜¾æ—¶ï¼Œè·Ÿéšå…¶æ–¹å‘

### 3. å®¶äººå¸­ä½åå‘ç­–ç•¥ (Retail Reverse Strategy)  
**æ ¸å¿ƒåŸç†**: æ•£æˆ·é€šå¸¸æ˜¯å¸‚åœºçš„åå‘æŒ‡æ ‡ï¼Œä¸æœºæ„å¤–èµ„è¡Œä¸ºç›¸åæ—¶ä¿¡å·æ›´å¼º
**å…³é”®æŒ‡æ ‡**:
- retail_net_change: æ•£æˆ·å‡€æŒä»“å˜åŒ–
- institutional_net_change: æœºæ„å‡€æŒä»“å˜åŒ–  
- foreign_net_change: å¤–èµ„å‡€æŒä»“å˜åŒ–
- retail_vs_smart_money: æ•£æˆ·vsèªæ˜é’±å¯¹æ¯”
**ä¿¡å·å¼ºåº¦**: æ•£æˆ·ä¸æœºæ„å¤–èµ„æ–¹å‘ç›¸åæ—¶ä¿¡å·æœ€å¼ºï¼Œä»…æ•£æˆ·æœ‰æ–¹å‘æ—¶ä¿¡å·ä¸­ç­‰

### 4. æŒä»“é›†ä¸­åº¦åˆ†æ (Concentration Analysis)
**æ ¸å¿ƒåŸç†**: å¤§æˆ·æ§ç›˜ç¨‹åº¦åæ˜ å¸‚åœºå½±å“åŠ›ï¼Œé›†ä¸­åº¦å˜åŒ–é¢„ç¤ºè¶‹åŠ¿è½¬æ¢
**å…³é”®æŒ‡æ ‡**:
- long_concentration/short_concentration: å¤šç©ºå•é›†ä¸­åº¦(HHIæŒ‡æ•°)
- concentration_differential: é›†ä¸­åº¦å·®å¼‚
- concentration_trend_5d: 5æ—¥é›†ä¸­åº¦è¶‹åŠ¿
**åˆ¤æ–­æ ‡å‡†**: é›†ä¸­åº¦>0.15ä¸”ä¸Šå‡è¶‹åŠ¿è¡¨ç¤ºå¤§æˆ·æ§ç›˜åŠ å¼º

### 5. è¶‹åŠ¿ä¸æƒ…ç»ªåˆ†æ
**å…³é”®æŒ‡æ ‡**:
- net_change_trend_5d/position_trend_5d: 5æ—¥è¶‹åŠ¿ç›¸å…³æ€§
- market_sentiment_score: å¸‚åœºæƒ…ç»ªè¯„åˆ†(-1åˆ°1)
- consensus_level: å¸‚åœºå…±è¯†åº¦(0åˆ°1)
- volatility_index: æ³¢åŠ¨ç‡æŒ‡æ•°

## è¾“å‡ºè¦æ±‚

è¯·åŸºäºæä¾›çš„ç»“æ„åŒ–æ•°æ®è¿›è¡Œä¸“ä¸šåˆ†æï¼Œè¾“å‡ºä¸¥æ ¼JSONæ ¼å¼ï¼š

```json
{
  "spider_web_analysis": {
    "signal": "å¼ºçƒˆçœ‹å¤š|çœ‹å¤š|ä¸­æ€§|çœ‹ç©º|å¼ºçƒˆçœ‹ç©º",
    "strength": 0.0-1.0,
    "confidence": 0.0-1.0,
    "reasoning": "è¯¦ç»†åˆ†æè¿‡ç¨‹ï¼ŒåŒ…æ‹¬dBã€dSæ•°å€¼åŠå…¶å«ä¹‰",
    "key_metrics": {"dB": æ•°å€¼, "dS": æ•°å€¼, "signal_strength": æ•°å€¼}
  },
  "smart_money_analysis": {
    "signal": "çœ‹å¤š|ä¸­æ€§|çœ‹ç©º", 
    "strength": 0.0-1.0,
    "confidence": 0.0-1.0,
    "reasoning": "èªæ˜é’±æ´»è·ƒåº¦åˆ†æï¼ŒæŒä»“æ•ˆç‡è¯„ä¼°",
    "key_metrics": {"oi_volume_ratio": æ•°å€¼, "smart_money_score": æ•°å€¼}
  },
  "retail_reverse_analysis": {
    "signal": "çœ‹å¤š|ä¸­æ€§|çœ‹ç©º",
    "strength": 0.0-1.0, 
    "confidence": 0.0-1.0,
    "reasoning": "æ•£æˆ·vsæœºæ„å¤–èµ„è¡Œä¸ºå¯¹æ¯”åˆ†æ",
    "key_metrics": {"retail_net": æ•°å€¼, "smart_money_net": æ•°å€¼}
  },
  "concentration_analysis": {
    "signal": "çœ‹å¤š|ä¸­æ€§|çœ‹ç©º",
    "strength": 0.0-1.0,
    "confidence": 0.0-1.0, 
    "reasoning": "æŒä»“é›†ä¸­åº¦å˜åŒ–åŠå…¶å¸‚åœºå«ä¹‰",
    "key_metrics": {"long_conc": æ•°å€¼, "short_conc": æ•°å€¼, "trend": æ•°å€¼}
  },
  "comprehensive_conclusion": {
    "overall_signal": "å¼ºçƒˆçœ‹å¤š|çœ‹å¤š|ä¸­æ€§|çœ‹ç©º|å¼ºçƒˆçœ‹ç©º",
    "confidence": 0.0-1.0,
    "time_horizon": "çŸ­æœŸ(1-3å¤©)|ä¸­æœŸ(1-2å‘¨)|é•¿æœŸ(1ä¸ªæœˆ+)",
    "key_factors": ["å…³é”®å› ç´ 1", "å…³é”®å› ç´ 2", "å…³é”®å› ç´ 3"],
    "supporting_evidence": "æ”¯æ’‘è¯æ®çš„è¯¦ç»†æè¿°",
    "risk_factors": ["é£é™©å› ç´ 1", "é£é™©å› ç´ 2"],
    "market_regime": "è¶‹åŠ¿å¸‚|éœ‡è¡å¸‚|è½¬æŠ˜æœŸ",
    "sentiment_assessment": "æåº¦ä¹è§‚|ä¹è§‚|ä¸­æ€§|æ‚²è§‚|æåº¦æ‚²è§‚"
  },
  "trading_recommendations": {
    "primary_strategy": "å…·ä½“äº¤æ˜“ç­–ç•¥å»ºè®®",
    "entry_timing": "å…¥åœºæ—¶æœºå»ºè®®", 
    "position_sizing": "å»ºè®®ä»“ä½å¤§å°",
    "stop_loss": "æ­¢æŸä½å»ºè®®",
    "profit_target": "ç›®æ ‡ä½å»ºè®®",
    "risk_management": "é£é™©ç®¡ç†è¦ç‚¹"
  },
  "professional_insights": {
    "market_microstructure": "å¸‚åœºå¾®è§‚ç»“æ„åˆ†æ",
    "institutional_behavior": "æœºæ„è¡Œä¸ºæ¨¡å¼è§‚å¯Ÿ", 
    "contrarian_signals": "åå‘æŒ‡æ ‡è¯†åˆ«",
    "momentum_analysis": "åŠ¨é‡ç‰¹å¾åˆ†æ",
    "regime_change_probability": "å¸‚åœºçŠ¶æ€è½¬æ¢æ¦‚ç‡è¯„ä¼°"
  }
}
```

## åˆ†æè¦æ±‚
1. ä¸¥æ ¼åŸºäºæ•°æ®è¿›è¡Œåˆ†æï¼Œä¸ç¼–é€ æ•°å€¼
2. ä½“ç°ä¸“ä¸šæœŸè´§åˆ†æå¸ˆçš„æ€ç»´æ·±åº¦
3. ç»“åˆå¤šä¸ªç­–ç•¥è¿›è¡Œç»¼åˆåˆ¤æ–­
4. æä¾›å…·ä½“å¯æ“ä½œçš„äº¤æ˜“å»ºè®®
5. è¯†åˆ«å…³é”®é£é™©ç‚¹å’Œæœºä¼šçª—å£
6. ä¿æŒå®¢è§‚ä¸­æ€§çš„ä¸“ä¸šæ€åº¦
"""
    
    def _prepare_analysis_data(self, symbol: str, metrics_list: List[EnhancedPositionMetrics]) -> Dict:
        """ä¸ºAIå‡†å¤‡åˆ†ææ•°æ®"""
        
        if not metrics_list:
            return {}
        
        latest = metrics_list[-1]
        
        # æœ€è¿‘5å¤©æ•°æ®ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰
        recent_data = []
        for m in metrics_list[-5:]:
            simplified = {
                'date': m.date,
                'net_position': m.net_position_top20,
                'net_change': m.net_change_top20,
                'spider_web_dB': m.spider_web_dB,
                'spider_web_dS': m.spider_web_dS,
                'smart_money_score': m.smart_money_score,
                'long_concentration': m.long_concentration,
                'short_concentration': m.short_concentration,
                'retail_net_change': m.retail_net_change,
                'institutional_net_change': m.institutional_net_change
            }
            recent_data.append(simplified)
        
        # è¶‹åŠ¿ç»Ÿè®¡
        trend_stats = {}
        if len(metrics_list) >= 5:
            recent_5 = metrics_list[-5:]
            trend_stats = {
                "net_change_trend": [m.net_change_top20 for m in recent_5],
                "net_position_trend": [m.net_position_top20 for m in recent_5],
                "long_concentration_trend": [m.long_concentration for m in recent_5],
                "short_concentration_trend": [m.short_concentration for m in recent_5],
                "smart_money_score_trend": [m.smart_money_score for m in recent_5]
            }
        
        # å…³é”®ç»Ÿè®¡æŒ‡æ ‡
        key_statistics = {
            "avg_net_change_5d": float(np.mean([m.net_change_top20 for m in metrics_list[-5:]])),
            "std_net_change_5d": float(np.std([m.net_change_top20 for m in metrics_list[-5:]])),
            "max_net_position": max(m.net_position_top20 for m in metrics_list),
            "min_net_position": min(m.net_position_top20 for m in metrics_list),
            "avg_smart_money_score": float(np.mean([m.smart_money_score for m in metrics_list[-10:]])),
            "current_vs_avg_position": latest.net_position_top20 - np.mean([m.net_position_top20 for m in metrics_list[:-1]])
        }
        
        return {
            "symbol": symbol,
            "analysis_date": datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
            "data_period": f"{metrics_list[0].date} è‡³ {metrics_list[-1].date}",
            "total_days": len(metrics_list),
            
            # å½“å‰æ•°æ®
            "latest_metrics": asdict(latest),
            
            # æœ€è¿‘è¶‹åŠ¿
            "recent_5_days": recent_data,
            "trend_statistics": trend_stats,
            "key_statistics": key_statistics,
            
            # ç­–ç•¥ç‰¹å®šæ•°æ®
            "spider_web_data": {
                "current_dB": latest.spider_web_dB,
                "current_dS": latest.spider_web_dS,
                "signal_strength": latest.spider_web_signal_strength,
                "recent_dB_trend": [m.spider_web_dB for m in metrics_list[-5:]],
                "recent_dS_trend": [m.spider_web_dS for m in metrics_list[-5:]]
            },
            
            "smart_money_data": {
                "current_score": latest.smart_money_score,
                "oi_volume_ratio": latest.oi_volume_ratio,
                "position_efficiency": latest.position_efficiency,
                "trend": [m.smart_money_score for m in metrics_list[-5:]]
            },
            
            "seat_behavior_data": {
                "retail_net": latest.retail_net_change,
                "institutional_net": latest.institutional_net_change,
                "foreign_net": latest.foreign_net_change,
                "retail_vs_smart": latest.retail_vs_smart_money,
                "recent_retail_trend": [m.retail_net_change for m in metrics_list[-5:]]
            },
            
            "concentration_data": {
                "long_concentration": latest.long_concentration,
                "short_concentration": latest.short_concentration,
                "differential": latest.concentration_differential,
                "trend_5d": latest.concentration_trend_5d
            }
        }

# ============================================================================
# 5. ä¸»åˆ†æå¼•æ“
# ============================================================================

class ProfessionalPositioningAnalysisEngine:
    """ä¸“ä¸šæŒä»“åˆ†æå¼•æ“"""
    
    def __init__(self):
        self.data_processor = ProfessionalDataProcessor()
        self.ai_analyst = ProfessionalAIAnalyst()
    
    def analyze_symbol_professional(self, symbol: str, days_back: int = 30) -> Dict:
        """ä¸“ä¸šå“ç§åˆ†æ"""
        
        print(f"ğŸ” å¼€å§‹ä¸“ä¸šåˆ†æå“ç§: {symbol}")
        print("-" * 50)
        
        # 1. æ•°æ®å‡†å¤‡
        print("ğŸ“Š å‡†å¤‡åˆ†ææ•°æ®...")
        metrics_data = self.data_processor.prepare_comprehensive_data(symbol, days_back)
        
        if not metrics_data:
            return {
                'symbol': symbol,
                'status': 'failed',
                'reason': 'æ— å¯ç”¨æ•°æ®',
                'analysis_time': datetime.now().isoformat()
            }
        
        print(f"âœ… æˆåŠŸå‡†å¤‡ {len(metrics_data)} å¤©çš„å®Œæ•´æ•°æ®")
        
        # 2. AIä¸“ä¸šåˆ†æ
        print("ğŸ¤– æ‰§è¡ŒAIä¸“ä¸šåˆ†æ...")
        ai_analysis = self.ai_analyst.comprehensive_positioning_analysis(symbol, metrics_data)
        
        if ai_analysis and not ai_analysis.get('analysis_failed'):
            print("âœ… AIä¸“ä¸šåˆ†æå®Œæˆ")
        else:
            print("âš ï¸ AIåˆ†æå¤±è´¥ï¼Œè¿”å›åŸºç¡€æ•°æ®")
        
        # 3. æ„å»ºç»“æœ
        result = {
            'symbol': symbol,
            'status': 'success',
            'analysis_time': datetime.now().isoformat(),
            'data_period': f"{metrics_data[0].date} ~ {metrics_data[-1].date}",
            'days_analyzed': len(metrics_data),
            
            # AIä¸“ä¸šåˆ†æç»“æœ
            'ai_professional_analysis': ai_analysis,
            
            # æœ€æ–°æ•°æ®å¿«ç…§
            'latest_metrics': asdict(metrics_data[-1]) if metrics_data else {},
            
            # åŸå§‹æ•°æ®ï¼ˆä¾›è¿›ä¸€æ­¥åˆ†æï¼‰
            'raw_metrics_data': [asdict(m) for m in metrics_data[-10:]]
        }
        
        return result
    
    def batch_analyze_professional(self, symbols: List[str], days_back: int = 30) -> Dict:
        """æ‰¹é‡ä¸“ä¸šåˆ†æ"""
        print(f"ğŸš€ å¼€å§‹æ‰¹é‡ä¸“ä¸šåˆ†æ {len(symbols)} ä¸ªå“ç§")
        print("=" * 80)
        
        results = {}
        successful = 0
        start_time = time.time()
        
        for i, symbol in enumerate(symbols, 1):
            print(f"\n[{i}/{len(symbols)}] åˆ†æå“ç§: {symbol}")
            try:
                result = self.analyze_symbol_professional(symbol, days_back)
                results[symbol] = result
                
                if result['status'] == 'success':
                    successful += 1
                    # å°è¯•æå–AIåˆ†æç»“è®º
                    ai_analysis = result.get('ai_professional_analysis', {})
                    if ai_analysis and not ai_analysis.get('parsing_error'):
                        conclusion = ai_analysis.get('comprehensive_conclusion', {})
                        overall_signal = conclusion.get('overall_signal', 'æœªçŸ¥')
                        confidence = conclusion.get('confidence', 0.0)
                        print(f"ğŸ¯ AIåˆ†æç»“è®º: {overall_signal} (ç½®ä¿¡åº¦: {confidence:.2f})")
                    else:
                        print("ğŸ¯ AIåˆ†æå®Œæˆï¼ˆéœ€æŸ¥çœ‹è¯¦ç»†ç»“æœï¼‰")
                        
            except Exception as e:
                results[symbol] = {
                    'symbol': symbol,
                    'status': 'error',
                    'reason': str(e),
                    'analysis_time': datetime.now().isoformat()
                }
                print(f"âŒ åˆ†æå¤±è´¥: {e}")
        
        total_time = time.time() - start_time
        
        # ç”Ÿæˆæ±‡æ€»
        summary = self._generate_professional_summary(results)
        
        return {
            'analysis_time': datetime.now().isoformat(),
            'total_symbols': len(symbols),
            'successful_analyses': successful,
            'processing_time_seconds': round(total_time, 2),
            'professional_summary': summary,
            'detailed_results': results
        }
    
    def _generate_professional_summary(self, results: Dict) -> Dict:
        """ç”Ÿæˆä¸“ä¸šæ±‡æ€»"""
        signal_counts = {'å¼ºçƒˆçœ‹å¤š': 0, 'çœ‹å¤š': 0, 'ä¸­æ€§': 0, 'çœ‹ç©º': 0, 'å¼ºçƒˆçœ‹ç©º': 0, 'æœªçŸ¥': 0}
        high_confidence_signals = []
        
        for symbol, result in results.items():
            if result.get('status') == 'success':
                ai_analysis = result.get('ai_professional_analysis', {})
                if ai_analysis and not ai_analysis.get('parsing_error'):
                    conclusion = ai_analysis.get('comprehensive_conclusion', {})
                    signal = conclusion.get('overall_signal', 'æœªçŸ¥')
                    confidence = conclusion.get('confidence', 0.0)
                    
                    if signal in signal_counts:
                        signal_counts[signal] += 1
                    else:
                        signal_counts['æœªçŸ¥'] += 1
                    
                    if confidence >= 0.7:
                        high_confidence_signals.append({
                            'symbol': symbol,
                            'signal': signal,
                            'confidence': confidence
                        })
        
        return {
            'signal_distribution': signal_counts,
            'high_confidence_signals': sorted(high_confidence_signals, key=lambda x: x['confidence'], reverse=True),
            'market_sentiment_overview': self._assess_overall_sentiment(signal_counts)
        }
    
    def _assess_overall_sentiment(self, signal_counts: Dict) -> str:
        """è¯„ä¼°æ•´ä½“å¸‚åœºæƒ…ç»ª"""
        total = sum(signal_counts.values())
        if total == 0:
            return "æ— æ³•è¯„ä¼°"
        
        bullish = signal_counts.get('å¼ºçƒˆçœ‹å¤š', 0) + signal_counts.get('çœ‹å¤š', 0)
        bearish = signal_counts.get('å¼ºçƒˆçœ‹ç©º', 0) + signal_counts.get('çœ‹ç©º', 0)
        neutral = signal_counts.get('ä¸­æ€§', 0)
        
        bullish_ratio = bullish / total
        bearish_ratio = bearish / total
        
        if bullish_ratio > 0.6:
            return "å¸‚åœºæ•´ä½“åå¤š"
        elif bearish_ratio > 0.6:
            return "å¸‚åœºæ•´ä½“åç©º"
        elif neutral / total > 0.5:
            return "å¸‚åœºè§‚æœ›æƒ…ç»ªæµ“åš"
        else:
            return "å¸‚åœºåˆ†æ­§è¾ƒå¤§"
    
    def save_professional_report(self, analysis_results: Dict, filename: str = None) -> Path:
        """ä¿å­˜ä¸“ä¸šåˆ†ææŠ¥å‘Š"""
        if filename is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"professional_positioning_analysis_{timestamp}.json"
        
        OUTPUT_DIR.mkdir(parents=True, exist_ok=True)
        output_file = OUTPUT_DIR / filename
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(analysis_results, f, ensure_ascii=False, indent=2)
        
        print(f"\nğŸ“„ ä¸“ä¸šåˆ†ææŠ¥å‘Šå·²ä¿å­˜: {output_file}")
        return output_file

# ============================================================================
# 6. ä½¿ç”¨ç¤ºä¾‹å’Œä¸»ç¨‹åº
# ============================================================================

def professional_single_analysis(symbol: str, days_back: int = 30):
    """ä¸“ä¸šå•å“ç§åˆ†æ"""
    engine = ProfessionalPositioningAnalysisEngine()
    result = engine.analyze_symbol_professional(symbol, days_back)
    
    if result['status'] != 'success':
        print(f"âŒ {symbol} åˆ†æå¤±è´¥: {result['reason']}")
        return result
    
    print(f"\nğŸ¯ {symbol} ä¸“ä¸šåˆ†æç»“æœ")
    print("=" * 60)
    
    ai_analysis = result.get('ai_professional_analysis', {})
    
    if ai_analysis and not ai_analysis.get('analysis_failed') and not ai_analysis.get('parsing_error'):
        # æ˜¾ç¤ºå„ç­–ç•¥åˆ†æ
        strategies = ['spider_web_analysis', 'smart_money_analysis', 'retail_reverse_analysis', 'concentration_analysis']
        
        for strategy in strategies:
            if strategy in ai_analysis:
                analysis = ai_analysis[strategy]
                signal = analysis.get('signal', 'æœªçŸ¥')
                strength = analysis.get('strength', 0.0)
                confidence = analysis.get('confidence', 0.0)
                reasoning = analysis.get('reasoning', 'æ— ')
                
                strategy_name = {
                    'spider_web_analysis': 'ğŸ•¸ï¸ èœ˜è››ç½‘ç­–ç•¥',
                    'smart_money_analysis': 'ğŸ§  èªæ˜é’±åˆ†æ', 
                    'retail_reverse_analysis': 'ğŸ”„ å®¶äººå¸­ä½åå‘',
                    'concentration_analysis': 'ğŸ“Š æŒä»“é›†ä¸­åº¦'
                }.get(strategy, strategy)
                
                print(f"\n{strategy_name}:")
                print(f"   ä¿¡å·: {signal} | å¼ºåº¦: {strength:.2f} | ç½®ä¿¡åº¦: {confidence:.2f}")
                print(f"   åˆ†æ: {reasoning}")
        
        # ç»¼åˆç»“è®º
        if 'comprehensive_conclusion' in ai_analysis:
            conclusion = ai_analysis['comprehensive_conclusion']
            print(f"\nğŸ¯ ç»¼åˆç»“è®º:")
            print(f"   æ€»ä½“ä¿¡å·: {conclusion.get('overall_signal', 'æœªçŸ¥')}")
            print(f"   ç½®ä¿¡åº¦: {conclusion.get('confidence', 0.0):.2f}")
            print(f"   æ—¶é—´å‘¨æœŸ: {conclusion.get('time_horizon', 'æœªçŸ¥')}")
            print(f"   å…³é”®å› ç´ : {', '.join(conclusion.get('key_factors', []))}")
            print(f"   å¸‚åœºçŠ¶æ€: {conclusion.get('market_regime', 'æœªçŸ¥')}")
            print(f"   æƒ…ç»ªè¯„ä¼°: {conclusion.get('sentiment_assessment', 'æœªçŸ¥')}")
        
        # äº¤æ˜“å»ºè®®
        if 'trading_recommendations' in ai_analysis:
            trading = ai_analysis['trading_recommendations']
            print(f"\nğŸ’¡ äº¤æ˜“å»ºè®®:")
            print(f"   ä¸»è¦ç­–ç•¥: {trading.get('primary_strategy', 'æ— ')}")
            print(f"   å…¥åœºæ—¶æœº: {trading.get('entry_timing', 'æ— ')}")
            print(f"   ä»“ä½å»ºè®®: {trading.get('position_sizing', 'æ— ')}")
            print(f"   æ­¢æŸå»ºè®®: {trading.get('stop_loss', 'æ— ')}")
            
        # ä¸“ä¸šæ´å¯Ÿ
        if 'professional_insights' in ai_analysis:
            insights = ai_analysis['professional_insights']
            print(f"\nğŸ”¬ ä¸“ä¸šæ´å¯Ÿ:")
            for key, value in insights.items():
                key_name = {
                    'market_microstructure': 'å¸‚åœºå¾®è§‚ç»“æ„',
                    'institutional_behavior': 'æœºæ„è¡Œä¸ºæ¨¡å¼',
                    'contrarian_signals': 'åå‘ä¿¡å·è¯†åˆ«',
                    'momentum_analysis': 'åŠ¨é‡ç‰¹å¾åˆ†æ',
                    'regime_change_probability': 'çŠ¶æ€è½¬æ¢æ¦‚ç‡'
                }.get(key, key)
                print(f"   {key_name}: {value}")
                
    else:
        print("âš ï¸ AIåˆ†æç»“æœè§£æå¤±è´¥æˆ–åˆ†æå‡ºé”™")
        if ai_analysis.get('raw_analysis'):
            print(f"åŸå§‹åˆ†æ: {ai_analysis['raw_analysis'][:500]}...")
    
    return result

def professional_batch_analysis():
    """ä¸“ä¸šæ‰¹é‡åˆ†æ"""
    engine = ProfessionalPositioningAnalysisEngine()
    
    # ä¸»è¦å“ç§
    symbols = ['RB', 'CU', 'AL', 'I', 'J', 'JM', 'MA', 'TA', 'CF', 'SR', 'M', 'Y', 'P', 'A', 'C']
    
    print("ğŸ¯ ä¸“ä¸šæœŸè´§æŒä»“æ•°æ®AIåˆ†æç³»ç»Ÿ")
    print(f"ğŸ“‹ åˆ†æå“ç§: {', '.join(symbols)}")
    print(f"ğŸ“… åˆ†æå‘¨æœŸ: æœ€è¿‘30ä¸ªäº¤æ˜“æ—¥")
    print(f"ğŸ¤– AIæ¨¡å‹: {DEEPSEEK_MODEL}")
    
    results = engine.batch_analyze_professional(symbols, days_back=30)
    
    # ä¿å­˜æŠ¥å‘Š
    report_file = engine.save_professional_report(results)
    
    # æ˜¾ç¤ºæ±‡æ€»
    summary = results['professional_summary']
    print(f"\nğŸ“Š ä¸“ä¸šåˆ†ææ±‡æ€»:")
    print(f"   âœ… æˆåŠŸåˆ†æ: {results['successful_analyses']}/{results['total_symbols']} ä¸ªå“ç§")
    print(f"   â±ï¸ å¤„ç†æ—¶é—´: {results['processing_time_seconds']} ç§’")
    
    print(f"\nğŸ“ˆ ä¿¡å·åˆ†å¸ƒ:")
    for signal, count in summary['signal_distribution'].items():
        if count > 0:
            print(f"   {signal}: {count} ä¸ªå“ç§")
    
    print(f"\nğŸŒ¡ï¸ å¸‚åœºæƒ…ç»ª: {summary['market_sentiment_overview']}")
    
    if summary['high_confidence_signals']:
        print(f"\nğŸ”¥ é«˜ç½®ä¿¡åº¦ä¿¡å· (â‰¥0.7):")
        for item in summary['high_confidence_signals'][:10]:
            print(f"   {item['symbol']}: {item['signal']} (ç½®ä¿¡åº¦: {item['confidence']:.2f})")
    
    return results

if __name__ == "__main__":
    print("ğŸš€ ä¸“ä¸šæœŸè´§æŒä»“æ•°æ®AIåˆ†æç³»ç»Ÿå·²åŠ è½½")
    print("\nä½¿ç”¨æ–¹æ³•:")
    print("1. å•å“ç§ä¸“ä¸šåˆ†æ: result = professional_single_analysis('RB')")
    print("2. æ‰¹é‡ä¸“ä¸šåˆ†æ: results = professional_batch_analysis()")
    print("\nğŸ’¡ æ‰€æœ‰åˆ†æé€»è¾‘å®Œå…¨ç”±AIå®Œæˆï¼Œç¡®ä¿ä¸“ä¸šæ€§å’Œå®¢è§‚æ€§")
