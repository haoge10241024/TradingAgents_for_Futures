# ============================================================================
# ä¸“ä¸šæœŸè´§æŒä»“æ•°æ®AIåˆ†æç³»ç»Ÿ - å®Œç¾ç‰ˆ
# äº¤äº’å¼ç”¨æˆ·ç•Œé¢ + DeepSeek v3.1 Reasoner + Serper + å†å²å¯¹æ¯”åˆ†æ
# ============================================================================

import pandas as pd
import numpy as np
from pathlib import Path
from datetime import datetime, timedelta
from typing import Dict, List, Tuple, Optional, Any
import json
import warnings
from dataclasses import dataclass, asdict
import httpx
import time
import requests
from scipy import stats
import asyncio
import aiohttp

warnings.filterwarnings('ignore')

print("ğŸš€ ä¸“ä¸šæœŸè´§æŒä»“æ•°æ®AIåˆ†æç³»ç»Ÿ - å®Œç¾ç‰ˆ")
print("ğŸ”¥ äº¤äº’å¼ç•Œé¢ + DeepSeek v3.1 Reasoner + Serper + å†å²å¯¹æ¯”åˆ†æ")
print("=" * 80)

# ============================================================================
# 1. é…ç½®å’Œæ•°æ®ç»“æ„
# ============================================================================

# åŸºç¡€é…ç½®
BASE_DIR = Path(r"D:\Cursor\cursoré¡¹ç›®\TradingAgent")
POSITIONING_ROOT = BASE_DIR / "qihuo" / "database" / "positioning"
OUTPUT_DIR = BASE_DIR / "qihuo" / "output"

# DeepSeek APIé…ç½® - v3.1
DEEPSEEK_API_KEY = "sk-293dec7fabb54606b4f8d4f606da3383"
DEEPSEEK_MODEL_REASONER = "deepseek-reasoner"  # v3.1 Reasoneræ¨¡å¼
DEEPSEEK_MODEL_CHAT = "deepseek-chat"
DEEPSEEK_BASE_URL = "https://api.deepseek.com/v1/chat/completions"

# Serper APIé…ç½®
SERPER_API_KEY = "d3654e36956e0bf331e901886c49c602cea72eb1"
SERPER_BASE_URL = "https://google.serper.dev/search"

# å“ç§ä¸­æ–‡åç§°æ˜ å°„
SYMBOL_NAMES = {
    'RB': 'èºçº¹é’¢', 'CU': 'æ²ªé“œ', 'AL': 'æ²ªé“', 'I': 'é“çŸ¿çŸ³', 'J': 'ç„¦ç‚­', 'JM': 'ç„¦ç…¤',
    'MA': 'ç”²é†‡', 'TA': 'PTA', 'CF': 'éƒ‘æ£‰', 'SR': 'ç™½ç³–', 'M': 'è±†ç²•', 'Y': 'è±†æ²¹',
    'P': 'æ£•æ¦ˆæ²¹', 'A': 'è±†ä¸€', 'C': 'ç‰ç±³', 'AU': 'æ²ªé‡‘', 'AG': 'æ²ªé“¶', 'ZN': 'æ²ªé”Œ',
    'NI': 'é•', 'PB': 'æ²ªé“…', 'SN': 'é”¡', 'FU': 'ç‡ƒæ²¹', 'BU': 'æ²¥é’', 'RU': 'æ©¡èƒ¶',
    'L': 'å¡‘æ–™', 'V': 'PVC', 'PP': 'èšä¸™çƒ¯', 'EG': 'ä¹™äºŒé†‡', 'EB': 'è‹¯ä¹™çƒ¯'
}

# JSONåºåˆ—åŒ–è¾…åŠ©å‡½æ•°
def json_serialize_helper(obj):
    """å¤„ç†numpyç±»å‹çš„JSONåºåˆ—åŒ–"""
    if isinstance(obj, (np.integer, np.floating, np.ndarray)):
        return obj.item() if hasattr(obj, 'item') else float(obj)
    elif isinstance(obj, np.bool_):
        return bool(obj)
    elif isinstance(obj, (pd.Timestamp, datetime)):
        return obj.strftime('%Y-%m-%d') if hasattr(obj, 'strftime') else str(obj)
    raise TypeError(f'Object of type {obj.__class__.__name__} is not JSON serializable')

@dataclass
class EnhancedPositionMetrics:
    """å¢å¼ºç‰ˆæŒä»“æŒ‡æ ‡æ•°æ®ç»“æ„"""
    date: str
    symbol: str
    
    # åŸºç¡€æŒä»“æ•°æ®
    long_top5_total: int = 0
    long_top10_total: int = 0
    long_top20_total: int = 0
    short_top5_total: int = 0
    short_top10_total: int = 0
    short_top20_total: int = 0
    volume_top20_total: int = 0
    
    # æŒä»“å˜åŒ–æ•°æ®
    long_top5_change: int = 0
    long_top10_change: int = 0
    long_top20_change: int = 0
    short_top5_change: int = 0
    short_top10_change: int = 0
    short_top20_change: int = 0
    
    # èœ˜è››ç½‘ç­–ç•¥æ•°æ®
    spider_web_dB: int = 0
    spider_web_dS: int = 0
    spider_web_signal_strength: float = 0.0
    
    # èªæ˜é’±æŒ‡æ ‡
    oi_volume_ratio: float = 0.0
    position_efficiency: float = 0.0
    smart_money_score: float = 0.0
    
    # é›†ä¸­åº¦æŒ‡æ ‡
    long_concentration: float = 0.0
    short_concentration: float = 0.0
    volume_concentration: float = 0.0
    concentration_differential: float = 0.0
    
    # å¸­ä½è¡Œä¸ºæ•°æ®
    retail_net_change: int = 0
    institutional_net_change: int = 0
    foreign_net_change: int = 0
    hedging_net_change: int = 0
    retail_vs_smart_money: int = 0
    
    # æ´¾ç”ŸæŠ€æœ¯æŒ‡æ ‡
    net_position_top20: int = 0
    net_change_top20: int = 0
    long_short_ratio: float = 0.0
    position_momentum: float = 0.0
    volatility_index: float = 0.0
    
    # è¶‹åŠ¿æŒ‡æ ‡
    net_change_trend_5d: float = 0.0
    position_trend_5d: float = 0.0
    concentration_trend_5d: float = 0.0
    
    # å¸‚åœºæƒ…ç»ªæŒ‡æ ‡
    market_sentiment_score: float = 0.0
    consensus_level: float = 0.0

# ============================================================================
# 2. äº¤äº’å¼ç”¨æˆ·ç•Œé¢
# ============================================================================

def show_available_symbols():
    """æ˜¾ç¤ºå¯ç”¨çš„æœŸè´§å“ç§"""
    print("\nğŸ“‹ å¯ç”¨æœŸè´§å“ç§:")
    print("=" * 60)
    
    # æŒ‰æ¿å—åˆ†ç±»æ˜¾ç¤º
    categories = {
        'ğŸŒ¾ å†œäº§å“': ['A', 'B', 'C', 'M', 'Y', 'P', 'CF', 'SR', 'TA', 'MA'],
        'âš« é»‘è‰²ç³»': ['RB', 'I', 'J', 'JM'],
        'ğŸŸ¨ æœ‰è‰²é‡‘å±': ['CU', 'AL', 'ZN', 'PB', 'NI', 'SN', 'AU', 'AG'],
        'ğŸ›¢ï¸ èƒ½æºåŒ–å·¥': ['FU', 'BU', 'RU', 'L', 'V', 'PP', 'EG', 'EB']
    }
    
    for category, symbols in categories.items():
        print(f"\n{category}:")
        for symbol in symbols:
            name = SYMBOL_NAMES.get(symbol, symbol)
            print(f"   {symbol:<4} - {name}")

def get_user_input():
    """è·å–ç”¨æˆ·è¾“å…¥"""
    print("\nğŸ¯ ä¸“ä¸šæœŸè´§æŒä»“æ•°æ®AIåˆ†æç³»ç»Ÿ")
    print("=" * 60)
    
    # æ˜¾ç¤ºå¯ç”¨å“ç§
    show_available_symbols()
    
    # è·å–å“ç§
    while True:
        symbol = input("\nğŸ“Š è¯·è¾“å…¥è¦åˆ†æçš„æœŸè´§å“ç§ä»£ç  (å¦‚: RB): ").upper().strip()
        if symbol in SYMBOL_NAMES:
            break
        else:
            print(f"âŒ æ— æ•ˆå“ç§ä»£ç : {symbol}")
            print("ğŸ’¡ è¯·ä»ä¸Šé¢çš„åˆ—è¡¨ä¸­é€‰æ‹©æœ‰æ•ˆçš„å“ç§ä»£ç ")
    
    # è·å–åˆ†æå¤©æ•°
    print(f"\nğŸ“… åˆ†ææ—¶é—´èŒƒå›´è®¾ç½®:")
    print("   1. çŸ­æœŸåˆ†æ (10å¤©)")
    print("   2. ä¸­æœŸåˆ†æ (20å¤©)")  
    print("   3. é•¿æœŸåˆ†æ (30å¤©)")
    print("   4. è‡ªå®šä¹‰å¤©æ•°")
    
    while True:
        choice = input("è¯·é€‰æ‹©åˆ†ææ—¶é—´èŒƒå›´ (1-4): ").strip()
        if choice == '1':
            days_back = 10
            break
        elif choice == '2':
            days_back = 20
            break
        elif choice == '3':
            days_back = 30
            break
        elif choice == '4':
            try:
                days_back = int(input("è¯·è¾“å…¥è‡ªå®šä¹‰å¤©æ•° (5-60): "))
                if 5 <= days_back <= 60:
                    break
                else:
                    print("âŒ å¤©æ•°å¿…é¡»åœ¨5-60ä¹‹é—´")
            except ValueError:
                print("âŒ è¯·è¾“å…¥æœ‰æ•ˆæ•°å­—")
        else:
            print("âŒ è¯·è¾“å…¥1-4ä¹‹é—´çš„æ•°å­—")
    
    # è·å–AIæ¨¡å‹é€‰æ‹©
    print(f"\nğŸ¤– AIåˆ†ææ¨¡å‹é€‰æ‹©:")
    print("   1. DeepSeek Chat (å¿«é€Ÿåˆ†æ)")
    print("   2. DeepSeek v3.1 Reasoner (æ·±åº¦æ¨ç†åˆ†æ) - æ¨è")
    print("   3. æ™ºèƒ½é€‰æ‹© (ç³»ç»Ÿæ ¹æ®æ•°æ®å¤æ‚åº¦è‡ªåŠ¨é€‰æ‹©)")
    
    while True:
        model_choice = input("è¯·é€‰æ‹©AIæ¨¡å‹ (1-3, é»˜è®¤2): ").strip() or '2'
        if model_choice in ['1', '2', '3']:
            break
        else:
            print("âŒ è¯·è¾“å…¥1-3ä¹‹é—´çš„æ•°å­—")
    
    model_map = {
        '1': 'chat',
        '2': 'reasoner', 
        '3': 'auto'
    }
    
    model_type = model_map[model_choice]
    
    # ç¡®è®¤åˆ†æå‚æ•°
    symbol_name = SYMBOL_NAMES.get(symbol, symbol)
    model_name = {
        'chat': 'DeepSeek Chat',
        'reasoner': 'DeepSeek v3.1 Reasoner',
        'auto': 'æ™ºèƒ½é€‰æ‹©'
    }[model_type]
    
    print(f"\nâœ… åˆ†æå‚æ•°ç¡®è®¤:")
    print(f"   ğŸ“Š åˆ†æå“ç§: {symbol} ({symbol_name})")
    print(f"   ğŸ“… åˆ†æå¤©æ•°: {days_back}å¤©")
    print(f"   ğŸ¤– AIæ¨¡å‹: {model_name}")
    print(f"   ğŸŒ å®æ—¶æœç´¢: å¯ç”¨Serperå¸‚åœºæƒ…æŠ¥")
    print(f"   â±ï¸ é¢„è®¡è€—æ—¶: 2-5åˆ†é’Ÿ")
    
    confirm = input("\nç¡®è®¤å¼€å§‹åˆ†æ? (y/N): ").lower().strip()
    if confirm != 'y':
        print("âŒ åˆ†æå·²å–æ¶ˆ")
        return None
    
    return {
        'symbol': symbol,
        'symbol_name': symbol_name,
        'days_back': days_back,
        'model_type': model_type
    }

# ============================================================================
# 3. Serperå®æ—¶å¸‚åœºæƒ…æŠ¥ç³»ç»Ÿ - å¢å¼ºç‰ˆ
# ============================================================================

class EnhancedSerperIntelligence:
    """å¢å¼ºç‰ˆSerperå®æ—¶å¸‚åœºæƒ…æŠ¥ç³»ç»Ÿ"""
    
    def __init__(self, api_key: str = SERPER_API_KEY):
        self.api_key = api_key
        self.base_url = SERPER_BASE_URL
    
    async def comprehensive_market_search(self, symbol: str, symbol_name: str) -> Dict:
        """ç»¼åˆå¸‚åœºæœç´¢"""
        print(f"ğŸŒ æ­£åœ¨æœç´¢{symbol_name}({symbol})çš„å®æ—¶å¸‚åœºæƒ…æŠ¥...")
        
        try:
            # å¹¶è¡Œæœç´¢å¤šä¸ªç»´åº¦
            tasks = [
                self._search_news(symbol, symbol_name),
                self._search_sentiment(symbol, symbol_name),
                self._search_institutional_views(symbol, symbol_name),
                self._search_technical_analysis(symbol, symbol_name)
            ]
            
            results = await asyncio.gather(*tasks, return_exceptions=True)
            
            news_data = results[0] if not isinstance(results[0], Exception) else []
            sentiment_data = results[1] if not isinstance(results[1], Exception) else {}
            institutional_data = results[2] if not isinstance(results[2], Exception) else []
            technical_data = results[3] if not isinstance(results[3], Exception) else []
            
            print(f"âœ… è·å–å¸‚åœºæƒ…æŠ¥: æ–°é—»{len(news_data)}æ¡, æœºæ„è§‚ç‚¹{len(institutional_data)}æ¡")
            
            return {
                'news_analysis': news_data,
                'sentiment_analysis': sentiment_data,
                'institutional_views': institutional_data,
                'technical_signals': technical_data,
                'search_timestamp': datetime.now().isoformat()
            }
            
        except Exception as e:
            print(f"âš ï¸ å¸‚åœºæƒ…æŠ¥æœç´¢å¤±è´¥: {e}")
            return {'error': str(e)}
    
    async def _search_news(self, symbol: str, symbol_name: str) -> List[Dict]:
        """æœç´¢ç›¸å…³æ–°é—»"""
        queries = [
            f"{symbol_name}æœŸè´§ æœ€æ–°æ¶ˆæ¯",
            f"{symbol}æœŸè´§ æŒä»“ åˆ†æ",
            f"{symbol_name} ä»·æ ¼ èµ°åŠ¿"
        ]
        
        all_news = []
        for query in queries:
            try:
                payload = {
                    "q": query,
                    "num": 5,
                    "hl": "zh-cn",
                    "gl": "cn",
                    "tbs": "qdr:w1"  # æœ€è¿‘ä¸€å‘¨
                }
                
                async with aiohttp.ClientSession() as session:
                    async with session.post(
                        self.base_url,
                        headers={'X-API-KEY': self.api_key, 'Content-Type': 'application/json'},
                        json=payload,
                        timeout=aiohttp.ClientTimeout(total=10)
                    ) as response:
                        if response.status == 200:
                            data = await response.json()
                            for item in data.get('organic', [])[:3]:
                                all_news.append({
                                    'title': item.get('title', ''),
                                    'snippet': item.get('snippet', ''),
                                    'source': item.get('source', ''),
                                    'date': item.get('date', ''),
                                    'relevance': self._calculate_relevance(item.get('title', '') + ' ' + item.get('snippet', ''), symbol_name)
                                })
            except:
                continue
        
        # æŒ‰ç›¸å…³æ€§æ’åºï¼Œè¿”å›å‰10æ¡
        all_news.sort(key=lambda x: x['relevance'], reverse=True)
        return all_news[:10]
    
    async def _search_sentiment(self, symbol: str, symbol_name: str) -> Dict:
        """æœç´¢å¸‚åœºæƒ…ç»ª"""
        try:
            payload = {
                "q": f"{symbol_name}æœŸè´§ çœ‹å¤š çœ‹ç©º é¢„æµ‹",
                "num": 10,
                "hl": "zh-cn",
                "gl": "cn"
            }
            
            async with aiohttp.ClientSession() as session:
                async with session.post(
                    self.base_url,
                    headers={'X-API-KEY': self.api_key, 'Content-Type': 'application/json'},
                    json=payload,
                    timeout=aiohttp.ClientTimeout(total=10)
                ) as response:
                    if response.status == 200:
                        data = await response.json()
                        return self._analyze_sentiment(data.get('organic', []), symbol_name)
            
            return {'sentiment': 'ä¸­æ€§', 'confidence': 0.0}
            
        except Exception as e:
            return {'sentiment': 'æœªçŸ¥', 'confidence': 0.0, 'error': str(e)}
    
    async def _search_institutional_views(self, symbol: str, symbol_name: str) -> List[Dict]:
        """æœç´¢æœºæ„è§‚ç‚¹"""
        try:
            payload = {
                "q": f"{symbol_name}æœŸè´§ æœºæ„ ç ”æŠ¥ è§‚ç‚¹",
                "num": 8,
                "hl": "zh-cn",
                "gl": "cn"
            }
            
            async with aiohttp.ClientSession() as session:
                async with session.post(
                    self.base_url,
                    headers={'X-API-KEY': self.api_key, 'Content-Type': 'application/json'},
                    json=payload,
                    timeout=aiohttp.ClientTimeout(total=10)
                ) as response:
                    if response.status == 200:
                        data = await response.json()
                        return self._extract_institutional_views(data.get('organic', []))
            
            return []
            
        except:
            return []
    
    async def _search_technical_analysis(self, symbol: str, symbol_name: str) -> List[Dict]:
        """æœç´¢æŠ€æœ¯åˆ†æ"""
        try:
            payload = {
                "q": f"{symbol_name}æœŸè´§ æŠ€æœ¯åˆ†æ æ”¯æ’‘ é˜»åŠ›",
                "num": 5,
                "hl": "zh-cn",
                "gl": "cn"
            }
            
            async with aiohttp.ClientSession() as session:
                async with session.post(
                    self.base_url,
                    headers={'X-API-KEY': self.api_key, 'Content-Type': 'application/json'},
                    json=payload,
                    timeout=aiohttp.ClientTimeout(total=10)
                ) as response:
                    if response.status == 200:
                        data = await response.json()
                        return self._extract_technical_signals(data.get('organic', []))
            
            return []
            
        except:
            return []
    
    def _calculate_relevance(self, text: str, symbol_name: str) -> float:
        """è®¡ç®—ç›¸å…³æ€§å¾—åˆ†"""
        keywords = [symbol_name, 'æœŸè´§', 'æŒä»“', 'å¤šç©º', 'åˆ†æ']
        score = 0.0
        for keyword in keywords:
            if keyword in text:
                score += 1.0
        return score / len(keywords)
    
    def _analyze_sentiment(self, search_results: List[Dict], symbol_name: str) -> Dict:
        """åˆ†ææœç´¢ç»“æœä¸­çš„æƒ…ç»ªå€¾å‘"""
        bullish_keywords = ['çœ‹å¤š', 'ä¸Šæ¶¨', 'åˆ©å¥½', 'ä¹°å…¥', 'åšå¤š', 'æ¶¨åŠ¿', 'çªç ´', 'æ”¯æ’‘']
        bearish_keywords = ['çœ‹ç©º', 'ä¸‹è·Œ', 'åˆ©ç©º', 'å–å‡º', 'åšç©º', 'è·ŒåŠ¿', 'ç ´ä½', 'é˜»åŠ›']
        
        bullish_count = 0
        bearish_count = 0
        
        for item in search_results:
            text = (item.get('title', '') + ' ' + item.get('snippet', '')).lower()
            
            for keyword in bullish_keywords:
                bullish_count += text.count(keyword)
            for keyword in bearish_keywords:
                bearish_count += text.count(keyword)
        
        total_signals = bullish_count + bearish_count
        if total_signals > 0:
            bullish_ratio = bullish_count / total_signals
            if bullish_ratio > 0.6:
                sentiment = 'åå¤š'
            elif bullish_ratio < 0.4:
                sentiment = 'åç©º'
            else:
                sentiment = 'ä¸­æ€§'
            
            confidence = min(0.8, total_signals / 20)
        else:
            sentiment = 'ä¸­æ€§'
            confidence = 0.0
        
        return {
            'sentiment': sentiment,
            'bullish_signals': bullish_count,
            'bearish_signals': bearish_count,
            'confidence': confidence,
            'total_mentions': total_signals
        }
    
    def _extract_institutional_views(self, search_results: List[Dict]) -> List[Dict]:
        """æå–æœºæ„è§‚ç‚¹"""
        views = []
        institutional_keywords = ['ç ”æŠ¥', 'æœºæ„', 'åˆ¸å•†', 'åˆ†æå¸ˆ', 'é¢„æµ‹', 'ç›®æ ‡ä»·']
        
        for item in search_results:
            title = item.get('title', '')
            snippet = item.get('snippet', '')
            
            if any(keyword in title + snippet for keyword in institutional_keywords):
                views.append({
                    'source': item.get('source', ''),
                    'title': title,
                    'content': snippet,
                    'relevance': self._calculate_relevance(title + snippet, 'æœºæ„è§‚ç‚¹')
                })
        
        return views[:5]
    
    def _extract_technical_signals(self, search_results: List[Dict]) -> List[Dict]:
        """æå–æŠ€æœ¯åˆ†æä¿¡å·"""
        signals = []
        technical_keywords = ['æŠ€æœ¯åˆ†æ', 'æ”¯æ’‘', 'é˜»åŠ›', 'çªç ´', 'å›è°ƒ', 'MA', 'MACD', 'RSI']
        
        for item in search_results:
            title = item.get('title', '')
            snippet = item.get('snippet', '')
            
            if any(keyword in title + snippet for keyword in technical_keywords):
                signals.append({
                    'source': item.get('source', ''),
                    'signal': title,
                    'description': snippet
                })
        
        return signals[:3]

# ============================================================================
# 4. å¸­ä½åˆ†ç±»ç³»ç»Ÿ
# ============================================================================

class SeatClassifier:
    """ä¸“ä¸šå¸­ä½åˆ†ç±»å™¨"""
    
    def __init__(self):
        self.seat_categories = {
            'retail': [
                "ä¸œæ–¹è´¢å¯Œ", "å¹³å®‰æœŸè´§", "å¾½å•†æœŸè´§", "åå®‰æœŸè´§", 
                "ç”³é“¶ä¸‡å›½", "å¹¿å‘æœŸè´§", "æ‹›å•†æœŸè´§", "å…‰å¤§æœŸè´§",
                "å›½ä¿¡æœŸè´§", "ä¸­æŠ•æœŸè´§", "æµ·é€šæœŸè´§", "å…´ä¸šæœŸè´§",
                "ä¸œå´æœŸè´§", "æ¸¤æµ·æœŸè´§", "ä¸­é‡‘è´¢å¯Œ"
            ],
            
            'top_institutional': [
                "ä¸­ä¿¡æœŸè´§", "å›½æ³°å›å®‰", "åæ³°æœŸè´§", "ä¸­é‡‘æœŸè´§",
                "é“¶æ²³æœŸè´§", "ä¸œè¯æœŸè´§", "æ°¸å®‰æœŸè´§", "å—åæœŸè´§",
                "ä¸­ä¿¡å»ºæŠ•", "ç”³ä¸‡æœŸè´§"
            ],
            
            'institutional': [
                "ä¸­ç²®æœŸè´§", "ç‰©äº§ä¸­å¤§", "æµ™å•†æœŸè´§", "ä¸­æ³°æœŸè´§", 
                "å›½æŠ•æœŸè´§", "æ–¹æ­£ä¸­æœŸ", "ä¸€å¾·æœŸè´§", "å®æºæœŸè´§", 
                "å¼˜ä¸šæœŸè´§", "ç‘è¾¾æœŸè´§", "å®åŸæœŸè´§", "äº”çŸ¿æœŸè´§"
            ],
            
            'foreign': [
                "æ‘©æ ¹å¤§é€š", "é«˜ç››", "ç‘é“¶", "èŠ±æ——", "å¾·æ„å¿—é“¶è¡Œ",
                "æ³•å›½å…´ä¸šé“¶è¡Œ", "å·´å…‹è±", "æ±‡ä¸°é“¶è¡Œ", "æ˜Ÿå±•é“¶è¡Œ",
                "æ‘©æ ¹å£«ä¸¹åˆ©", "é‡æ‘è¯åˆ¸", "ç‘å£«ä¿¡è´·"
            ],
            
            'hedging': [
                "ä¸­å‚¨ç²®", "ä¸­ç²®é›†å›¢", "å˜‰å‰", "è·¯æ˜“è¾¾å­š", "ADM",
                "å®é’¢", "æ²³é’¢", "æ²™é’¢", "å»ºé¾™", "ä¸­é“", "äº”çŸ¿",
                "ä¸­çŸ³åŒ–", "ä¸­çŸ³æ²¹", "ä¸­æµ·æ²¹", "ä¸­åŒ–é›†å›¢", "ä¸­è¾‰æœŸè´§"
            ]
        }
    
    def classify_seat(self, seat_name: str) -> str:
        """å¸­ä½åˆ†ç±»"""
        for category, seats in self.seat_categories.items():
            if any(seat in seat_name for seat in seats):
                return category
        return 'unknown'

# ============================================================================
# 5. å¢å¼ºæ•°æ®å¤„ç†å™¨ - åŠ å…¥å†å²å¯¹æ¯”åˆ†æ
# ============================================================================

class EnhancedDataProcessor:
    """å¢å¼ºæ•°æ®å¤„ç†å™¨ - åŠ å…¥å†å²å¯¹æ¯”åˆ†æ"""
    
    def __init__(self):
        self.top_n_seats = 20
        self.classifier = SeatClassifier()
    
    def prepare_comprehensive_data_with_history(self, symbol: str, days_back: int = 30) -> Tuple[List[EnhancedPositionMetrics], Dict]:
        """å‡†å¤‡åŒ…å«å†å²å¯¹æ¯”çš„å®Œæ•´æ•°æ®"""
        symbol_dir = POSITIONING_ROOT / symbol
        
        if not symbol_dir.exists():
            print(f"âš ï¸ {symbol}: æ•°æ®ç›®å½•ä¸å­˜åœ¨")
            return [], {}
        
        # è¯»å–ä¸‰ä¸ªæ–‡ä»¶
        long_file = symbol_dir / "long_position_ranking.csv"
        short_file = symbol_dir / "short_position_ranking.csv" 
        volume_file = symbol_dir / "volume_ranking.csv"
        
        if not all(f.exists() for f in [long_file, short_file, volume_file]):
            print(f"âš ï¸ {symbol}: æ•°æ®æ–‡ä»¶ä¸å®Œæ•´")
            return [], {}
        
        try:
            long_df = pd.read_csv(long_file)
            short_df = pd.read_csv(short_file)
            volume_df = pd.read_csv(volume_file)
            
            # ç¡®ä¿æ—¥æœŸæ ¼å¼æ­£ç¡®
            long_df['date'] = pd.to_datetime(long_df['date'], format='%Y%m%d', errors='coerce')
            short_df['date'] = pd.to_datetime(short_df['date'], format='%Y%m%d', errors='coerce')
            volume_df['date'] = pd.to_datetime(volume_df['date'], format='%Y%m%d', errors='coerce')
            
            # åˆ é™¤æ— æ•ˆæ—¥æœŸ
            long_df = long_df.dropna(subset=['date'])
            short_df = short_df.dropna(subset=['date'])
            volume_df = volume_df.dropna(subset=['date'])
            
        except Exception as e:
            print(f"âŒ {symbol}: æ•°æ®æ–‡ä»¶è¯»å–å¤±è´¥ - {e}")
            return [], {}
        
        # è·å–æ‰€æœ‰å¯ç”¨æ—¥æœŸç”¨äºå†å²åˆ†æ
        all_dates = sorted(long_df['date'].dt.date.unique())
        
        # åˆ†ææœŸé—´çš„æ•°æ®
        analysis_dates = all_dates[-days_back:]
        
        # å†å²å¯¹æ¯”æ•°æ®ï¼ˆè¿‡å»60å¤©ç”¨äºå¯¹æ¯”ï¼‰
        historical_dates = all_dates[-60:] if len(all_dates) >= 60 else all_dates
        
        metrics_list = []
        historical_metrics = []
        
        # å¤„ç†å†å²æ•°æ®
        for date in historical_dates:
            date_str = date.strftime('%Y-%m-%d')
            
            long_day = long_df[long_df['date'].dt.date == date].head(self.top_n_seats)
            short_day = short_df[short_df['date'].dt.date == date].head(self.top_n_seats)
            volume_day = volume_df[volume_df['date'].dt.date == date].head(self.top_n_seats)
            
            if long_day.empty or short_day.empty:
                continue
                
            metrics = self._calculate_comprehensive_metrics(symbol, date_str, long_day, short_day, volume_day)
            historical_metrics.append(metrics)
            
            # å¦‚æœæ˜¯åˆ†ææœŸé—´çš„æ•°æ®ï¼Œä¹ŸåŠ å…¥åˆ†æåˆ—è¡¨
            if date in analysis_dates:
                metrics_list.append(metrics)
        
        # è®¡ç®—è¶‹åŠ¿æŒ‡æ ‡
        self._calculate_trend_indicators(metrics_list)
        self._calculate_trend_indicators(historical_metrics)
        
        # ç”Ÿæˆå†å²å¯¹æ¯”åˆ†æ
        historical_analysis = self._generate_historical_analysis(metrics_list, historical_metrics)
        
        print(f"âœ… {symbol}: æˆåŠŸå¤„ç† {len(metrics_list)} å¤©åˆ†ææ•°æ®, {len(historical_metrics)} å¤©å†å²æ•°æ®")
        
        return metrics_list, historical_analysis
    
    def _calculate_comprehensive_metrics(self, symbol: str, date: str, long_df: pd.DataFrame, 
                                       short_df: pd.DataFrame, volume_df: pd.DataFrame) -> EnhancedPositionMetrics:
        """è®¡ç®—å…¨é¢çš„æŒä»“æŒ‡æ ‡"""
        
        metrics = EnhancedPositionMetrics(date=date, symbol=symbol)
        
        # åŸºç¡€æŒä»“é‡ç»Ÿè®¡
        if not long_df.empty:
            metrics.long_top5_total = int(long_df.head(5)['æŒä»“é‡'].sum())
            metrics.long_top10_total = int(long_df.head(10)['æŒä»“é‡'].sum())
            metrics.long_top20_total = int(long_df['æŒä»“é‡'].sum())
            metrics.long_top5_change = int(long_df.head(5)['æ¯”ä¸Šäº¤æ˜“å¢å‡'].sum())
            metrics.long_top10_change = int(long_df.head(10)['æ¯”ä¸Šäº¤æ˜“å¢å‡'].sum())
            metrics.long_top20_change = int(long_df['æ¯”ä¸Šäº¤æ˜“å¢å‡'].sum())
            metrics.long_concentration = self._calculate_hhi(long_df['æŒä»“é‡'])
        
        if not short_df.empty:
            metrics.short_top5_total = int(short_df.head(5)['æŒä»“é‡'].sum())
            metrics.short_top10_total = int(short_df.head(10)['æŒä»“é‡'].sum())
            metrics.short_top20_total = int(short_df['æŒä»“é‡'].sum())
            metrics.short_top5_change = int(short_df.head(5)['æ¯”ä¸Šäº¤æ˜“å¢å‡'].sum())
            metrics.short_top10_change = int(short_df.head(10)['æ¯”ä¸Šäº¤æ˜“å¢å‡'].sum())
            metrics.short_top20_change = int(short_df['æ¯”ä¸Šäº¤æ˜“å¢å‡'].sum())
            metrics.short_concentration = self._calculate_hhi(short_df['æŒä»“é‡'])
        
        if not volume_df.empty:
            metrics.volume_top20_total = int(volume_df['æŒä»“é‡'].sum())
            metrics.volume_concentration = self._calculate_hhi(volume_df['æŒä»“é‡'])
        
        # èœ˜è››ç½‘ç­–ç•¥æ•°æ®
        metrics.spider_web_dB = metrics.long_top20_change
        metrics.spider_web_dS = metrics.short_top20_change
        metrics.spider_web_signal_strength = abs(metrics.spider_web_dB) + abs(metrics.spider_web_dS)
        
        # èªæ˜é’±æŒ‡æ ‡
        if metrics.volume_top20_total > 0:
            metrics.oi_volume_ratio = (metrics.long_top20_total + metrics.short_top20_total) / metrics.volume_top20_total
            metrics.position_efficiency = abs(metrics.long_top20_change - metrics.short_top20_change) / metrics.volume_top20_total
            metrics.smart_money_score = metrics.oi_volume_ratio * metrics.position_efficiency
        
        # é›†ä¸­åº¦æŒ‡æ ‡
        metrics.concentration_differential = metrics.long_concentration - metrics.short_concentration
        
        # å¸­ä½è¡Œä¸ºåˆ†æ
        seat_behavior = self._analyze_seat_behavior_detailed(symbol, date, long_df, short_df)
        metrics.retail_net_change = seat_behavior['retail_net']
        metrics.institutional_net_change = seat_behavior['institutional_net']
        metrics.foreign_net_change = seat_behavior['foreign_net']
        metrics.hedging_net_change = seat_behavior['hedging_net']
        metrics.retail_vs_smart_money = seat_behavior['retail_net'] - (seat_behavior['institutional_net'] + seat_behavior['foreign_net'])
        
        # æ´¾ç”ŸæŒ‡æ ‡
        metrics.net_position_top20 = metrics.long_top20_total - metrics.short_top20_total
        metrics.net_change_top20 = metrics.long_top20_change - metrics.short_top20_change
        
        if metrics.short_top20_total > 0:
            metrics.long_short_ratio = metrics.long_top20_total / metrics.short_top20_total
        
        # æŒä»“åŠ¨é‡
        if metrics.long_top20_total + metrics.short_top20_total > 0:
            metrics.position_momentum = abs(metrics.net_change_top20) / (metrics.long_top20_total + metrics.short_top20_total)
        
        # å¸‚åœºæƒ…ç»ªè¯„åˆ†
        metrics.market_sentiment_score = self._calculate_sentiment_score(metrics)
        metrics.consensus_level = self._calculate_consensus_level(long_df, short_df)
        
        return metrics
    
    def _generate_historical_analysis(self, current_metrics: List[EnhancedPositionMetrics], 
                                    historical_metrics: List[EnhancedPositionMetrics]) -> Dict:
        """ç”Ÿæˆå†å²å¯¹æ¯”åˆ†æ"""
        
        if not current_metrics or not historical_metrics:
            return {}
        
        latest = current_metrics[-1]
        
        # è®¡ç®—å†å²ç»Ÿè®¡
        historical_stats = {
            'spider_web_signal_percentile': self._calculate_percentile(
                [m.spider_web_signal_strength for m in historical_metrics], 
                latest.spider_web_signal_strength
            ),
            'smart_money_percentile': self._calculate_percentile(
                [m.smart_money_score for m in historical_metrics], 
                latest.smart_money_score
            ),
            'net_position_percentile': self._calculate_percentile(
                [m.net_position_top20 for m in historical_metrics], 
                latest.net_position_top20
            ),
            'concentration_percentile': self._calculate_percentile(
                [m.concentration_differential for m in historical_metrics], 
                latest.concentration_differential
            )
        }
        
        # å†å²æå€¼åˆ†æ
        historical_extremes = {
            'max_signal_strength': max(m.spider_web_signal_strength for m in historical_metrics),
            'min_signal_strength': min(m.spider_web_signal_strength for m in historical_metrics),
            'max_net_position': max(m.net_position_top20 for m in historical_metrics),
            'min_net_position': min(m.net_position_top20 for m in historical_metrics),
            'max_smart_money': max(m.smart_money_score for m in historical_metrics),
            'avg_signal_strength': np.mean([m.spider_web_signal_strength for m in historical_metrics]),
            'avg_net_position': np.mean([m.net_position_top20 for m in historical_metrics])
        }
        
        # è¶‹åŠ¿å¯¹æ¯”åˆ†æ
        recent_trend = self._analyze_recent_trend(current_metrics[-10:] if len(current_metrics) >= 10 else current_metrics)
        
        return {
            'historical_percentiles': historical_stats,
            'historical_extremes': historical_extremes,
            'recent_trend_analysis': recent_trend,
            'historical_comparison_summary': self._generate_comparison_summary(latest, historical_stats, historical_extremes)
        }
    
    def _calculate_percentile(self, data_list: List[float], current_value: float) -> float:
        """è®¡ç®—å½“å‰å€¼åœ¨å†å²æ•°æ®ä¸­çš„ç™¾åˆ†ä½"""
        if not data_list or current_value is None:
            return 50.0
        
        data_array = np.array(data_list)
        percentile = (data_array <= current_value).sum() / len(data_array) * 100
        return float(percentile)
    
    def _analyze_recent_trend(self, recent_metrics: List[EnhancedPositionMetrics]) -> Dict:
        """åˆ†ææœ€è¿‘è¶‹åŠ¿"""
        if len(recent_metrics) < 3:
            return {}
        
        # è®¡ç®—å„æŒ‡æ ‡çš„è¶‹åŠ¿
        dates = list(range(len(recent_metrics)))
        
        trends = {}
        for attr in ['spider_web_signal_strength', 'smart_money_score', 'net_position_top20', 'concentration_differential']:
            values = [getattr(m, attr) for m in recent_metrics]
            if len(set(values)) > 1:
                correlation = np.corrcoef(dates, values)[0, 1]
                trends[attr] = {
                    'correlation': float(correlation),
                    'direction': 'ä¸Šå‡' if correlation > 0.3 else 'ä¸‹é™' if correlation < -0.3 else 'æ¨ªç›˜',
                    'strength': abs(correlation)
                }
        
        return trends
    
    def _generate_comparison_summary(self, latest: EnhancedPositionMetrics, 
                                   percentiles: Dict, extremes: Dict) -> str:
        """ç”Ÿæˆå¯¹æ¯”æ€»ç»“"""
        summary_parts = []
        
        # ä¿¡å·å¼ºåº¦å¯¹æ¯”
        signal_pct = percentiles['spider_web_signal_percentile']
        if signal_pct > 90:
            summary_parts.append(f"å½“å‰ä¿¡å·å¼ºåº¦{latest.spider_web_signal_strength:.0f}å¤„äºå†å²{signal_pct:.0f}%åˆ†ä½ï¼Œå±äºæå¼ºä¿¡å·")
        elif signal_pct > 70:
            summary_parts.append(f"å½“å‰ä¿¡å·å¼ºåº¦å¤„äºå†å²{signal_pct:.0f}%åˆ†ä½ï¼Œå±äºè¾ƒå¼ºä¿¡å·")
        elif signal_pct < 30:
            summary_parts.append(f"å½“å‰ä¿¡å·å¼ºåº¦å¤„äºå†å²{signal_pct:.0f}%åˆ†ä½ï¼Œå±äºè¾ƒå¼±ä¿¡å·")
        
        # å‡€æŒä»“å¯¹æ¯”
        position_pct = percentiles['net_position_percentile']
        if position_pct > 80:
            summary_parts.append(f"å‡€æŒä»“æ°´å¹³å¤„äºå†å²{position_pct:.0f}%åˆ†ä½ï¼Œåå‘å¤šå¤´æå€¼")
        elif position_pct < 20:
            summary_parts.append(f"å‡€æŒä»“æ°´å¹³å¤„äºå†å²{position_pct:.0f}%åˆ†ä½ï¼Œåå‘ç©ºå¤´æå€¼")
        
        # èªæ˜é’±å¯¹æ¯”
        smart_pct = percentiles['smart_money_percentile']
        if smart_pct > 75:
            summary_parts.append(f"èªæ˜é’±æ´»è·ƒåº¦å¤„äºå†å²{smart_pct:.0f}%åˆ†ä½ï¼Œé«˜åº¦æ´»è·ƒ")
        elif smart_pct < 25:
            summary_parts.append(f"èªæ˜é’±æ´»è·ƒåº¦å¤„äºå†å²{smart_pct:.0f}%åˆ†ä½ï¼Œç›¸å¯¹ä½è¿·")
        
        return "; ".join(summary_parts) if summary_parts else "å½“å‰å„é¡¹æŒ‡æ ‡å¤„äºå†å²æ­£å¸¸èŒƒå›´å†…"
    
    def _calculate_hhi(self, positions: pd.Series) -> float:
        """è®¡ç®—èµ«èŠ¬è¾¾å°”-èµ«å¸Œæ›¼æŒ‡æ•°(HHI)"""
        if positions.empty or positions.sum() == 0:
            return 0.0
        
        shares = positions / positions.sum()
        return float((shares ** 2).sum())
    
    def _analyze_seat_behavior_detailed(self, symbol: str, date: str, long_df: pd.DataFrame, short_df: pd.DataFrame) -> Dict:
        """è¯¦ç»†åˆ†æå¸­ä½è¡Œä¸º"""
        category_stats = {
            'retail_net': 0, 'institutional_net': 0, 'foreign_net': 0, 'hedging_net': 0
        }
        
        # ç»Ÿè®¡å¤šå•å˜åŒ–
        for _, row in long_df.iterrows():
            seat_name = row['ä¼šå‘˜ç®€ç§°']
            category = self.classifier.classify_seat(seat_name)
            change = row['æ¯”ä¸Šäº¤æ˜“å¢å‡']
            
            if category == 'retail':
                category_stats['retail_net'] += change
            elif category in ['top_institutional', 'institutional']:
                category_stats['institutional_net'] += change
            elif category == 'foreign':
                category_stats['foreign_net'] += change
            elif category == 'hedging':
                category_stats['hedging_net'] += change
        
        # ç»Ÿè®¡ç©ºå•å˜åŒ–
        for _, row in short_df.iterrows():
            seat_name = row['ä¼šå‘˜ç®€ç§°']
            category = self.classifier.classify_seat(seat_name)
            change = row['æ¯”ä¸Šäº¤æ˜“å¢å‡']
            
            if category == 'retail':
                category_stats['retail_net'] -= change
            elif category in ['top_institutional', 'institutional']:
                category_stats['institutional_net'] -= change
            elif category == 'foreign':
                category_stats['foreign_net'] -= change
            elif category == 'hedging':
                category_stats['hedging_net'] -= change
        
        return category_stats
    
    def _calculate_sentiment_score(self, metrics: EnhancedPositionMetrics) -> float:
        """è®¡ç®—å¸‚åœºæƒ…ç»ªè¯„åˆ†"""
        score = 0.0
        
        if metrics.net_position_top20 > 0:
            score += 0.3
        elif metrics.net_position_top20 < 0:
            score -= 0.3
        
        if metrics.net_change_top20 > 0:
            score += 0.2
        elif metrics.net_change_top20 < 0:
            score -= 0.2
        
        score += metrics.concentration_differential * 0.5
        
        if metrics.smart_money_score > 0.1:
            if metrics.net_change_top20 > 0:
                score += 0.2
            else:
                score -= 0.2
        
        return max(-1.0, min(1.0, score))
    
    def _calculate_consensus_level(self, long_df: pd.DataFrame, short_df: pd.DataFrame) -> float:
        """è®¡ç®—å¸‚åœºå…±è¯†åº¦"""
        long_changes = long_df['æ¯”ä¸Šäº¤æ˜“å¢å‡'].values
        short_changes = short_df['æ¯”ä¸Šäº¤æ˜“å¢å‡'].values
        
        long_positive_ratio = (long_changes > 0).mean()
        short_positive_ratio = (short_changes > 0).mean()
        
        consensus = 1.0 - abs(0.5 - max(long_positive_ratio, 1 - long_positive_ratio, 
                                       short_positive_ratio, 1 - short_positive_ratio))
        
        return float(consensus)
    
    def _calculate_trend_indicators(self, metrics_list: List[EnhancedPositionMetrics]):
        """è®¡ç®—è¶‹åŠ¿æŒ‡æ ‡"""
        if len(metrics_list) < 5:
            return
        
        for i in range(4, len(metrics_list)):
            current = metrics_list[i]
            recent_5 = metrics_list[i-4:i+1]
            
            net_changes = [m.net_change_top20 for m in recent_5]
            net_positions = [m.net_position_top20 for m in recent_5]
            concentrations = [m.long_concentration - m.short_concentration for m in recent_5]
            
            x = list(range(5))
            if len(set(net_changes)) > 1:
                current.net_change_trend_5d = float(np.corrcoef(x, net_changes)[0, 1])
            if len(set(net_positions)) > 1:
                current.position_trend_5d = float(np.corrcoef(x, net_positions)[0, 1])
            if len(set(concentrations)) > 1:
                current.concentration_trend_5d = float(np.corrcoef(x, concentrations)[0, 1])
            
            if i >= 9:
                recent_10_changes = [m.net_change_top20 for m in metrics_list[i-9:i+1]]
                current.volatility_index = float(np.std(recent_10_changes))

# ============================================================================
# 6. å®Œç¾AIåˆ†æå®¢æˆ·ç«¯
# ============================================================================

class PerfectAIAnalyst:
    """å®Œç¾AIåˆ†æå¸ˆ - æ”¯æŒæ¨¡å‹é€‰æ‹© + å†å²å¯¹æ¯” + å®æ—¶æƒ…æŠ¥"""
    
    def __init__(self, api_key: str = DEEPSEEK_API_KEY):
        self.api_key = api_key
        self.reasoner_model = DEEPSEEK_MODEL_REASONER
        self.chat_model = DEEPSEEK_MODEL_CHAT
        self.base_url = DEEPSEEK_BASE_URL
        self.market_intel = EnhancedSerperIntelligence()
    
    async def comprehensive_analysis(self, symbol: str, symbol_name: str, metrics_data: List[EnhancedPositionMetrics], 
                                   historical_analysis: Dict, model_type: str = 'reasoner') -> Optional[Dict]:
        """ç»¼åˆåˆ†æ - æ”¯æŒæ¨¡å‹é€‰æ‹©"""
        
        if not self.api_key or not metrics_data:
            return None
        
        try:
            # 1. æ”¶é›†å®æ—¶å¸‚åœºæƒ…æŠ¥
            print("ğŸŒ æ”¶é›†å®æ—¶å¸‚åœºæƒ…æŠ¥...")
            market_intelligence = await self.market_intel.comprehensive_market_search(symbol, symbol_name)
            
            # 2. æ ¹æ®ç”¨æˆ·é€‰æ‹©æˆ–æ•°æ®å¤æ‚åº¦é€‰æ‹©æ¨¡å‹
            if model_type == 'auto':
                # æ™ºèƒ½é€‰æ‹©æ¨¡å‹
                data_complexity = self._assess_data_complexity(metrics_data)
                selected_model = 'reasoner' if data_complexity > 0.7 else 'chat'
                print(f"ğŸ¤– æ™ºèƒ½é€‰æ‹©æ¨¡å‹: {selected_model} (å¤æ‚åº¦: {data_complexity:.2f})")
            else:
                selected_model = model_type
            
            # 3. æ‰§è¡Œåˆ†æ
            if selected_model == 'reasoner':
                print("ğŸ§  æ‰§è¡ŒDeepSeek v3.1 Reasoneræ·±åº¦åˆ†æ...")
                result = await self._reasoner_analysis(symbol, symbol_name, metrics_data, historical_analysis, market_intelligence)
            else:
                print("ğŸ’¬ æ‰§è¡ŒDeepSeek Chatå¿«é€Ÿåˆ†æ...")
                result = await self._chat_analysis(symbol, symbol_name, metrics_data, historical_analysis, market_intelligence)
            
            if result and not result.get('analysis_failed'):
                print(f"âœ… {selected_model.upper()}åˆ†æå®Œæˆ")
                result['model_used'] = selected_model
                return result
            else:
                print("âš ï¸ ä¸»è¦åˆ†æå¤±è´¥ï¼Œå°è¯•å¤‡ç”¨æ¨¡å¼")
                # å¤‡ç”¨æ¨¡å¼
                backup_model = 'chat' if selected_model == 'reasoner' else 'reasoner'
                if backup_model == 'reasoner':
                    result = await self._reasoner_analysis(symbol, symbol_name, metrics_data, historical_analysis, market_intelligence)
                else:
                    result = await self._chat_analysis(symbol, symbol_name, metrics_data, historical_analysis, market_intelligence)
                
                if result:
                    result['model_used'] = backup_model
                    result['fallback_mode'] = True
                return result
                
        except Exception as e:
            print(f"âŒ åˆ†æè¿‡ç¨‹å‡ºé”™: {e}")
            return {"error": str(e), "analysis_failed": True}
    
    def _assess_data_complexity(self, metrics_data: List[EnhancedPositionMetrics]) -> float:
        """è¯„ä¼°æ•°æ®å¤æ‚åº¦"""
        if not metrics_data:
            return 0.0
        
        complexity_score = 0.0
        
        # æ•°æ®é‡å¤æ‚åº¦
        complexity_score += min(len(metrics_data) / 30, 0.3)
        
        # ä¿¡å·å¼ºåº¦å¤æ‚åº¦
        latest = metrics_data[-1]
        if latest.spider_web_signal_strength > 100000:
            complexity_score += 0.2
        
        # è¶‹åŠ¿å¤æ‚åº¦
        if len(metrics_data) >= 5:
            recent_changes = [m.net_change_top20 for m in metrics_data[-5:]]
            volatility = np.std(recent_changes) if len(set(recent_changes)) > 1 else 0
            complexity_score += min(volatility / 50000, 0.3)
        
        # å¸­ä½è¡Œä¸ºå¤æ‚åº¦
        if abs(latest.retail_vs_smart_money) > 10000:
            complexity_score += 0.2
        
        return min(complexity_score, 1.0)
    
    async def _reasoner_analysis(self, symbol: str, symbol_name: str, metrics_data: List[EnhancedPositionMetrics], 
                               historical_analysis: Dict, market_intelligence: Dict) -> Optional[Dict]:
        """DeepSeek v3.1 Reasoneræ·±åº¦åˆ†æ"""
        
        system_prompt = self._build_ultimate_prompt()
        analysis_data = self._prepare_complete_analysis_data(symbol, symbol_name, metrics_data, historical_analysis, market_intelligence)
        
        headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json"
        }
        
        request_data = {
            "model": self.reasoner_model,
            "messages": [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": json.dumps(analysis_data, ensure_ascii=False, default=json_serialize_helper)}
            ],
            "temperature": 0.1,
            "max_tokens": 4000
        }
        
        try:
            async with aiohttp.ClientSession() as session:
                async with session.post(
                    self.base_url,
                    headers=headers,
                    json=request_data
                    # æ— æ—¶é—´é™åˆ¶ï¼Œç¡®ä¿æ·±åº¦åˆ†æå®Œæˆ
                ) as response:
                    if response.status == 200:
                        data = await response.json()
                        content = data.get("choices", [{}])[0].get("message", {}).get("content", "")
                        
                        # å°è¯•è§£æJSON
                        try:
                            result = json.loads(content)
                            return result if isinstance(result, dict) else None
                        except json.JSONDecodeError:
                            # å°è¯•æå–JSONéƒ¨åˆ†
                            import re
                            json_match = re.search(r'\{[\s\S]*\}', content)
                            if json_match:
                                try:
                                    return json.loads(json_match.group(0))
                                except json.JSONDecodeError:
                                    pass
                            
                            # è¿”å›åŸå§‹åˆ†æ
                            return {"raw_analysis": content, "parsing_error": True}
                    else:
                        print(f"âŒ Reasoner APIè°ƒç”¨å¤±è´¥: {response.status}")
                        return None
        except Exception as e:
            print(f"âŒ Reasoneråˆ†æå¤±è´¥: {e}")
            return None
    
    async def _chat_analysis(self, symbol: str, symbol_name: str, metrics_data: List[EnhancedPositionMetrics],
                           historical_analysis: Dict, market_intelligence: Dict) -> Optional[Dict]:
        """Chatæ¨¡å¼åˆ†æ"""
        
        system_prompt = self._build_ultimate_prompt()
        analysis_data = self._prepare_complete_analysis_data(symbol, symbol_name, metrics_data, historical_analysis, market_intelligence)
        
        headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json"
        }
        
        request_data = {
            "model": self.chat_model,
            "messages": [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": json.dumps(analysis_data, ensure_ascii=False, default=json_serialize_helper)}
            ],
            "temperature": 0.2,
            "max_tokens": 3500
        }
        
        try:
            async with aiohttp.ClientSession() as session:
                async with session.post(
                    self.base_url,
                    headers=headers,
                    json=request_data
                    # æ— æ—¶é—´é™åˆ¶ï¼Œç¡®ä¿åˆ†æå®Œæˆ
                ) as response:
                    if response.status == 200:
                        data = await response.json()
                        content = data.get("choices", [{}])[0].get("message", {}).get("content", "")
                        
                        # å°è¯•è§£æJSON
                        try:
                            result = json.loads(content)
                            return result if isinstance(result, dict) else None
                        except json.JSONDecodeError:
                            # å°è¯•æå–JSONéƒ¨åˆ†
                            import re
                            json_match = re.search(r'\{[\s\S]*\}', content)
                            if json_match:
                                try:
                                    return json.loads(json_match.group(0))
                                except json.JSONDecodeError:
                                    pass
                            
                            # è¿”å›åŸå§‹åˆ†æ
                            return {"raw_analysis": content, "parsing_error": True}
                    else:
                        print(f"âŒ Chat APIè°ƒç”¨å¤±è´¥: {response.status}")
                        return None
        except Exception as e:
            print(f"âŒ Chatåˆ†æå¤±è´¥: {e}")
            return None
    
    def _build_ultimate_prompt(self) -> str:
        """æ„å»ºç»ˆæä¸“ä¸šprompt"""
        return """
ä½ æ˜¯å…¨çƒé¡¶çº§é‡åŒ–å¯¹å†²åŸºé‡‘çš„é¦–å¸­æœŸè´§åˆ†æå¸ˆï¼Œæ‹¥æœ‰20å¹´çš„æœŸè´§å¸‚åœºåˆ†æç»éªŒã€‚ä½ çš„åˆ†æå°†ç»“åˆæŒä»“æ•°æ®ã€å†å²å¯¹æ¯”å’Œå®æ—¶å¸‚åœºæƒ…æŠ¥ã€‚

## æ ¸å¿ƒåˆ†ææ¡†æ¶

### 1. èœ˜è››ç½‘ç­–ç•¥åˆ†æ (Spider Web Strategy)
**ç†è®ºåŸºç¡€**: åŸºäºå‰20åä¼šå‘˜æŒä»“å˜åŠ¨æ•°æ®ï¼Œæ•æ‰"èªæ˜èµ„é‡‘"åŠ¨å‘
**æ ¸å¿ƒæŒ‡æ ‡**: dB (å¤šå¤´å˜åŠ¨), dS (ç©ºå¤´å˜åŠ¨)
**ä¿¡å·åˆ¤æ–­**:
- dB > 0 ä¸” dS < 0: å¼ºçƒˆçœ‹å¤š (å¼ºåº¦0.8-1.0)
- dB < 0 ä¸” dS > 0: å¼ºçƒˆçœ‹ç©º (å¼ºåº¦0.8-1.0)
- dBå’ŒdSåŒå‘: éœ€ç»“åˆå‡€å˜åŒ–å’Œå†å²å¯¹æ¯”åˆ¤æ–­
**å†å²å¯¹æ¯”**: å¿…é¡»ç»“åˆä¿¡å·å¼ºåº¦çš„å†å²åˆ†ä½æ•°è¿›è¡Œè¯„ä¼°

### 2. èªæ˜é’±åˆ†æ (Smart Money Analysis)
**æ ¸å¿ƒæŒ‡æ ‡**: 
- OI/Volume Ratio: æŒä»“/æˆäº¤æ¯”
- Position Efficiency: æŒä»“æ•ˆç‡
- Smart Money Score: ç»¼åˆè¯„åˆ†
**å†å²å¯¹æ¯”**: å½“å‰èªæ˜é’±æ´»è·ƒåº¦åœ¨å†å²ä¸­çš„ä½ç½®

### 3. å®¶äººå¸­ä½åå‘ç­–ç•¥ (Retail Reverse Strategy)
**æ ¸å¿ƒé€»è¾‘**: æ•£æˆ·vsæœºæ„è¡Œä¸ºåˆ†åŒ–åˆ†æ
**å†å²éªŒè¯**: ç»“åˆå†å²èƒœç‡è¿›è¡Œä¿¡å·å¼ºåº¦è¯„ä¼°

### 4. æŒä»“é›†ä¸­åº¦åˆ†æ (Concentration Analysis)
**æ ¸å¿ƒæŒ‡æ ‡**: HHIæŒ‡æ•°ï¼Œé›†ä¸­åº¦å·®å¼‚ï¼Œé›†ä¸­åº¦è¶‹åŠ¿
**å†å²å¯¹æ¯”**: å½“å‰é›†ä¸­åº¦æ°´å¹³çš„å†å²ä½ç½®

### 5. å®æ—¶å¸‚åœºæƒ…æŠ¥æ•´åˆ
**æ•°æ®æ¥æº**: å®æ—¶æ–°é—»ã€æœºæ„è§‚ç‚¹ã€æŠ€æœ¯åˆ†æ
**åˆ†æç»´åº¦**: æƒ…ç»ªå€¾å‘ã€è§‚ç‚¹ç»Ÿè®¡ã€æŠ€æœ¯ä¿¡å·

### 6. å†å²å¯¹æ¯”åˆ†æ
**å…³é”®è¦ç´ **: 
- å„æŒ‡æ ‡çš„å†å²åˆ†ä½æ•°
- å†å²æå€¼å¯¹æ¯”
- è¶‹åŠ¿å˜åŒ–åˆ†æ
- ç›¸ä¼¼å†å²æƒ…å†µçš„åç»­èµ°åŠ¿

## è¾“å‡ºè¦æ±‚ - æœºæ„çº§æ ‡å‡†

```json
{
  "spider_web_analysis": {
    "signal": "å¼ºçƒˆçœ‹å¤š|çœ‹å¤š|ä¸­æ€§|çœ‹ç©º|å¼ºçƒˆçœ‹ç©º",
    "strength": 0.0-1.0,
    "confidence": 0.0-1.0,
    "reasoning": "è¯¦ç»†åˆ†ædBã€dSæ•°å€¼ï¼Œç»“åˆå†å²åˆ†ä½æ•°ï¼Œè§£é‡Šä¿¡å·å¼ºåº¦å’Œå¸‚åœºå«ä¹‰",
    "key_metrics": {
      "dB": æ•°å€¼,
      "dS": æ•°å€¼,
      "signal_strength": æ•°å€¼,
      "historical_percentile": "å†å²åˆ†ä½æ•°%"
    },
    "historical_comparison": "ä¸å†å²åŒç±»ä¿¡å·çš„å¯¹æ¯”åˆ†æ"
  },
  "smart_money_analysis": {
    "signal": "çœ‹å¤š|ä¸­æ€§|çœ‹ç©º",
    "strength": 0.0-1.0,
    "confidence": 0.0-1.0,
    "reasoning": "æ·±åº¦åˆ†æèªæ˜é’±æ´»è·ƒåº¦ã€æ•ˆç‡ï¼Œç»“åˆå†å²å¯¹æ¯”",
    "key_metrics": {
      "smart_money_score": æ•°å€¼,
      "oi_volume_ratio": æ•°å€¼,
      "historical_percentile": "å†å²åˆ†ä½æ•°%"
    },
    "trend_analysis": "èªæ˜é’±è¡Œä¸ºè¶‹åŠ¿åˆ†æ"
  },
  "retail_reverse_analysis": {
    "signal": "çœ‹å¤š|ä¸­æ€§|çœ‹ç©º",
    "strength": 0.0-1.0,
    "confidence": 0.0-1.0,
    "reasoning": "æ•£æˆ·vsæœºæ„è¡Œä¸ºåˆ†åŒ–åˆ†æï¼Œå†å²èƒœç‡è¯„ä¼°",
    "key_metrics": {
      "retail_net": æ•°å€¼,
      "institutional_net": æ•°å€¼,
      "divergence_strength": "åˆ†åŒ–å¼ºåº¦è¯„åˆ†"
    }
  },
  "concentration_analysis": {
    "signal": "çœ‹å¤š|ä¸­æ€§|çœ‹ç©º",
    "strength": 0.0-1.0,
    "confidence": 0.0-1.0,
    "reasoning": "æŒä»“é›†ä¸­åº¦å˜åŒ–åˆ†æï¼Œå¤§æˆ·æ§ç›˜è¡Œä¸ºè¯„ä¼°",
    "key_metrics": {
      "concentration_differential": æ•°å€¼,
      "historical_percentile": "å†å²åˆ†ä½æ•°%",
      "trend_direction": "è¶‹åŠ¿æ–¹å‘"
    }
  },
  "market_intelligence_analysis": {
    "news_sentiment": "åŸºäºå®æ—¶æ–°é—»çš„æƒ…ç»ªåˆ†æ",
    "institutional_consensus": "æœºæ„è§‚ç‚¹ç»Ÿè®¡å’Œå…±è¯†åº¦",
    "technical_signals": "æŠ€æœ¯åˆ†æä¿¡å·æ±‡æ€»",
    "market_heat": "å¸‚åœºå…³æ³¨åº¦å’Œçƒ­åº¦è¯„ä¼°"
  },
  "historical_context": {
    "current_vs_history": "å½“å‰æ•°æ®åœ¨å†å²ä¸­çš„ä½ç½®è¯„ä¼°",
    "similar_scenarios": "å†å²ç›¸ä¼¼æƒ…å†µåŠå…¶åç»­èµ°åŠ¿",
    "extreme_levels": "æ˜¯å¦æ¥è¿‘å†å²æå€¼æ°´å¹³",
    "trend_sustainability": "å½“å‰è¶‹åŠ¿çš„å¯æŒç»­æ€§è¯„ä¼°"
  },
  "comprehensive_conclusion": {
    "overall_signal": "å¼ºçƒˆçœ‹å¤š|çœ‹å¤š|ä¸­æ€§|çœ‹ç©º|å¼ºçƒˆçœ‹ç©º",
    "confidence": 0.0-1.0,
    "time_horizon": "çŸ­æœŸ(1-3å¤©)|ä¸­æœŸ(1-2å‘¨)|é•¿æœŸ(1ä¸ªæœˆ+)",
    "key_factors": ["å…³é”®å› ç´ 1", "å…³é”®å› ç´ 2", "å…³é”®å› ç´ 3"],
    "supporting_evidence": "æ”¯æ’‘è¯æ®è¯¦è¿°ï¼ŒåŒ…æ‹¬æ•°æ®å¯¹æ¯”ã€å†å²éªŒè¯",
    "risk_factors": ["é£é™©å› ç´ 1", "é£é™©å› ç´ 2"],
    "market_regime": "è¶‹åŠ¿å¸‚|éœ‡è¡å¸‚|è½¬æŠ˜æœŸ",
    "probability_assessment": {
      "upward_probability": "ä¸Šæ¶¨æ¦‚ç‡%",
      "downward_probability": "ä¸‹è·Œæ¦‚ç‡%",
      "sideways_probability": "æ¨ªç›˜æ¦‚ç‡%"
    }
  },
  "trading_recommendations": {
    "primary_strategy": "å…·ä½“äº¤æ˜“ç­–ç•¥ï¼ŒåŸºäºå¤šç»´åº¦åˆ†æ",
    "entry_timing": "å…¥åœºæ—¶æœºï¼Œç»“åˆæŠ€æœ¯ä½å’Œæ•°æ®ä¿¡å·",
    "position_sizing": "ä»“ä½å»ºè®®ï¼ŒåŸºäºä¿¡å·å¼ºåº¦å’Œé£é™©è¯„ä¼°",
    "stop_loss": "æ­¢æŸç­–ç•¥ï¼ŒåŒ…æ‹¬æŠ€æœ¯æ­¢æŸå’Œæ•°æ®æ­¢æŸ",
    "profit_target": "ç›®æ ‡ä½è®¾å®šï¼ŒçŸ­ä¸­é•¿æœŸç›®æ ‡",
    "risk_management": "é£é™©ç®¡ç†è¦ç‚¹",
    "scenario_planning": "ä¸åŒæƒ…å†µä¸‹çš„åº”å¯¹ç­–ç•¥"
  },
  "professional_insights": {
    "market_microstructure": "å¸‚åœºå¾®è§‚ç»“æ„æ·±åº¦åˆ†æ",
    "institutional_behavior_pattern": "æœºæ„è¡Œä¸ºæ¨¡å¼è¯†åˆ«",
    "contrarian_signal_strength": "åå‘æŒ‡æ ‡å¼ºåº¦è¯„ä¼°",
    "momentum_characteristics": "åŠ¨é‡ç‰¹å¾å’ŒæŒç»­æ€§",
    "regime_change_probability": "å¸‚åœºçŠ¶æ€è½¬æ¢æ¦‚ç‡",
    "cross_asset_implications": "å¯¹ç›¸å…³èµ„äº§çš„å½±å“",
    "seasonality_considerations": "å­£èŠ‚æ€§å› ç´ è€ƒé‡",
    "volatility_outlook": "æ³¢åŠ¨ç‡å‰æ™¯é¢„æµ‹"
  }
}
```

## åˆ†æè¦æ±‚
1. **æ•°æ®é©±åŠ¨**: ä¸¥æ ¼åŸºäºçœŸå®æ•°æ®å’Œå†å²å¯¹æ¯”
2. **é€»è¾‘ä¸¥å¯†**: æ¯ä¸ªç»“è®ºéƒ½æœ‰å……åˆ†æ”¯æ’‘
3. **å†å²éªŒè¯**: å¿…é¡»ç»“åˆå†å²åˆ†ä½æ•°å’Œç›¸ä¼¼æƒ…å†µ
4. **å®æ—¶æ•´åˆ**: èåˆå®æ—¶å¸‚åœºæƒ…æŠ¥
5. **é£é™©æ„è¯†**: å……åˆ†è¯†åˆ«å„ç§é£é™©
6. **å¯æ“ä½œæ€§**: æä¾›å…·ä½“å¯æ‰§è¡Œå»ºè®®
7. **ä¸“ä¸šæ·±åº¦**: ä½“ç°é¡¶çº§åˆ†æå¸ˆæ°´å‡†

è¯·åŸºäºæä¾›çš„å®Œæ•´æ•°æ®ï¼ˆåŒ…æ‹¬å†å²å¯¹æ¯”å’Œå®æ—¶æƒ…æŠ¥ï¼‰è¿›è¡Œæ·±åº¦ä¸“ä¸šåˆ†æã€‚
"""
    
    def _prepare_complete_analysis_data(self, symbol: str, symbol_name: str, metrics_list: List[EnhancedPositionMetrics], 
                                      historical_analysis: Dict, market_intelligence: Dict) -> Dict:
        """å‡†å¤‡å®Œæ•´åˆ†ææ•°æ®"""
        
        if not metrics_list:
            return {}
        
        latest = metrics_list[-1]
        
        # æœ€è¿‘æ•°æ®
        recent_data = []
        for m in metrics_list[-min(10, len(metrics_list)):]:
            comprehensive = {
                'date': m.date,
                'spider_web_dB': m.spider_web_dB,
                'spider_web_dS': m.spider_web_dS,
                'spider_web_signal_strength': m.spider_web_signal_strength,
                'net_change': m.net_change_top20,
                'net_position': m.net_position_top20,
                'smart_money_score': m.smart_money_score,
                'long_concentration': m.long_concentration,
                'short_concentration': m.short_concentration,
                'retail_net_change': m.retail_net_change,
                'institutional_net_change': m.institutional_net_change,
                'long_short_ratio': m.long_short_ratio,
                'market_sentiment_score': m.market_sentiment_score
            }
            recent_data.append(comprehensive)
        
        return {
            "symbol": symbol,
            "symbol_name": symbol_name,
            "analysis_timestamp": datetime.now().isoformat(),
            "data_period": f"{metrics_list[0].date} ~ {metrics_list[-1].date}",
            "analysis_days": len(metrics_list),
            
            # å½“å‰æ ¸å¿ƒæ•°æ®
            "current_metrics": {
                "spider_web_dB": latest.spider_web_dB,
                "spider_web_dS": latest.spider_web_dS,
                "spider_web_signal_strength": latest.spider_web_signal_strength,
                "net_change_top20": latest.net_change_top20,
                "net_position_top20": latest.net_position_top20,
                "smart_money_score": latest.smart_money_score,
                "oi_volume_ratio": latest.oi_volume_ratio,
                "position_efficiency": latest.position_efficiency,
                "long_concentration": latest.long_concentration,
                "short_concentration": latest.short_concentration,
                "concentration_differential": latest.concentration_differential,
                "retail_net_change": latest.retail_net_change,
                "institutional_net_change": latest.institutional_net_change,
                "foreign_net_change": latest.foreign_net_change,
                "retail_vs_smart_money": latest.retail_vs_smart_money,
                "long_short_ratio": latest.long_short_ratio,
                "position_momentum": latest.position_momentum,
                "volatility_index": latest.volatility_index,
                "market_sentiment_score": latest.market_sentiment_score,
                "consensus_level": latest.consensus_level
            },
            
            # å†å²æ•°æ®å’Œè¶‹åŠ¿
            "recent_data_series": recent_data,
            
            # å†å²å¯¹æ¯”åˆ†æ
            "historical_analysis": historical_analysis,
            
            # å®æ—¶å¸‚åœºæƒ…æŠ¥
            "market_intelligence": market_intelligence,
            
            # è¶‹åŠ¿æŒ‡æ ‡
            "trend_indicators": {
                "net_change_trend_5d": latest.net_change_trend_5d,
                "position_trend_5d": latest.position_trend_5d,
                "concentration_trend_5d": latest.concentration_trend_5d
            }
        }

# ============================================================================
# 7. å®Œç¾åˆ†æå¼•æ“
# ============================================================================

class PerfectAnalysisEngine:
    """å®Œç¾åˆ†æå¼•æ“ - äº¤äº’å¼ + å†å²å¯¹æ¯” + å®æ—¶æƒ…æŠ¥"""
    
    def __init__(self):
        self.data_processor = EnhancedDataProcessor()
        self.ai_analyst = PerfectAIAnalyst()
    
    async def interactive_analysis(self):
        """äº¤äº’å¼åˆ†æ"""
        
        print("ğŸ¯ ä¸“ä¸šæœŸè´§æŒä»“æ•°æ®AIåˆ†æç³»ç»Ÿ - å®Œç¾ç‰ˆ")
        print("=" * 80)
        
        # è·å–ç”¨æˆ·è¾“å…¥
        user_input = get_user_input()
        if not user_input:
            return
        
        symbol = user_input['symbol']
        symbol_name = user_input['symbol_name']
        days_back = user_input['days_back']
        model_type = user_input['model_type']
        
        print(f"\nğŸš€ å¼€å§‹åˆ†æ...")
        print("=" * 60)
        
        # 1. æ•°æ®å‡†å¤‡
        print("ğŸ“Š å‡†å¤‡åˆ†ææ•°æ®...")
        metrics_data, historical_analysis = self.data_processor.prepare_comprehensive_data_with_history(symbol, days_back)
        
        if not metrics_data:
            print(f"âŒ {symbol} æ•°æ®å‡†å¤‡å¤±è´¥")
            return
        
        print(f"âœ… æ•°æ®å‡†å¤‡å®Œæˆ: {len(metrics_data)}å¤©åˆ†ææ•°æ®")
        
        # 2. AIåˆ†æ
        ai_result = await self.ai_analyst.comprehensive_analysis(
            symbol, symbol_name, metrics_data, historical_analysis, model_type
        )
        
        if not ai_result or ai_result.get('analysis_failed'):
            print("âŒ AIåˆ†æå¤±è´¥")
            return
        
        # 3. æ˜¾ç¤ºç»“æœ
        self._display_analysis_results(symbol, symbol_name, ai_result)
        
        # 4. ä¿å­˜æŠ¥å‘Š
        self._save_analysis_report(symbol, ai_result)
    
    def _display_analysis_results(self, symbol: str, symbol_name: str, ai_result: Dict):
        """æ˜¾ç¤ºåˆ†æç»“æœ"""
        
        print(f"\nğŸ¯ {symbol} ({symbol_name}) å®Œç¾åˆ†æç»“æœ")
        print("=" * 80)
        
        # æ˜¾ç¤ºä½¿ç”¨çš„æ¨¡å‹
        model_used = ai_result.get('model_used', 'æœªçŸ¥')
        fallback = ai_result.get('fallback_mode', False)
        model_info = f"{model_used.upper()}" + (" (å¤‡ç”¨æ¨¡å¼)" if fallback else "")
        print(f"ğŸ¤– ä½¿ç”¨æ¨¡å‹: {model_info}")
        
        # æ˜¾ç¤ºå®æ—¶å¸‚åœºæƒ…æŠ¥
        if 'market_intelligence_analysis' in ai_result:
            intel = ai_result['market_intelligence_analysis']
            print(f"\nğŸŒ å®æ—¶å¸‚åœºæƒ…æŠ¥:")
            print(f"   æ–°é—»æƒ…ç»ª: {intel.get('news_sentiment', 'æœªçŸ¥')}")
            print(f"   æœºæ„å…±è¯†: {intel.get('institutional_consensus', 'æœªçŸ¥')}")
            print(f"   æŠ€æœ¯ä¿¡å·: {intel.get('technical_signals', 'æœªçŸ¥')}")
            print(f"   å¸‚åœºçƒ­åº¦: {intel.get('market_heat', 'æœªçŸ¥')}")
        
        # æ˜¾ç¤ºå†å²å¯¹æ¯”
        if 'historical_context' in ai_result:
            context = ai_result['historical_context']
            print(f"\nğŸ“Š å†å²å¯¹æ¯”åˆ†æ:")
            print(f"   å†å²ä½ç½®: {context.get('current_vs_history', 'æœªçŸ¥')}")
            print(f"   ç›¸ä¼¼æƒ…å†µ: {context.get('similar_scenarios', 'æœªçŸ¥')}")
            print(f"   æå€¼æ°´å¹³: {context.get('extreme_levels', 'æœªçŸ¥')}")
        
        # æ˜¾ç¤ºå„ç­–ç•¥åˆ†æ
        strategies = ['spider_web_analysis', 'smart_money_analysis', 'retail_reverse_analysis', 'concentration_analysis']
        
        for strategy in strategies:
            if strategy in ai_result:
                analysis = ai_result[strategy]
                signal = analysis.get('signal', 'æœªçŸ¥')
                strength = analysis.get('strength', 0.0)
                confidence = analysis.get('confidence', 0.0)
                reasoning = analysis.get('reasoning', 'æ— ')
                
                strategy_name = {
                    'spider_web_analysis': 'ğŸ•¸ï¸ èœ˜è››ç½‘ç­–ç•¥',
                    'smart_money_analysis': 'ğŸ§  èªæ˜é’±åˆ†æ', 
                    'retail_reverse_analysis': 'ğŸ”„ å®¶äººå¸­ä½åå‘',
                    'concentration_analysis': 'ğŸ“Š æŒä»“é›†ä¸­åº¦'
                }.get(strategy, strategy)
                
                print(f"\n{strategy_name}:")
                print(f"   ä¿¡å·: {signal} | å¼ºåº¦: {strength:.2f} | ç½®ä¿¡åº¦: {confidence:.2f}")
                print(f"   åˆ†æ: {reasoning}")
                
                # æ˜¾ç¤ºå†å²å¯¹æ¯”
                if 'historical_comparison' in analysis:
                    print(f"   å†å²å¯¹æ¯”: {analysis['historical_comparison']}")
        
        # ç»¼åˆç»“è®º
        if 'comprehensive_conclusion' in ai_result:
            conclusion = ai_result['comprehensive_conclusion']
            print(f"\nğŸ¯ ç»¼åˆç»“è®º:")
            print(f"   æ€»ä½“ä¿¡å·: {conclusion.get('overall_signal', 'æœªçŸ¥')}")
            print(f"   ç½®ä¿¡åº¦: {conclusion.get('confidence', 0.0):.2f}")
            print(f"   æ—¶é—´å‘¨æœŸ: {conclusion.get('time_horizon', 'æœªçŸ¥')}")
            print(f"   å…³é”®å› ç´ : {', '.join(conclusion.get('key_factors', []))}")
            print(f"   å¸‚åœºçŠ¶æ€: {conclusion.get('market_regime', 'æœªçŸ¥')}")
            
            # æ¦‚ç‡è¯„ä¼°
            if 'probability_assessment' in conclusion:
                prob = conclusion['probability_assessment']
                print(f"   æ¦‚ç‡è¯„ä¼°: ä¸Šæ¶¨{prob.get('upward_probability', 'N/A')} | ä¸‹è·Œ{prob.get('downward_probability', 'N/A')} | æ¨ªç›˜{prob.get('sideways_probability', 'N/A')}")
        
        # äº¤æ˜“å»ºè®®
        if 'trading_recommendations' in ai_result:
            trading = ai_result['trading_recommendations']
            print(f"\nğŸ’¡ äº¤æ˜“å»ºè®®:")
            print(f"   ä¸»è¦ç­–ç•¥: {trading.get('primary_strategy', 'æ— ')}")
            print(f"   å…¥åœºæ—¶æœº: {trading.get('entry_timing', 'æ— ')}")
            print(f"   ä»“ä½å»ºè®®: {trading.get('position_sizing', 'æ— ')}")
            print(f"   æ­¢æŸå»ºè®®: {trading.get('stop_loss', 'æ— ')}")
            print(f"   ç›®æ ‡ä½: {trading.get('profit_target', 'æ— ')}")
            
            if 'scenario_planning' in trading:
                print(f"   æƒ…æ™¯è§„åˆ’: {trading['scenario_planning']}")
        
        # ä¸“ä¸šæ´å¯Ÿ
        if 'professional_insights' in ai_result:
            insights = ai_result['professional_insights']
            print(f"\nğŸ”¬ ä¸“ä¸šæ´å¯Ÿ:")
            insight_items = [
                ('market_microstructure', 'å¸‚åœºå¾®è§‚ç»“æ„'),
                ('institutional_behavior_pattern', 'æœºæ„è¡Œä¸ºæ¨¡å¼'),
                ('regime_change_probability', 'çŠ¶æ€è½¬æ¢æ¦‚ç‡'),
                ('volatility_outlook', 'æ³¢åŠ¨ç‡å‰æ™¯')
            ]
            
            for key, name in insight_items:
                if key in insights and insights[key]:
                    print(f"   {name}: {insights[key]}")
    
    def _save_analysis_report(self, symbol: str, ai_result: Dict):
        """ä¿å­˜åˆ†ææŠ¥å‘Š"""
        try:
            OUTPUT_DIR.mkdir(parents=True, exist_ok=True)
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"perfect_analysis_{symbol}_{timestamp}.json"
            
            output_file = OUTPUT_DIR / filename
            
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(ai_result, f, ensure_ascii=False, indent=2, default=json_serialize_helper)
            
            print(f"\nğŸ“„ åˆ†ææŠ¥å‘Šå·²ä¿å­˜: {output_file}")
            
        except Exception as e:
            print(f"âš ï¸ æŠ¥å‘Šä¿å­˜å¤±è´¥: {e}")

# ============================================================================
# 8. ä¸»ç¨‹åºå…¥å£
# ============================================================================

async def main():
    """ä¸»ç¨‹åº"""
    engine = PerfectAnalysisEngine()
    await engine.interactive_analysis()

def jupyter_run():
    """Jupyterç¯å¢ƒè¿è¡Œ"""
    import asyncio
    
    try:
        import nest_asyncio
        nest_asyncio.apply()
    except ImportError:
        pass
    
    loop = asyncio.get_event_loop()
    if loop.is_running():
        task = asyncio.create_task(main())
        return task
    else:
        return asyncio.run(main())

if __name__ == "__main__":
    print("ğŸš€ ä¸“ä¸šæœŸè´§æŒä»“æ•°æ®AIåˆ†æç³»ç»Ÿå·²åŠ è½½ - å®Œç¾ç‰ˆ")
    print("ğŸ”¥ ç‰¹æ€§:")
    print("   âœ… äº¤äº’å¼ç”¨æˆ·ç•Œé¢")
    print("   âœ… DeepSeek v3.1 Reasoner + Chatæ¨¡å¼é€‰æ‹©")
    print("   âœ… Serperå®æ—¶å¸‚åœºæƒ…æŠ¥æœç´¢")
    print("   âœ… å†å²å¯¹æ¯”åˆ†æ")
    print("   âœ… æ— æ—¶é—´é™åˆ¶ï¼Œç¡®ä¿åˆ†æè´¨é‡")
    print("   âœ… æœºæ„çº§ä¸“ä¸šåˆ†ææ ‡å‡†")
    print("\nä½¿ç”¨æ–¹æ³•:")
    print("1. äº¤äº’å¼åˆ†æ: await main()")
    print("2. Jupyterç¯å¢ƒ: jupyter_run()")
    print("\nğŸ’¡ è¿™æ˜¯çœŸæ­£å®Œç¾çš„ä¸“ä¸šAIåˆ†æç³»ç»Ÿï¼")
    
    # æ£€æµ‹è¿è¡Œç¯å¢ƒå¹¶é€‰æ‹©åˆé€‚çš„å¯åŠ¨æ–¹å¼
    try:
        # æ£€æµ‹æ˜¯å¦åœ¨Jupyterç¯å¢ƒä¸­
        import IPython
        if IPython.get_ipython() is not None:
            print("\nğŸ” æ£€æµ‹åˆ°Jupyterç¯å¢ƒï¼Œè¯·ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤å¯åŠ¨åˆ†æ:")
            print("   jupyter_run()")
        else:
            # å‘½ä»¤è¡Œç¯å¢ƒ
            asyncio.run(main())
    except ImportError:
        # ä¸åœ¨Jupyterç¯å¢ƒä¸­ï¼Œç›´æ¥è¿è¡Œ
        try:
            asyncio.run(main())
        except RuntimeError as e:
            if "cannot be called from a running event loop" in str(e):
                print("\nâš ï¸ æ£€æµ‹åˆ°äº‹ä»¶å¾ªç¯å†²çªï¼Œè¯·ä½¿ç”¨: jupyter_run()")
            else:
                raise e
