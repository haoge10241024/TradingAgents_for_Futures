#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æœŸè´§Trading Agentsç³»ç»Ÿ - å·¥å…·æ¨¡å—
æä¾›å„ç§è¾…åŠ©åŠŸèƒ½å’Œå®ç”¨å·¥å…·

åŠŸèƒ½åŒ…æ‹¬ï¼š
1. DeepSeek APIè°ƒç”¨å°è£…
2. æ•°æ®éªŒè¯å’Œæ¸…æ´—
3. æ–‡ä»¶æ“ä½œå·¥å…·
4. æ—¶é—´å¤„ç†å·¥å…·
5. æ•°æ®æ ¼å¼è½¬æ¢
6. ç¼“å­˜ç®¡ç†
7. é”™è¯¯å¤„ç†è£…é¥°å™¨

ä½œè€…: AI Assistant
ç‰ˆæœ¬: 1.0.0
åˆ›å»ºæ—¶é—´: 2025-01-19
"""

import json
import asyncio
import aiohttp
import hashlib
import pickle
import time
import functools
import logging
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Any, Callable, Union
from pathlib import Path
import pandas as pd
import numpy as np
from dataclasses import asdict

# ============================================================================
# 1. DeepSeek APIè°ƒç”¨å°è£…
# ============================================================================

class DeepSeekAPIClient:
    """DeepSeek APIå®¢æˆ·ç«¯å°è£…"""
    
    def __init__(self, api_key: str, base_url: str = "https://api.deepseek.com/v1"):
        self.api_key = api_key
        self.base_url = base_url
        self.session = None
        self.logger = logging.getLogger("DeepSeekAPI")
        
    async def __aenter__(self):
        # å¢åŠ è¿æ¥è¶…æ—¶å’ŒDNSè§£æè¶…æ—¶è®¾ç½®
        timeout = aiohttp.ClientTimeout(
            total=600,  # æ€»è¶…æ—¶10åˆ†é’Ÿï¼ˆå¢åŠ ï¼‰
            connect=30,  # è¿æ¥è¶…æ—¶30ç§’
            sock_read=180,  # è¯»å–è¶…æ—¶180ç§’ï¼ˆå¢åŠ åˆ°3åˆ†é’Ÿï¼Œé€‚åº”å¤æ‚åˆ†æï¼‰
            sock_connect=30  # socketè¿æ¥è¶…æ—¶30ç§’
        )
        
        # åˆ›å»ºè¿æ¥å™¨ï¼Œå¢åŠ DNSç¼“å­˜å’Œè¿æ¥æ± è®¾ç½®
        connector = aiohttp.TCPConnector(
            limit=100,  # è¿æ¥æ± å¤§å°
            limit_per_host=30,  # æ¯ä¸ªä¸»æœºçš„è¿æ¥æ•°
            ttl_dns_cache=300,  # DNSç¼“å­˜5åˆ†é’Ÿ
            use_dns_cache=True,
            keepalive_timeout=30,
            enable_cleanup_closed=True
        )
        
        self.session = aiohttp.ClientSession(
            headers={
                "Authorization": f"Bearer {self.api_key}",
                "Content-Type": "application/json"
            },
            timeout=timeout,
            connector=connector
        )
        return self
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        if self.session:
            await self.session.close()
    
    async def ensure_session(self):
        """ç¡®ä¿sessionå·²åˆå§‹åŒ–"""
        if self.session is None or self.session.closed:
            # ä½¿ç”¨ä¸__aenter__ç›¸åŒçš„è¶…æ—¶å’Œè¿æ¥å™¨é…ç½®
            timeout = aiohttp.ClientTimeout(
                total=600,  # æ€»è¶…æ—¶10åˆ†é’Ÿï¼ˆå¢åŠ ï¼‰
                connect=30,  # è¿æ¥è¶…æ—¶30ç§’
                sock_read=180,  # è¯»å–è¶…æ—¶180ç§’ï¼ˆå¢åŠ åˆ°3åˆ†é’Ÿï¼Œé€‚åº”å¤æ‚åˆ†æï¼‰
                sock_connect=30  # socketè¿æ¥è¶…æ—¶30ç§’
            )
            
            connector = aiohttp.TCPConnector(
                limit=100,
                limit_per_host=30,
                ttl_dns_cache=300,
                use_dns_cache=True,
                keepalive_timeout=30,
                enable_cleanup_closed=True
            )
            
            self.session = aiohttp.ClientSession(
                headers={
                    "Authorization": f"Bearer {self.api_key}",
                    "Content-Type": "application/json"
                },
                timeout=timeout,
                connector=connector
            )

    async def close_session(self):
        """å…³é—­session"""
        if self.session and not self.session.closed:
            await self.session.close()
            self.session = None
    
    async def __aenter__(self):
        """å¼‚æ­¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨å…¥å£"""
        await self.ensure_session()
        return self
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """å¼‚æ­¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨å‡ºå£"""
        try:
            await self.close_session()
        except Exception as e:
            # å¿½ç•¥äº‹ä»¶å¾ªç¯å…³é—­æ—¶çš„æ¸…ç†é”™è¯¯
            if "Event loop is closed" not in str(e):
                self.logger.warning(f"å…³é—­sessionæ—¶å‡ºç°å¼‚å¸¸: {e}")

    async def chat_completion(self, messages: List[Dict], model: str = "deepseek-chat",
                            temperature: float = 0.1, max_tokens: int = 4000, max_retries: int = 5) -> Dict:
        """èŠå¤©è¡¥å…¨APIè°ƒç”¨ï¼ˆå¸¦é‡è¯•æœºåˆ¶ï¼‰"""

        url = f"{self.base_url}/chat/completions"

        payload = {
            "model": model,
            "messages": messages,
            "temperature": temperature,
            "max_tokens": max_tokens,
            "stream": False
        }

        # é‡è¯•æœºåˆ¶
        for attempt in range(max_retries):
            try:
                # ç¡®ä¿sessionå·²åˆå§‹åŒ–
                await self.ensure_session()

                # ç›´æ¥ä½¿ç”¨sessionï¼Œä¸ä½¿ç”¨async with
                response = await self.session.post(url, json=payload)

                try:
                    if response.status == 200:
                        result = await response.json()
                        return {
                            "success": True,
                            "content": result["choices"][0]["message"]["content"],
                            "usage": result.get("usage", {}),
                            "model": model
                        }
                    else:
                        error_text = await response.text()
                        self.logger.warning(f"APIè°ƒç”¨å¤±è´¥ (å°è¯• {attempt+1}/{max_retries}): {response.status} - {error_text}")

                        # å¦‚æœæ˜¯æœ€åä¸€æ¬¡å°è¯•ï¼Œè¿”å›é”™è¯¯
                        if attempt == max_retries - 1:
                            return {
                                "success": False,
                                "error": f"HTTP {response.status}: {error_text}",
                                "model": model
                            }

                finally:
                    response.close()  # ç¡®ä¿å“åº”è¢«æ­£ç¡®å…³é—­

                # ç­‰å¾…åé‡è¯•ï¼Œä½¿ç”¨æŒ‡æ•°é€€é¿
                delay = min(2 ** attempt, 30)  # æœ€å¤§å»¶è¿Ÿ30ç§’
                await asyncio.sleep(delay)

            except Exception as e:
                error_msg = str(e)
                self.logger.warning(f"APIè°ƒç”¨å¼‚å¸¸ (å°è¯• {attempt+1}/{max_retries}): {error_msg}")
                
                # å¯¹äºDNSæˆ–è¿æ¥é—®é¢˜ï¼Œç»™å‡ºæ›´è¯¦ç»†çš„é”™è¯¯ä¿¡æ¯
                if "DNS" in error_msg or "Timeout" in error_msg or "Cannot connect" in error_msg:
                    self.logger.warning(f"ç½‘ç»œè¿æ¥é—®é¢˜ï¼Œå°†åœ¨ {min(2 ** attempt, 30)} ç§’åé‡è¯•...")

                # å¦‚æœæ˜¯æœ€åä¸€æ¬¡å°è¯•ï¼Œè¿”å›é”™è¯¯
                if attempt == max_retries - 1:
                    return {
                        "success": False,
                        "error": f"APIè°ƒç”¨å¤±è´¥ (é‡è¯•{max_retries}æ¬¡å): {error_msg}",
                        "model": model
                    }

                # ç­‰å¾…åé‡è¯•ï¼Œä½¿ç”¨æŒ‡æ•°é€€é¿
                delay = min(2 ** attempt, 30)  # æœ€å¤§å»¶è¿Ÿ30ç§’
                await asyncio.sleep(delay)
    
    async def reasoning_completion(self, prompt: str, model: str = "deepseek-reasoner",
                                 temperature: float = 0.1, max_tokens: int = 4000) -> Dict:
        """æ¨ç†æ¨¡å¼APIè°ƒç”¨"""
        
        messages = [
            {"role": "user", "content": prompt}
        ]
        
        return await self.chat_completion(messages, model, temperature, max_tokens)
    
    async def analyze_with_context(self, system_prompt: str, user_prompt: str,
                                 context_data: Dict = None, model: str = "deepseek-chat") -> Dict:
        """å¸¦ä¸Šä¸‹æ–‡çš„åˆ†æè°ƒç”¨"""
        
        messages = [
            {"role": "system", "content": system_prompt}
        ]
        
        # æ·»åŠ ä¸Šä¸‹æ–‡æ•°æ®
        if context_data:
            context_str = json.dumps(context_data, ensure_ascii=False, indent=2)
            user_prompt = f"{user_prompt}\n\n## ä¸Šä¸‹æ–‡æ•°æ®\n```json\n{context_str}\n```"
        
        messages.append({"role": "user", "content": user_prompt})
        
        return await self.chat_completion(messages, model)

# ============================================================================
# 2. æ•°æ®éªŒè¯å’Œæ¸…æ´—å·¥å…·
# ============================================================================

class DataValidator:
    """æ•°æ®éªŒè¯å™¨"""
    
    @staticmethod
    def validate_commodity_code(commodity: str) -> bool:
        """éªŒè¯å•†å“ä»£ç æ ¼å¼"""
        if not isinstance(commodity, str):
            return False
        
        # æœŸè´§å•†å“ä»£ç é€šå¸¸æ˜¯2-3ä¸ªå¤§å†™å­—æ¯
        return len(commodity) in [2, 3] and commodity.isupper() and commodity.isalpha()
    
    @staticmethod
    def validate_date_format(date_str: str) -> bool:
        """éªŒè¯æ—¥æœŸæ ¼å¼ YYYY-MM-DD"""
        try:
            datetime.strptime(date_str, "%Y-%m-%d")
            return True
        except ValueError:
            return False
    
    @staticmethod
    def validate_analysis_result(result: Dict) -> Dict:
        """éªŒè¯åˆ†æç»“æœæ ¼å¼"""
        
        validation_result = {
            "is_valid": True,
            "errors": [],
            "warnings": []
        }
        
        # æ£€æŸ¥å¿…éœ€å­—æ®µ
        required_fields = ["commodity", "analysis_date", "analysis_type"]
        for field in required_fields:
            if field not in result:
                validation_result["errors"].append(f"ç¼ºå°‘å¿…éœ€å­—æ®µ: {field}")
                validation_result["is_valid"] = False
        
        # æ£€æŸ¥ä¿¡å¿ƒåˆ†æ•°
        if "confidence_score" in result:
            confidence = result["confidence_score"]
            if not isinstance(confidence, (int, float)) or not 0 <= confidence <= 1:
                validation_result["errors"].append("confidence_scoreå¿…é¡»æ˜¯0-1ä¹‹é—´çš„æ•°å€¼")
                validation_result["is_valid"] = False
        
        # æ£€æŸ¥æ—¶é—´æˆ³
        if "timestamp" in result:
            if not DataValidator.validate_date_format(result["timestamp"][:10]):
                validation_result["warnings"].append("æ—¶é—´æˆ³æ ¼å¼å¯èƒ½ä¸æ­£ç¡®")
        
        return validation_result
    
    @staticmethod
    def clean_numeric_data(data: Any) -> Optional[float]:
        """æ¸…æ´—æ•°å€¼æ•°æ®"""
        if data is None or data == "":
            return None
        
        if isinstance(data, (int, float)):
            return float(data)
        
        if isinstance(data, str):
            # ç§»é™¤å¸¸è§çš„éæ•°å­—å­—ç¬¦
            cleaned = data.replace(",", "").replace("%", "").replace("å…ƒ", "").strip()
            try:
                return float(cleaned)
            except ValueError:
                return None
        
        return None
    
    @staticmethod
    def normalize_commodity_name(name: str) -> str:
        """æ ‡å‡†åŒ–å•†å“åç§°"""
        name_mapping = {
            "èºçº¹é’¢": "RB", "çƒ­è½§å·æ¿": "HC", "é“çŸ¿çŸ³": "I", "ç„¦ç‚­": "J", "ç„¦ç…¤": "JM",
            "æ²ªé“œ": "CU", "æ²ªé“": "AL", "æ²ªé”Œ": "ZN", "æ²ªé•": "NI", "æ²ªé”¡": "SN",
            "é»„é‡‘": "AU", "ç™½é“¶": "AG", "æ©¡èƒ¶": "RU", "åŸæ²¹": "SC", "ç‡ƒæ²¹": "FU",
            "ç™½ç³–": "SR", "æ£‰èŠ±": "CF", "è±†ç²•": "M", "èœç²•": "RM", "è±†æ²¹": "Y"
        }
        
        return name_mapping.get(name, name.upper())

# ============================================================================
# 3. æ–‡ä»¶æ“ä½œå·¥å…·
# ============================================================================

class FileManager:
    """æ–‡ä»¶ç®¡ç†å™¨"""
    
    def __init__(self, base_dir: str):
        self.base_dir = Path(base_dir)
        self.logger = logging.getLogger("FileManager")
        
    def ensure_dir(self, path: Union[str, Path]) -> Path:
        """ç¡®ä¿ç›®å½•å­˜åœ¨"""
        dir_path = Path(path)
        dir_path.mkdir(parents=True, exist_ok=True)
        return dir_path
    
    def save_json(self, data: Any, file_path: Union[str, Path], 
                  ensure_dir: bool = True) -> bool:
        """ä¿å­˜JSONæ•°æ®"""
        try:
            file_path = Path(file_path)
            
            if ensure_dir:
                file_path.parent.mkdir(parents=True, exist_ok=True)
            
            with open(file_path, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2, default=str)
            
            return True
            
        except Exception as e:
            self.logger.error(f"ä¿å­˜JSONæ–‡ä»¶å¤±è´¥: {e}")
            return False
    
    def load_json(self, file_path: Union[str, Path]) -> Optional[Dict]:
        """åŠ è½½JSONæ•°æ®"""
        try:
            file_path = Path(file_path)
            
            if not file_path.exists():
                return None
            
            with open(file_path, 'r', encoding='utf-8') as f:
                return json.load(f)
                
        except Exception as e:
            self.logger.error(f"åŠ è½½JSONæ–‡ä»¶å¤±è´¥: {e}")
            return None
    
    def save_dataframe(self, df: pd.DataFrame, file_path: Union[str, Path],
                      format: str = "csv") -> bool:
        """ä¿å­˜DataFrame"""
        try:
            file_path = Path(file_path)
            file_path.parent.mkdir(parents=True, exist_ok=True)
            
            if format.lower() == "csv":
                df.to_csv(file_path, index=False, encoding='utf-8-sig')
            elif format.lower() == "excel":
                df.to_excel(file_path, index=False)
            elif format.lower() == "parquet":
                df.to_parquet(file_path, index=False)
            else:
                raise ValueError(f"ä¸æ”¯æŒçš„æ ¼å¼: {format}")
            
            return True
            
        except Exception as e:
            self.logger.error(f"ä¿å­˜DataFrameå¤±è´¥: {e}")
            return False
    
    def load_dataframe(self, file_path: Union[str, Path]) -> Optional[pd.DataFrame]:
        """åŠ è½½DataFrame"""
        try:
            file_path = Path(file_path)
            
            if not file_path.exists():
                return None
            
            suffix = file_path.suffix.lower()
            
            if suffix == ".csv":
                return pd.read_csv(file_path, encoding='utf-8-sig')
            elif suffix in [".xlsx", ".xls"]:
                return pd.read_excel(file_path)
            elif suffix == ".parquet":
                return pd.read_parquet(file_path)
            else:
                raise ValueError(f"ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {suffix}")
                
        except Exception as e:
            self.logger.error(f"åŠ è½½DataFrameå¤±è´¥: {e}")
            return None
    
    def get_file_info(self, file_path: Union[str, Path]) -> Dict:
        """è·å–æ–‡ä»¶ä¿¡æ¯"""
        file_path = Path(file_path)
        
        if not file_path.exists():
            return {"exists": False}
        
        stat = file_path.stat()
        
        return {
            "exists": True,
            "size_bytes": stat.st_size,
            "size_mb": stat.st_size / (1024 * 1024),
            "modified_time": datetime.fromtimestamp(stat.st_mtime),
            "created_time": datetime.fromtimestamp(stat.st_ctime),
            "is_file": file_path.is_file(),
            "is_dir": file_path.is_dir(),
            "extension": file_path.suffix
        }

# ============================================================================
# 4. ç¼“å­˜ç®¡ç†
# ============================================================================

class CacheManager:
    """ç¼“å­˜ç®¡ç†å™¨"""
    
    def __init__(self, cache_dir: str, expire_hours: int = 24):
        self.cache_dir = Path(cache_dir)
        self.expire_hours = expire_hours
        self.cache_dir.mkdir(parents=True, exist_ok=True)
        self.logger = logging.getLogger("CacheManager")
    
    def _get_cache_key(self, key: str) -> str:
        """ç”Ÿæˆç¼“å­˜é”®çš„å“ˆå¸Œå€¼"""
        return hashlib.md5(key.encode()).hexdigest()
    
    def _get_cache_path(self, key: str) -> Path:
        """è·å–ç¼“å­˜æ–‡ä»¶è·¯å¾„"""
        cache_key = self._get_cache_key(key)
        return self.cache_dir / f"{cache_key}.cache"
    
    def set(self, key: str, data: Any, expire_hours: Optional[int] = None) -> bool:
        """è®¾ç½®ç¼“å­˜"""
        try:
            cache_path = self._get_cache_path(key)
            expire_time = datetime.now() + timedelta(hours=expire_hours or self.expire_hours)
            
            cache_data = {
                "data": data,
                "expire_time": expire_time,
                "created_time": datetime.now(),
                "key": key
            }
            
            with open(cache_path, 'wb') as f:
                pickle.dump(cache_data, f)
            
            return True
            
        except Exception as e:
            self.logger.error(f"è®¾ç½®ç¼“å­˜å¤±è´¥: {e}")
            return False
    
    def get(self, key: str) -> Optional[Any]:
        """è·å–ç¼“å­˜"""
        try:
            cache_path = self._get_cache_path(key)
            
            if not cache_path.exists():
                return None
            
            with open(cache_path, 'rb') as f:
                cache_data = pickle.load(f)
            
            # æ£€æŸ¥æ˜¯å¦è¿‡æœŸ
            if datetime.now() > cache_data["expire_time"]:
                cache_path.unlink()  # åˆ é™¤è¿‡æœŸç¼“å­˜
                return None
            
            return cache_data["data"]
            
        except Exception as e:
            self.logger.error(f"è·å–ç¼“å­˜å¤±è´¥: {e}")
            return None
    
    def exists(self, key: str) -> bool:
        """æ£€æŸ¥ç¼“å­˜æ˜¯å¦å­˜åœ¨ä¸”æœªè¿‡æœŸ"""
        return self.get(key) is not None
    
    def delete(self, key: str) -> bool:
        """åˆ é™¤ç¼“å­˜"""
        try:
            cache_path = self._get_cache_path(key)
            if cache_path.exists():
                cache_path.unlink()
            return True
        except Exception as e:
            self.logger.error(f"åˆ é™¤ç¼“å­˜å¤±è´¥: {e}")
            return False
    
    def clear_expired(self) -> int:
        """æ¸…ç†è¿‡æœŸç¼“å­˜"""
        cleared_count = 0
        
        for cache_file in self.cache_dir.glob("*.cache"):
            try:
                with open(cache_file, 'rb') as f:
                    cache_data = pickle.load(f)
                
                if datetime.now() > cache_data["expire_time"]:
                    cache_file.unlink()
                    cleared_count += 1
                    
            except Exception as e:
                # æ— æ³•è¯»å–çš„ç¼“å­˜æ–‡ä»¶ä¹Ÿåˆ é™¤
                cache_file.unlink()
                cleared_count += 1
        
        self.logger.info(f"æ¸…ç†äº† {cleared_count} ä¸ªè¿‡æœŸç¼“å­˜æ–‡ä»¶")
        return cleared_count

# ============================================================================
# 5. é”™è¯¯å¤„ç†è£…é¥°å™¨
# ============================================================================

def retry_on_failure(max_retries: int = 3, delay: float = 1.0, 
                    backoff: float = 2.0, exceptions: tuple = (Exception,)):
    """é‡è¯•è£…é¥°å™¨"""
    def decorator(func: Callable) -> Callable:
        @functools.wraps(func)
        async def async_wrapper(*args, **kwargs):
            last_exception = None
            
            for attempt in range(max_retries + 1):
                try:
                    return await func(*args, **kwargs)
                except exceptions as e:
                    last_exception = e
                    if attempt < max_retries:
                        wait_time = delay * (backoff ** attempt)
                        logging.warning(f"å‡½æ•° {func.__name__} ç¬¬ {attempt + 1} æ¬¡å°è¯•å¤±è´¥: {e}ï¼Œ{wait_time:.1f}ç§’åé‡è¯•")
                        await asyncio.sleep(wait_time)
                    else:
                        logging.error(f"å‡½æ•° {func.__name__} åœ¨ {max_retries + 1} æ¬¡å°è¯•åä»ç„¶å¤±è´¥")
            
            raise last_exception
        
        @functools.wraps(func)
        def sync_wrapper(*args, **kwargs):
            last_exception = None
            
            for attempt in range(max_retries + 1):
                try:
                    return func(*args, **kwargs)
                except exceptions as e:
                    last_exception = e
                    if attempt < max_retries:
                        wait_time = delay * (backoff ** attempt)
                        logging.warning(f"å‡½æ•° {func.__name__} ç¬¬ {attempt + 1} æ¬¡å°è¯•å¤±è´¥: {e}ï¼Œ{wait_time:.1f}ç§’åé‡è¯•")
                        time.sleep(wait_time)
                    else:
                        logging.error(f"å‡½æ•° {func.__name__} åœ¨ {max_retries + 1} æ¬¡å°è¯•åä»ç„¶å¤±è´¥")
            
            raise last_exception
        
        return async_wrapper if asyncio.iscoroutinefunction(func) else sync_wrapper
    
    return decorator

def log_execution_time(logger: Optional[logging.Logger] = None):
    """æ‰§è¡Œæ—¶é—´è®°å½•è£…é¥°å™¨"""
    def decorator(func: Callable) -> Callable:
        log = logger or logging.getLogger(func.__module__)
        
        @functools.wraps(func)
        async def async_wrapper(*args, **kwargs):
            start_time = time.time()
            try:
                result = await func(*args, **kwargs)
                execution_time = time.time() - start_time
                log.info(f"å‡½æ•° {func.__name__} æ‰§è¡Œå®Œæˆï¼Œè€—æ—¶: {execution_time:.2f}ç§’")
                return result
            except Exception as e:
                execution_time = time.time() - start_time
                log.error(f"å‡½æ•° {func.__name__} æ‰§è¡Œå¤±è´¥ï¼Œè€—æ—¶: {execution_time:.2f}ç§’ï¼Œé”™è¯¯: {e}")
                raise
        
        @functools.wraps(func)
        def sync_wrapper(*args, **kwargs):
            start_time = time.time()
            try:
                result = func(*args, **kwargs)
                execution_time = time.time() - start_time
                log.info(f"å‡½æ•° {func.__name__} æ‰§è¡Œå®Œæˆï¼Œè€—æ—¶: {execution_time:.2f}ç§’")
                return result
            except Exception as e:
                execution_time = time.time() - start_time
                log.error(f"å‡½æ•° {func.__name__} æ‰§è¡Œå¤±è´¥ï¼Œè€—æ—¶: {execution_time:.2f}ç§’ï¼Œé”™è¯¯: {e}")
                raise
        
        return async_wrapper if asyncio.iscoroutinefunction(func) else sync_wrapper
    
    return decorator

# ============================================================================
# 6. æ—¶é—´å¤„ç†å·¥å…·
# ============================================================================

class TimeUtils:
    """æ—¶é—´å¤„ç†å·¥å…·ç±»"""
    
    @staticmethod
    def get_trading_days(start_date: str, end_date: str, 
                        exclude_weekends: bool = True) -> List[str]:
        """è·å–äº¤æ˜“æ—¥åˆ—è¡¨"""
        start = datetime.strptime(start_date, "%Y-%m-%d")
        end = datetime.strptime(end_date, "%Y-%m-%d")
        
        trading_days = []
        current = start
        
        while current <= end:
            if not exclude_weekends or current.weekday() < 5:  # å‘¨ä¸€åˆ°å‘¨äº”
                trading_days.append(current.strftime("%Y-%m-%d"))
            current += timedelta(days=1)
        
        return trading_days
    
    @staticmethod
    def get_recent_trading_day(offset_days: int = 0) -> str:
        """è·å–æœ€è¿‘çš„äº¤æ˜“æ—¥"""
        current = datetime.now() - timedelta(days=offset_days)
        
        # å¦‚æœæ˜¯å‘¨æœ«ï¼Œå›é€€åˆ°å‘¨äº”
        while current.weekday() >= 5:
            current -= timedelta(days=1)
        
        return current.strftime("%Y-%m-%d")
    
    @staticmethod
    def format_timestamp(timestamp: Optional[datetime] = None, 
                        format_str: str = "%Y-%m-%d %H:%M:%S") -> str:
        """æ ¼å¼åŒ–æ—¶é—´æˆ³"""
        if timestamp is None:
            timestamp = datetime.now()
        return timestamp.strftime(format_str)
    
    @staticmethod
    def parse_date_string(date_str: str, formats: List[str] = None) -> Optional[datetime]:
        """è§£ææ—¥æœŸå­—ç¬¦ä¸²"""
        if formats is None:
            formats = [
                "%Y-%m-%d",
                "%Y/%m/%d", 
                "%Y.%m.%d",
                "%Y-%m-%d %H:%M:%S",
                "%Y/%m/%d %H:%M:%S"
            ]
        
        for fmt in formats:
            try:
                return datetime.strptime(date_str, fmt)
            except ValueError:
                continue
        
        return None

# ============================================================================
# 7. æ•°æ®æ ¼å¼è½¬æ¢å·¥å…·
# ============================================================================

class DataConverter:
    """æ•°æ®æ ¼å¼è½¬æ¢å·¥å…·"""
    
    @staticmethod
    def dict_to_dataframe(data_dict: Dict, orient: str = "records") -> pd.DataFrame:
        """å­—å…¸è½¬DataFrame"""
        try:
            if orient == "records":
                return pd.DataFrame(data_dict)
            elif orient == "index":
                return pd.DataFrame.from_dict(data_dict, orient="index")
            elif orient == "columns":
                return pd.DataFrame.from_dict(data_dict, orient="columns")
            else:
                raise ValueError(f"ä¸æ”¯æŒçš„orientå‚æ•°: {orient}")
        except Exception as e:
            logging.error(f"å­—å…¸è½¬DataFrameå¤±è´¥: {e}")
            return pd.DataFrame()
    
    @staticmethod
    def dataframe_to_dict(df: pd.DataFrame, orient: str = "records") -> Dict:
        """DataFrameè½¬å­—å…¸"""
        try:
            return df.to_dict(orient=orient)
        except Exception as e:
            logging.error(f"DataFrameè½¬å­—å…¸å¤±è´¥: {e}")
            return {}
    
    @staticmethod
    def flatten_dict(nested_dict: Dict, separator: str = ".") -> Dict:
        """æ‰å¹³åŒ–åµŒå¥—å­—å…¸"""
        def _flatten(obj, parent_key=""):
            items = []
            if isinstance(obj, dict):
                for key, value in obj.items():
                    new_key = f"{parent_key}{separator}{key}" if parent_key else key
                    items.extend(_flatten(value, new_key).items())
            else:
                return {parent_key: obj}
            return dict(items)
        
        return _flatten(nested_dict)
    
    @staticmethod
    def unflatten_dict(flat_dict: Dict, separator: str = ".") -> Dict:
        """åæ‰å¹³åŒ–å­—å…¸"""
        nested_dict = {}
        
        for key, value in flat_dict.items():
            keys = key.split(separator)
            current = nested_dict
            
            for k in keys[:-1]:
                if k not in current:
                    current[k] = {}
                current = current[k]
            
            current[keys[-1]] = value
        
        return nested_dict

# ============================================================================
# 8. æµ‹è¯•å·¥å…·æ¨¡å—åŠŸèƒ½
# ============================================================================

async def test_tools():
    """æµ‹è¯•å·¥å…·æ¨¡å—åŠŸèƒ½"""
    
    print("ğŸ”§ æµ‹è¯•æœŸè´§Trading Agentså·¥å…·æ¨¡å—...")
    
    # 1. æµ‹è¯•æ•°æ®éªŒè¯å™¨
    print("\n1. æµ‹è¯•æ•°æ®éªŒè¯å™¨...")
    validator = DataValidator()
    
    print(f"   å•†å“ä»£ç éªŒè¯ 'RB': {validator.validate_commodity_code('RB')}")
    print(f"   å•†å“ä»£ç éªŒè¯ 'invalid': {validator.validate_commodity_code('invalid')}")
    print(f"   æ—¥æœŸæ ¼å¼éªŒè¯ '2025-01-19': {validator.validate_date_format('2025-01-19')}")
    
    # 2. æµ‹è¯•æ–‡ä»¶ç®¡ç†å™¨
    print("\n2. æµ‹è¯•æ–‡ä»¶ç®¡ç†å™¨...")
    file_manager = FileManager("./test_data")
    
    test_data = {"test": "data", "timestamp": datetime.now()}
    success = file_manager.save_json(test_data, "./test_data/test.json")
    print(f"   JSONä¿å­˜: {success}")
    
    loaded_data = file_manager.load_json("./test_data/test.json")
    print(f"   JSONåŠ è½½: {loaded_data is not None}")
    
    # 3. æµ‹è¯•ç¼“å­˜ç®¡ç†å™¨
    print("\n3. æµ‹è¯•ç¼“å­˜ç®¡ç†å™¨...")
    cache = CacheManager("./test_cache")
    
    cache.set("test_key", {"cached": "data"}, expire_hours=1)
    cached_data = cache.get("test_key")
    print(f"   ç¼“å­˜è®¾ç½®å’Œè·å–: {cached_data is not None}")
    
    # 4. æµ‹è¯•æ—¶é—´å·¥å…·
    print("\n4. æµ‹è¯•æ—¶é—´å·¥å…·...")
    time_utils = TimeUtils()
    
    recent_day = time_utils.get_recent_trading_day()
    trading_days = time_utils.get_trading_days("2025-01-15", "2025-01-19")
    print(f"   æœ€è¿‘äº¤æ˜“æ—¥: {recent_day}")
    print(f"   äº¤æ˜“æ—¥åˆ—è¡¨: {trading_days}")
    
    # 5. æµ‹è¯•æ•°æ®è½¬æ¢å™¨
    print("\n5. æµ‹è¯•æ•°æ®è½¬æ¢å™¨...")
    converter = DataConverter()
    
    test_dict = {"a": {"b": {"c": 1}}, "d": 2}
    flat_dict = converter.flatten_dict(test_dict)
    unflat_dict = converter.unflatten_dict(flat_dict)
    print(f"   å­—å…¸æ‰å¹³åŒ–: {flat_dict}")
    print(f"   å­—å…¸åæ‰å¹³åŒ–: {unflat_dict}")
    
    print("\nâœ… å·¥å…·æ¨¡å—æµ‹è¯•å®Œæˆï¼")

if __name__ == "__main__":
    # è¿è¡Œæµ‹è¯•
    asyncio.run(test_tools())
