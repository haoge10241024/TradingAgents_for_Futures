#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æŒä»“æ•°æ®æ›´æ–°å™¨
åŸºäºcomprehensive_positioning_data_systemçš„é€»è¾‘ï¼Œæ”¯æŒå¤šæ•°æ®æºå’Œå¢é‡æ›´æ–°
"""

import akshare as ak
import pandas as pd
from pathlib import Path
from datetime import datetime, timedelta
import time
import random
import json
import warnings
from typing import Dict, List, Optional, Tuple

warnings.filterwarnings('ignore')

# å“ç§åç§°æ˜ å°„
SYMBOL_NAMES = {
    'A': 'è±†ä¸€', 'AG': 'ç™½é“¶', 'AL': 'é“', 'AU': 'é»„é‡‘', 'B': 'è±†äºŒ',
    'BU': 'æ²¥é’', 'C': 'ç‰ç±³', 'CF': 'æ£‰èŠ±', 'CU': 'é“œ', 'CY': 'æ£‰çº±',
    'EB': 'è‹¯ä¹™çƒ¯', 'EG': 'ä¹™äºŒé†‡', 'FG': 'ç»ç’ƒ', 'FU': 'ç‡ƒæ²¹', 'HC': 'çƒ­å·',
    'I': 'é“çŸ¿çŸ³', 'J': 'ç„¦ç‚­', 'JD': 'é¸¡è›‹', 'JM': 'ç„¦ç…¤', 'L': 'èšä¹™çƒ¯',
    'LC': 'ç¢³é…¸é”‚', 'LH': 'ç”ŸçŒª', 'LU': 'ä½ç¡«ç‡ƒæ–™æ²¹', 'M': 'è±†ç²•', 'MA': 'ç”²é†‡',
    'NI': 'é•', 'NR': '20å·èƒ¶', 'OI': 'èœç±½æ²¹', 'P': 'æ£•æ¦ˆæ²¹', 'PB': 'é“…',
    'PF': 'çŸ­çº¤', 'PG': 'æ¶²åŒ–çŸ³æ²¹æ°”', 'PP': 'èšä¸™çƒ¯', 'PR': 'ç“¶ç‰‡', 'PS': 'å¤šæ™¶ç¡…',
    'PX': 'å¯¹äºŒç”²è‹¯', 'RB': 'èºçº¹é’¢', 'RM': 'èœç±½ç²•', 'RU': 'å¤©ç„¶æ©¡èƒ¶', 'SA': 'çº¯ç¢±',
    'SF': 'ç¡…é“', 'SI': 'å·¥ä¸šç¡…', 'SM': 'é”°ç¡…', 'SN': 'é”¡', 'SP': 'çº¸æµ†',
    'SR': 'ç™½ç³–', 'SS': 'ä¸é”ˆé’¢', 'TA': 'PTA', 'UR': 'å°¿ç´ ', 'V': 'PVC',
    'Y': 'è±†æ²¹', 'ZN': 'é”Œ'
}

class PositioningDataUpdater:
    """æŒä»“æ•°æ®æ›´æ–°å™¨"""
    
    def __init__(self, database_path: str = "D:/Cursor/cursoré¡¹ç›®/TradingAgent/qihuo/database/positioning"):
        """
        åˆå§‹åŒ–æŒä»“æ•°æ®æ›´æ–°å™¨
        
        Args:
            database_path: æ•°æ®åº“è·¯å¾„
        """
        self.database_path = Path(database_path)
        self.database_path.mkdir(parents=True, exist_ok=True)
        
        self.update_stats = {
            "start_time": None,
            "end_time": None,
            "target_date": None,
            "lookback_days": 0,
            "updated_varieties": [],
            "failed_varieties": [],
            "skipped_varieties": [],
            "new_varieties": [],
            "total_new_records": 0,
            "error_messages": []
        }
    
    def get_existing_data_status(self) -> Tuple[List[str], Dict]:
        """
        è·å–ç°æœ‰æ•°æ®çŠ¶æ€
        
        Returns:
            varieties: ç°æœ‰å“ç§åˆ—è¡¨
            variety_info: å„å“ç§è¯¦ç»†ä¿¡æ¯
        """
        print("ğŸ” æ£€æŸ¥ç°æœ‰æŒä»“æ•°æ®çŠ¶æ€...")
        
        varieties = []
        variety_info = {}
        
        if not self.database_path.exists():
            return [], {}
        
        variety_folders = [d for d in self.database_path.iterdir() if d.is_dir()]
        print(f"ğŸ“‚ å‘ç° {len(variety_folders)} ä¸ªå“ç§æ–‡ä»¶å¤¹")
        
        for folder in variety_folders:
            variety = folder.name
            
            # æ£€æŸ¥å„ç±»æŒä»“æ•°æ®æ–‡ä»¶
            files_info = {
                "long_position_ranking.csv": None,
                "short_position_ranking.csv": None,
                "volume_ranking.csv": None,
                "positioning_summary.json": None
            }
            
            has_data = False
            total_records = 0
            date_range = {"earliest": None, "latest": None}
            
            for filename in files_info.keys():
                file_path = folder / filename
                if file_path.exists():
                    try:
                        if filename.endswith('.csv'):
                            df = pd.read_csv(file_path)
                            if len(df) > 0 and 'date' in df.columns:
                                df['date'] = pd.to_datetime(df['date'])
                                file_earliest = df['date'].min()
                                file_latest = df['date'].max()
                                file_records = len(df)
                                
                                files_info[filename] = {
                                    "records": file_records,
                                    "earliest": file_earliest,
                                    "latest": file_latest
                                }
                                
                                total_records += file_records
                                has_data = True
                                
                                # æ›´æ–°å“ç§æ•´ä½“æ—¥æœŸèŒƒå›´
                                if date_range["earliest"] is None or file_earliest < date_range["earliest"]:
                                    date_range["earliest"] = file_earliest
                                if date_range["latest"] is None or file_latest > date_range["latest"]:
                                    date_range["latest"] = file_latest
                        
                        elif filename.endswith('.json'):
                            files_info[filename] = {"exists": True}
                            
                    except Exception as e:
                        files_info[filename] = {"error": str(e)[:50]}
            
            if has_data:
                variety_info[variety] = {
                    "files": files_info,
                    "total_records": total_records,
                    "date_range": date_range
                }
                varieties.append(variety)
                
                earliest_str = date_range["earliest"].strftime('%Y-%m-%d') if date_range["earliest"] else "æ— "
                latest_str = date_range["latest"].strftime('%Y-%m-%d') if date_range["latest"] else "æ— "
                print(f"  {variety}: {total_records} æ¡è®°å½• ({earliest_str} ~ {latest_str})")
        
        print(f"\nğŸ“Š æ€»è®¡: {len(varieties)} ä¸ªæœ‰æ•ˆå“ç§")
        return varieties, variety_info
    
    def generate_trading_dates(self, start_date: datetime, end_date: datetime) -> List[str]:
        """
        ç”Ÿæˆäº¤æ˜“æ—¥åˆ—è¡¨
        
        Args:
            start_date: å¼€å§‹æ—¥æœŸ
            end_date: ç»“æŸæ—¥æœŸ
        
        Returns:
            äº¤æ˜“æ—¥åˆ—è¡¨ (YYYYMMDDæ ¼å¼)
        """
        dates = []
        current = start_date
        
        while current <= end_date:
            # ç®€å•çš„å·¥ä½œæ—¥åˆ¤æ–­ï¼ˆä¸è€ƒè™‘èŠ‚å‡æ—¥ï¼‰
            if current.weekday() < 5:
                dates.append(current.strftime('%Y%m%d'))
            current += timedelta(days=1)
        
        return dates
    
    def get_dominant_contracts(self, symbol: str, start_date: datetime, end_date: datetime) -> Dict[str, str]:
        """
        è·å–ä¸»åŠ›åˆçº¦ä¿¡æ¯ï¼ˆæ¨¡æ‹Ÿå®ç°ï¼‰
        
        Args:
            symbol: å“ç§ä»£ç 
            start_date: å¼€å§‹æ—¥æœŸ
            end_date: ç»“æŸæ—¥æœŸ
        
        Returns:
            æ—¥æœŸåˆ°ä¸»åŠ›åˆçº¦çš„æ˜ å°„
        """
        # ç®€åŒ–å®ç°ï¼šç”Ÿæˆå½“å‰å¹´ä»½çš„ä¸»è¦åˆçº¦æœˆä»½
        current_year = datetime.now().year
        year_suffix = str(current_year)[-2:]
        
        # å¸¸è§çš„ä¸»åŠ›åˆçº¦æœˆä»½
        main_months = ['01', '03', '05', '07', '09', '11']
        
        # æ„é€ ä¸»åŠ›åˆçº¦ä»£ç 
        contracts = [f"{symbol.lower()}{year_suffix}{month}" for month in main_months]
        
        # ä¸ºæ¯ä¸ªäº¤æ˜“æ—¥åˆ†é…ä¸»åŠ›åˆçº¦ï¼ˆç®€åŒ–é€»è¾‘ï¼‰
        trading_dates = self.generate_trading_dates(start_date, end_date)
        contract_map = {}
        
        for date_str in trading_dates:
            # æ ¹æ®æ—¥æœŸé€‰æ‹©åˆçº¦ï¼ˆç®€åŒ–é€»è¾‘ï¼‰
            month = int(date_str[4:6])
            contract_index = min(month // 2, len(contracts) - 1)
            contract_map[date_str] = contracts[contract_index]
        
        return contract_map
    
    def fetch_positioning_data_by_contracts(self, dominant_contracts: Dict[str, Dict[str, str]], 
                                          start_date: datetime, target_date: datetime) -> Dict[str, List]:
        """
        åŸºäºä¸»åŠ›åˆçº¦è·å–æŒä»“æ•°æ®
        
        Args:
            dominant_contracts: {symbol: {date: contract}} ä¸»åŠ›åˆçº¦ä¿¡æ¯
            start_date: å¼€å§‹æ—¥æœŸ
            target_date: ç»“æŸæ—¥æœŸ
        
        Returns:
            æŒ‰å“ç§åˆ†ç»„çš„æŒä»“æ•°æ®
        """
        print(f"    ğŸ“¡ åŸºäºä¸»åŠ›åˆçº¦è·å–æŒä»“æ•°æ®...")
        print(f"    ğŸ“… æ•°æ®æ—¶é—´èŒƒå›´: {start_date.strftime('%Y-%m-%d')} ~ {target_date.strftime('%Y-%m-%d')}")
        
        # ç”Ÿæˆäº¤æ˜“æ—¥åˆ—è¡¨
        trading_dates = self.generate_trading_dates(start_date, target_date)
        print(f"    ğŸ“… äº¤æ˜“æ—¥æ•°é‡: {len(trading_dates)} å¤©")
        
        # æŒä»“æ•°æ®ç±»å‹
        position_types = ["æˆäº¤é‡", "å¤šå•æŒä»“", "ç©ºå•æŒä»“"]
        
        all_positioning_data = {}  # {symbol: [data_list]}
        total_requests = 0
        successful_requests = 0
        
        for symbol, contracts_dict in dominant_contracts.items():
            print(f"\n    ğŸ” å¤„ç†å“ç§: {symbol} ({SYMBOL_NAMES.get(symbol, symbol)})")
            
            symbol_data = []
            symbol_requests = 0
            symbol_success = 0
            
            for date_str in trading_dates:
                if date_str not in contracts_dict:
                    continue
                
                contract = contracts_dict[date_str]
                print(f"      ğŸ“… {date_str[:4]}-{date_str[4:6]}-{date_str[6:8]} | {contract}", end=" ")
                
                daily_success = 0
                
                # è·å–ä¸‰ç§ç±»å‹çš„æŒä»“æ•°æ®
                for position_type in position_types:
                    try:
                        df = ak.futures_hold_pos_sina(
                            symbol=position_type, 
                            contract=contract, 
                            date=date_str
                        )
                        
                        symbol_requests += 1
                        total_requests += 1
                        
                        if df is not None and not df.empty:
                            # æ·»åŠ å…ƒæ•°æ®
                            df = df.copy()
                            df['date'] = date_str
                            df['contract'] = contract
                            df['position_type'] = position_type
                            df['symbol'] = symbol
                            
                            # æ ‡å‡†åŒ–åˆ—å
                            if len(df.columns) >= 4:
                                if position_type == "å¤šå•æŒä»“":
                                    df.columns = ['æ’å', 'ä¼šå‘˜ç®€ç§°', 'æŒä»“é‡', 'æ¯”ä¸Šäº¤æ˜“å¢å‡', 'date', 'contract', 'position_type', 'symbol']
                                elif position_type == "ç©ºå•æŒä»“":
                                    df.columns = ['æ’å', 'ä¼šå‘˜ç®€ç§°', 'æŒä»“é‡', 'æ¯”ä¸Šäº¤æ˜“å¢å‡', 'date', 'contract', 'position_type', 'symbol']
                                elif position_type == "æˆäº¤é‡":
                                    df.columns = ['æ’å', 'ä¼šå‘˜ç®€ç§°', 'æˆäº¤é‡', 'æ¯”ä¸Šäº¤æ˜“å¢å‡', 'date', 'contract', 'position_type', 'symbol']
                            
                            symbol_data.append(df)
                            symbol_success += 1
                            successful_requests += 1
                            daily_success += 1
                        
                        # é¿å…è¯·æ±‚è¿‡å¿«
                        time.sleep(random.uniform(0.5, 1.0))
                        
                    except Exception as e:
                        print(f"âŒ{position_type[:2]}", end="")
                        continue
                
                if daily_success > 0:
                    print(f" âœ…({daily_success}/3)")
                else:
                    print(" âŒ")
            
            if symbol_data:
                all_positioning_data[symbol] = symbol_data
                success_rate = symbol_success / symbol_requests * 100 if symbol_requests > 0 else 0
                print(f"      âœ… {symbol}: è·å– {len(symbol_data)} æ‰¹æ•°æ® (æˆåŠŸç‡: {success_rate:.1f}%)")
            else:
                print(f"      âŒ {symbol}: æœªè·å–åˆ°ä»»ä½•æ•°æ®")
        
        overall_success_rate = successful_requests / total_requests * 100 if total_requests > 0 else 0
        print(f"    ğŸ“Š æ€»ä½“ç»Ÿè®¡: {successful_requests}/{total_requests} è¯·æ±‚æˆåŠŸ (æˆåŠŸç‡: {overall_success_rate:.1f}%)")
        
        return all_positioning_data
    
    def load_dominant_contracts_from_basis(self, start_date: datetime, target_date: datetime) -> Dict[str, Dict[str, str]]:
        """
        ä»åŸºå·®æ•°æ®ä¸­æå–å„å“ç§çš„ä¸»åŠ›åˆçº¦ä¿¡æ¯
        
        Args:
            start_date: å¼€å§‹æ—¥æœŸ
            target_date: ç»“æŸæ—¥æœŸ
            
        Returns:
            {symbol: {date: contract}} ä¸»åŠ›åˆçº¦ä¿¡æ¯
        """
        print(f"    ğŸ“– ä»åŸºå·®æ•°æ®ä¸­æå–ä¸»åŠ›åˆçº¦ä¿¡æ¯...")
        
        # åŸºå·®æ•°æ®ç›®å½•
        basis_root = self.database_path.parent / "basis"
        
        dominant_contracts = {}
        
        for symbol in SYMBOL_NAMES.keys():
            basis_file = basis_root / symbol / "basis_data.csv"
            
            if not basis_file.exists():
                print(f"      âš ï¸ {symbol}: åŸºå·®æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨")
                continue
            
            try:
                df = pd.read_csv(basis_file)
                
                if 'date' not in df.columns or 'dominant_contract' not in df.columns:
                    print(f"      âš ï¸ {symbol}: åŸºå·®æ•°æ®æ ¼å¼ä¸æ­£ç¡®")
                    continue
                
                # è½¬æ¢æ—¥æœŸæ ¼å¼
                df['date'] = pd.to_datetime(df['date'])
                
                # ç­›é€‰æ—¶é—´èŒƒå›´
                mask = (df['date'] >= start_date) & (df['date'] <= target_date)
                filtered_df = df[mask].copy()
                
                if len(filtered_df) == 0:
                    print(f"      âš ï¸ {symbol}: æŒ‡å®šæ—¶é—´èŒƒå›´å†…æ— åŸºå·®æ•°æ®")
                    continue
                
                # æå–ä¸»åŠ›åˆçº¦ä¿¡æ¯
                contracts_dict = {}
                for _, row in filtered_df.iterrows():
                    date_str = row['date'].strftime('%Y%m%d')
                    contract = str(row['dominant_contract']).strip()
                    if contract and contract != 'nan':
                        # ä¿®å¤åˆçº¦ä»£ç æ ¼å¼ï¼š3ä½æ•°å­—çš„åˆçº¦éœ€è¦åœ¨è‹±æ–‡åè¡¥2
                        # ä¾‹å¦‚ï¼šCY511 -> CY2511
                        if len(contract) >= 5:  # è‡³å°‘è¦æœ‰å­—æ¯+æ•°å­—
                            # æ‰¾åˆ°å­—æ¯å’Œæ•°å­—çš„åˆ†ç•Œç‚¹
                            alpha_part = ""
                            num_part = ""
                            for i, char in enumerate(contract):
                                if char.isalpha():
                                    alpha_part += char
                                elif char.isdigit():
                                    num_part = contract[i:]
                                    break
                            
                            # å¦‚æœæ•°å­—éƒ¨åˆ†æ˜¯3ä½ï¼Œåœ¨å‰é¢è¡¥2
                            if len(num_part) == 3 and num_part.isdigit():
                                contract = alpha_part + "2" + num_part
                                print(f"        ğŸ”§ ä¿®æ­£åˆçº¦ä»£ç : {row['dominant_contract']} -> {contract}")
                        
                        contracts_dict[date_str] = contract
                
                dominant_contracts[symbol] = contracts_dict
                print(f"      âœ… {symbol}: æå–äº† {len(contracts_dict)} ä¸ªäº¤æ˜“æ—¥çš„ä¸»åŠ›åˆçº¦")
                
            except Exception as e:
                print(f"      âŒ {symbol}: å¤„ç†åŸºå·®æ•°æ®å¤±è´¥ - {str(e)[:50]}")
                continue
        
        print(f"    âœ… ä¸»åŠ›åˆçº¦ä¿¡æ¯æå–å®Œæˆï¼Œå…± {len(dominant_contracts)} ä¸ªå“ç§")
        return dominant_contracts
    
    def _extract_symbol_from_contract(self, contract: str) -> str:
        """ä»åˆçº¦ä»£ç ä¸­æå–å“ç§ä»£ç """
        import re
        
        # ç§»é™¤æ•°å­—ï¼Œä¿ç•™å­—æ¯
        symbol = re.sub(r'\d+', '', str(contract).upper())
        
        # å¤„ç†ç‰¹æ®Šæƒ…å†µ
        if symbol in ['IF', 'IH', 'IC', 'IM', 'TS', 'TF', 'T']:  # ä¸­é‡‘æ‰€å“ç§
            return symbol
        elif len(symbol) >= 1:
            return symbol
        else:
            return str(contract)[:2].upper()
    
    def _process_contract_positioning_data(self, contract_data: pd.DataFrame, 
                                         contract: str, symbol: str, date_str: str,
                                         exchange_name: str, all_data: Dict):
        """å¤„ç†å•ä¸ªåˆçº¦çš„æŒä»“æ•°æ®"""
        
        # ä¸åŒäº¤æ˜“æ‰€çš„åˆ—åæ˜ å°„
        column_mappings = {
            "ä¸­é‡‘æ‰€": {
                "long_party_name": "ä¼šå‘˜ç®€ç§°",
                "long_open_interest": "æŒä»“é‡", 
                "long_open_interest_chg": "æ¯”ä¸Šäº¤æ˜“å¢å‡",
                "short_party_name": "ä¼šå‘˜ç®€ç§°",
                "short_open_interest": "æŒä»“é‡",
                "short_open_interest_chg": "æ¯”ä¸Šäº¤æ˜“å¢å‡",
                "vol_party_name": "ä¼šå‘˜ç®€ç§°",
                "vol": "æˆäº¤é‡",
                "vol_chg": "æ¯”ä¸Šäº¤æ˜“å¢å‡"
            },
            "éƒ‘å•†æ‰€": {
                "long_party_name": "ä¼šå‘˜ç®€ç§°",
                "long_open_interest": "æŒä»“é‡",
                "long_open_interest_chg": "æ¯”ä¸Šäº¤æ˜“å¢å‡", 
                "short_party_name": "ä¼šå‘˜ç®€ç§°",
                "short_open_interest": "æŒä»“é‡",
                "short_open_interest_chg": "æ¯”ä¸Šäº¤æ˜“å¢å‡",
                "vol_party_name": "ä¼šå‘˜ç®€ç§°",
                "vol": "æˆäº¤é‡"
            },
            "ä¸ŠæœŸæ‰€": {
                "long_party_name": "ä¼šå‘˜ç®€ç§°",
                "long_open_interest": "æŒä»“é‡",
                "long_open_interest_chg": "æ¯”ä¸Šäº¤æ˜“å¢å‡",
                "short_party_name": "ä¼šå‘˜ç®€ç§°", 
                "short_open_interest": "æŒä»“é‡",
                "short_open_interest_chg": "æ¯”ä¸Šäº¤æ˜“å¢å‡",
                "vol_party_name": "ä¼šå‘˜ç®€ç§°",
                "vol": "æˆäº¤é‡"
            },
            "å¹¿æœŸæ‰€": {
                "long_party_name": "ä¼šå‘˜ç®€ç§°",
                "long_open_interest": "æŒä»“é‡",
                "short_party_name": "ä¼šå‘˜ç®€ç§°",
                "short_open_interest": "æŒä»“é‡", 
                "vol_party_name": "ä¼šå‘˜ç®€ç§°",
                "vol": "æˆäº¤é‡"
            }
        }
        
        mapping = column_mappings.get(exchange_name, {})
        
        # å¤„ç†å¤šå¤´æŒä»“
        if 'long_party_name' in contract_data.columns:
            for i, (_, row) in enumerate(contract_data.iterrows()):
                if i >= 20:  # é™åˆ¶æ•°é‡
                    break
                    
                all_data['long_positions'].append({
                    "æ’å": i + 1,
                    "ä¼šå‘˜ç®€ç§°": str(row.get('long_party_name', f'ä¼šå‘˜{i+1}')),
                    "æŒä»“é‡": self._parse_int_with_comma(row.get('long_open_interest', 0)),
                    "æ¯”ä¸Šäº¤æ˜“å¢å‡": self._parse_int_with_comma(row.get('long_open_interest_chg', 0)),
                    "date": date_str,
                    "contract": contract,
                    "position_type": "å¤šå•æŒä»“",
                    "symbol": symbol,
                    "exchange": exchange_name
                })
        
        # å¤„ç†ç©ºå¤´æŒä»“
        if 'short_party_name' in contract_data.columns:
            for i, (_, row) in enumerate(contract_data.iterrows()):
                if i >= 20:  # é™åˆ¶æ•°é‡
                    break
                    
                all_data['short_positions'].append({
                    "æ’å": i + 1,
                    "ä¼šå‘˜ç®€ç§°": str(row.get('short_party_name', f'ä¼šå‘˜{i+1}')),
                    "æŒä»“é‡": self._parse_int_with_comma(row.get('short_open_interest', 0)),
                    "æ¯”ä¸Šäº¤æ˜“å¢å‡": self._parse_int_with_comma(row.get('short_open_interest_chg', 0)),
                    "date": date_str,
                    "contract": contract,
                    "position_type": "ç©ºå•æŒä»“", 
                    "symbol": symbol,
                    "exchange": exchange_name
                })
        
        # å¤„ç†æˆäº¤é‡æ’å
        if 'vol_party_name' in contract_data.columns:
            for i, (_, row) in enumerate(contract_data.iterrows()):
                if i >= 20:  # é™åˆ¶æ•°é‡
                    break
                    
                all_data['volume_rankings'].append({
                    "æ’å": i + 1,
                    "ä¼šå‘˜ç®€ç§°": str(row.get('vol_party_name', f'ä¼šå‘˜{i+1}')),
                    "æˆäº¤é‡": self._parse_int_with_comma(row.get('vol', 0)),
                    "æ¯”ä¸Šäº¤æ˜“å¢å‡": self._parse_int_with_comma(row.get('vol_chg', 0)),
                    "date": date_str,
                    "contract": contract,
                    "symbol": symbol,
                    "exchange": exchange_name
                })
    
    def _parse_int_with_comma(self, value):
        """è§£æå¯èƒ½åŒ…å«é€—å·çš„æ•°å­—å­—ç¬¦ä¸²"""
        if value is None or value == '':
            return 0
        
        try:
            # å¦‚æœæ˜¯æ•°å­—ï¼Œç›´æ¥è¿”å›
            if isinstance(value, (int, float)):
                return int(value)
            
            # å¦‚æœæ˜¯å­—ç¬¦ä¸²ï¼Œç§»é™¤é€—å·åè½¬æ¢
            if isinstance(value, str):
                # ç§»é™¤é€—å·å’Œç©ºæ ¼
                clean_value = value.replace(',', '').replace(' ', '').strip()
                if clean_value == '' or clean_value == '-':
                    return 0
                return int(float(clean_value))
            
            return int(value)
        except (ValueError, TypeError):
            return 0
    
    def _fetch_dce_data_with_fallback(self, date: str):
        """
        å¤§å•†æ‰€æ•°æ®è·å–ï¼ˆå¤šç§å¤‡ç”¨ç­–ç•¥ï¼‰
        
        Args:
            date: æ—¥æœŸå­—ç¬¦ä¸² (YYYYMMDD)
            
        Returns:
            å¤§å•†æ‰€æŒä»“æ•°æ®å­—å…¸ï¼Œæ ¼å¼ä¸å…¶ä»–äº¤æ˜“æ‰€ä¸€è‡´
        """
        print(f"        ğŸ¢ å¤§å•†æ‰€æ•°æ®è·å– (å¤šç­–ç•¥)...")
        
        strategies = [
            {
                "name": "ä¸»æ¥å£",
                "func": ak.futures_dce_position_rank,
                "params": {"date": date}
            },
            {
                "name": "å¤‡ç”¨æ¥å£", 
                "func": ak.futures_dce_position_rank_other,
                "params": {"date": date}
            },
            {
                "name": "æ’åè¡¨æ¥å£",
                "func": ak.get_dce_rank_table,
                "params": {"date": date}
            }
        ]
        
        for strategy in strategies:
            try:
                print(f"          ğŸ”„ å°è¯•{strategy['name']}...", end="")
                data = strategy['func'](**strategy['params'])
                
                if data and isinstance(data, dict) and len(data) > 0:
                    print(f" âœ… æˆåŠŸ ({len(data)}ä¸ªåˆçº¦)")
                    return data
                else:
                    print(f" âŒ æ— æ•°æ®")
                    
            except Exception as e:
                print(f" âŒ å¤±è´¥: {str(e)[:30]}...")
                continue
        
        # å¦‚æœæ‰€æœ‰ç­–ç•¥éƒ½å¤±è´¥ï¼Œè®°å½•é—®é¢˜å¹¶è¿”å›ç©ºå­—å…¸
        print(f"        âš ï¸ å¤§å•†æ‰€æ•°æ®æš‚æ—¶æ— æ³•è·å–ï¼ŒåŸå› :")
        print(f"          - ä¸»æ¥å£: æ•°æ®æºå¯èƒ½å­˜åœ¨é—®é¢˜")
        print(f"          - å¤‡ç”¨æ¥å£: ç½‘ç«™ç»“æ„å¯èƒ½å·²å˜åŒ–")
        print(f"          - æ’åè¡¨æ¥å£: æ¥å£å¯èƒ½å·²åœç”¨")
        print(f"        ğŸ“ å»ºè®®: å¤§å•†æ‰€æ•°æ®å°†åœ¨æ¥å£ä¿®å¤åè‡ªåŠ¨æ¢å¤")
        
        # è¿”å›ç©ºå­—å…¸ï¼Œä¸å…¶ä»–æ¥å£ä¿æŒä¸€è‡´çš„æ ¼å¼
        return {}
    

    
    def save_positioning_data(self, symbol: str, all_data: List[pd.DataFrame]) -> Tuple[bool, int]:
        """
        ä¿å­˜å•ä¸ªå“ç§çš„æŒä»“æ•°æ®
        
        Args:
            symbol: å“ç§ä»£ç 
            all_data: è¯¥å“ç§çš„æ‰€æœ‰æŒä»“æ•°æ®åˆ—è¡¨
            
        Returns:
            (æ˜¯å¦æˆåŠŸ, æ€»è®°å½•æ•°)
        """
        try:
            # åˆ›å»ºå“ç§ç›®å½•
            symbol_dir = self.database_path / symbol
            symbol_dir.mkdir(parents=True, exist_ok=True)
            
            # æŒä»“æ•°æ®ç±»å‹
            position_types = ["æˆäº¤é‡", "å¤šå•æŒä»“", "ç©ºå•æŒä»“"]
            
            total_records = 0
            
            # æŒ‰æ•°æ®ç±»å‹åˆ†åˆ«ä¿å­˜
            for position_type in position_types:
                type_data = [df for df in all_data if not df.empty and df.iloc[0]['position_type'] == position_type]
                
                if not type_data:
                    continue
                    
                # åˆå¹¶åŒç±»å‹æ•°æ®
                combined_df = pd.concat(type_data, ignore_index=True)
                
                # è½¬æ¢æ—¥æœŸæ ¼å¼
                combined_df['date'] = pd.to_datetime(combined_df['date'], format='%Y%m%d')
                
                # æŒ‰æ—¥æœŸæ’åº
                combined_df = combined_df.sort_values(['date', 'æ’å']).reset_index(drop=True)
                
                # ä¿å­˜æ–‡ä»¶
                filename_map = {
                    "æˆäº¤é‡": "volume_ranking.csv",
                    "å¤šå•æŒä»“": "long_position_ranking.csv", 
                    "ç©ºå•æŒä»“": "short_position_ranking.csv"
                }
                
                file_path = symbol_dir / filename_map[position_type]
                combined_df.to_csv(file_path, index=False, encoding='utf-8-sig')
                
                total_records += len(combined_df)
                print(f"      ğŸ’¾ {position_type}: {len(combined_df)} æ¡è®°å½•å·²ä¿å­˜")
            
            # ä¿å­˜æ±‡æ€»ä¿¡æ¯
            summary_data = {
                'symbol': symbol,
                'symbol_name': SYMBOL_NAMES.get(symbol, symbol),
                'total_records': total_records,
                'position_types': position_types,
                'update_time': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
            }
            
            summary_file = symbol_dir / "positioning_summary.json"
            with open(summary_file, 'w', encoding='utf-8') as f:
                json.dump(summary_data, f, ensure_ascii=False, indent=2)
            
            return True, total_records
            
        except Exception as e:
            print(f"      âŒ ä¿å­˜å¤±è´¥: {e}")
            return False, 0
    
    def save_variety_data(self, symbol: str, new_data: Dict, existing_info: Optional[Dict] = None) -> bool:
        """
        ä¿å­˜å“ç§æ•°æ®
        
        Args:
            symbol: å“ç§ä»£ç 
            new_data: æ–°æ•°æ®
            existing_info: ç°æœ‰æ•°æ®ä¿¡æ¯
        
        Returns:
            æ˜¯å¦ä¿å­˜æˆåŠŸ
        """
        try:
            variety_dir = self.database_path / symbol
            variety_dir.mkdir(parents=True, exist_ok=True)
            
            new_records_count = 0
            
            # ä¿å­˜å„ç±»æ•°æ®
            for data_type, file_name in [
                ("long_positions", "long_position_ranking.csv"),
                ("short_positions", "short_position_ranking.csv"),
                ("volume_rankings", "volume_ranking.csv")
            ]:
                if data_type not in new_data or not new_data[data_type]:
                    continue
                
                file_path = variety_dir / file_name
                new_df = pd.DataFrame(new_data[data_type])
                
                if file_path.exists():
                    # åˆå¹¶æ•°æ®
                    existing_df = pd.read_csv(file_path)
                    combined_df = pd.concat([existing_df, new_df], ignore_index=True)
                    
                    # å»é‡
                    key_columns = ['date', 'contract'] + (['-name'] if 'ä¼šå‘˜ç®€ç§°' in combined_df.columns else [])
                    if 'ä¼šå‘˜ç®€ç§°' in combined_df.columns:
                        key_columns = ['date', 'contract', 'ä¼šå‘˜ç®€ç§°']
                    combined_df = combined_df.drop_duplicates(subset=key_columns).reset_index(drop=True)
                    
                    new_records_count += len(combined_df) - len(existing_df)
                else:
                    # æ–°æ–‡ä»¶
                    combined_df = new_df
                    new_records_count += len(new_df)
                
                combined_df.to_csv(file_path, index=False, encoding='utf-8')
            
            # ä¿å­˜æ‘˜è¦ä¿¡æ¯
            summary_file = variety_dir / "positioning_summary.json"
            summary_info = {
                "symbol": symbol,
                "symbol_name": SYMBOL_NAMES.get(symbol, symbol),
                "last_updated": datetime.now().isoformat(),
                "files": {
                    "long_position_ranking": len(new_data.get("long_positions", [])),
                    "short_position_ranking": len(new_data.get("short_positions", [])),
                    "volume_ranking": len(new_data.get("volume_rankings", []))
                }
            }
            
            with open(summary_file, 'w', encoding='utf-8') as f:
                json.dump(summary_info, f, ensure_ascii=False, indent=2)
            
            if new_records_count > 0:
                print(f"    âœ… {symbol}: æ–°å¢ {new_records_count} æ¡è®°å½•")
                self.update_stats["updated_varieties"].append(symbol)
                self.update_stats["total_new_records"] += new_records_count
            else:
                print(f"    â„¹ï¸ {symbol}: æ— æ–°æ•°æ®")
                self.update_stats["skipped_varieties"].append(symbol)
            
            return True
            
        except Exception as e:
            print(f"    âŒ {symbol}: ä¿å­˜å¤±è´¥ - {str(e)}")
            self.update_stats["failed_varieties"].append(symbol)
            self.update_stats["error_messages"].append(f"{symbol}: ä¿å­˜å¤±è´¥ - {str(e)}")
            return False
    
    def update_to_date(self, target_date_str: str, lookback_days: int = 5, specific_varieties: Optional[List[str]] = None) -> Dict:
        """
        æ›´æ–°æ•°æ®åˆ°æŒ‡å®šæ—¥æœŸ
        
        Args:
            target_date_str: ç›®æ ‡æ—¥æœŸ (YYYY-MM-DDæ ¼å¼)
            lookback_days: å›æœ›å¤©æ•°
            specific_varieties: æŒ‡å®šå“ç§åˆ—è¡¨ï¼ŒNoneè¡¨ç¤ºå…¨éƒ¨å“ç§
        
        Returns:
            æ›´æ–°ç»“æœç»Ÿè®¡
        """
        print(f"ğŸš€ æŒä»“æ•°æ®æ›´æ–°å™¨")
        print("=" * 60)
        
        # è§£æç›®æ ‡æ—¥æœŸ
        try:
            target_date = datetime.strptime(target_date_str, '%Y-%m-%d')
        except ValueError:
            try:
                target_date = datetime.strptime(target_date_str, '%Y%m%d')
            except ValueError:
                raise ValueError(f"æ—¥æœŸæ ¼å¼é”™è¯¯: {target_date_str}ï¼Œè¯·ä½¿ç”¨ YYYY-MM-DD æˆ– YYYYMMDD æ ¼å¼")
        
        self.update_stats["start_time"] = datetime.now()
        self.update_stats["target_date"] = target_date_str
        self.update_stats["lookback_days"] = lookback_days
        
        # è®¡ç®—æ—¥æœŸèŒƒå›´
        start_date = target_date - timedelta(days=lookback_days)
        
        print(f"ğŸ“… æ›´æ–°æ—¥æœŸèŒƒå›´: {start_date.strftime('%Y-%m-%d')} ~ {target_date.strftime('%Y-%m-%d')}")
        print(f"ğŸ“Š å›æœ›å¤©æ•°: {lookback_days}")
        
        # è·å–ç°æœ‰æ•°æ®çŠ¶æ€
        existing_varieties, variety_info = self.get_existing_data_status()
        
        # ç¡®å®šè¦æ›´æ–°çš„å“ç§
        if specific_varieties:
            target_symbols = [s for s in specific_varieties if s.upper() in SYMBOL_NAMES]
            print(f"ğŸ¯ æŒ‡å®šæ›´æ–°å“ç§: {len(target_symbols)} ä¸ª")
        else:
            target_symbols = list(SYMBOL_NAMES.keys())
            print(f"ğŸ¯ å…¨å“ç§æ›´æ–°: {len(target_symbols)} ä¸ª")
        
        # ç”Ÿæˆäº¤æ˜“æ—¥æœŸ
        trading_dates = self.generate_trading_dates(start_date, target_date)
        print(f"ğŸ“… è®¡åˆ’å¤„ç† {len(trading_dates)} ä¸ªäº¤æ˜“æ—¥")
        
        # æ‰§è¡Œæ›´æ–° - åŸºäºä¸»åŠ›åˆçº¦è·å–æŒä»“æ•°æ®
        processed_count = 0
        
        print(f"\nğŸ”„ å¼€å§‹åŸºäºä¸»åŠ›åˆçº¦è·å–æŒä»“æ•°æ®...")
        
        # 1. ä»åŸºå·®æ•°æ®ä¸­æå–ä¸»åŠ›åˆçº¦ä¿¡æ¯
        dominant_contracts = self.load_dominant_contracts_from_basis(start_date, target_date)
        
        if not dominant_contracts:
            print("âŒ æ— æ³•è·å–ä¸»åŠ›åˆçº¦ä¿¡æ¯ï¼Œæ— æ³•æ›´æ–°æŒä»“æ•°æ®")
            self.update_stats["end_time"] = datetime.now()
            return
        
        # å¦‚æœæŒ‡å®šäº†å“ç§ï¼Œåªå¤„ç†æŒ‡å®šå“ç§
        if specific_varieties:
            filtered_contracts = {}
            for symbol in specific_varieties:
                if symbol in dominant_contracts:
                    filtered_contracts[symbol] = dominant_contracts[symbol]
            dominant_contracts = filtered_contracts
        
        if not dominant_contracts:
            print("âŒ æŒ‡å®šçš„å“ç§éƒ½æ²¡æœ‰ä¸»åŠ›åˆçº¦ä¿¡æ¯")
            self.update_stats["end_time"] = datetime.now()
            return
        
        # 2. åŸºäºä¸»åŠ›åˆçº¦è·å–æŒä»“æ•°æ®
        all_positioning_data = self.fetch_positioning_data_by_contracts(
            dominant_contracts, start_date, target_date
        )
        
        print(f"\nğŸ’¾ å¼€å§‹ä¿å­˜å“ç§æ•°æ®...")
        
        # 3. ä¿å­˜å„å“ç§æ•°æ®
        for symbol, symbol_data in all_positioning_data.items():
            print(f"\n  å¤„ç†å“ç§: {symbol} ({SYMBOL_NAMES.get(symbol, symbol)})")
            
            if symbol_data:
                success, record_count = self.save_positioning_data(symbol, symbol_data)
                if success:
                    processed_count += 1
                    print(f"    âœ… {symbol}: æˆåŠŸä¿å­˜ {record_count} æ¡è®°å½•")
                    self.update_stats["updated_varieties"].append(symbol)
                else:
                    print(f"    âŒ {symbol}: ä¿å­˜å¤±è´¥")
                    self.update_stats["failed_varieties"].append(symbol)
            else:
                print(f"    âŒ {symbol}: æ— æœ‰æ•ˆæ•°æ®")
                self.update_stats["failed_varieties"].append(symbol)
        
        # å®Œæˆç»Ÿè®¡
        self.update_stats["end_time"] = datetime.now()
        
        print(f"\nğŸ“Š æ›´æ–°å®Œæˆç»Ÿè®¡:")
        print(f"  âœ… æˆåŠŸæ›´æ–°å“ç§: {len(self.update_stats['updated_varieties'])} ä¸ª")
        print(f"  ğŸ†• æ–°å¢å“ç§: {len(self.update_stats['new_varieties'])} ä¸ª")
        print(f"  âŒ å¤±è´¥å“ç§: {len(self.update_stats['failed_varieties'])} ä¸ª")
        print(f"  â­ï¸ è·³è¿‡å“ç§: {len(self.update_stats['skipped_varieties'])} ä¸ª")
        print(f"  ğŸ“ˆ æ–°å¢è®°å½•æ€»æ•°: {self.update_stats['total_new_records']} æ¡")
        print(f"  â±ï¸ è€—æ—¶: {(self.update_stats['end_time'] - self.update_stats['start_time']).total_seconds():.1f} ç§’")
        
        if self.update_stats["failed_varieties"]:
            print(f"  âš ï¸ å¤±è´¥å“ç§åˆ—è¡¨: {', '.join(self.update_stats['failed_varieties'])}")
        
        return self.update_stats

def main():
    """æµ‹è¯•ä¸»å‡½æ•°"""
    updater = PositioningDataUpdater()
    
    # è·å–ç°æœ‰æ•°æ®çŠ¶æ€
    varieties, info = updater.get_existing_data_status()
    
    # æ¨¡æ‹Ÿæ›´æ–°å‰3ä¸ªå“ç§åˆ°ä»Šå¤©
    target_date = datetime.now().strftime('%Y-%m-%d')
    test_varieties = ['RB', 'CU', 'AL'] if varieties else None
    
    result = updater.update_to_date(target_date, lookback_days=3, specific_varieties=test_varieties)
    
    print(f"\nğŸ¯ æ›´æ–°ç»“æœ: {result}")

if __name__ == "__main__":
    main()
