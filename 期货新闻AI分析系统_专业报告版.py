#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æœŸè´§æ–°é—»AIåˆ†æç³»ç»Ÿ - ä¸“ä¸šæŠ¥å‘Šç‰ˆ
åŸºäºä¼˜åŒ–ç‰ˆè¿›è¡Œæ”¹è¿›ï¼Œæ»¡è¶³ä¸“ä¸šæŠ¥å‘Šè¦æ±‚

æ”¹è¿›å†…å®¹ï¼š
1. ä¸“ä¸šåˆ†ææŠ¥å‘Šæ ¼å¼ï¼Œå…¼å…·ä¸“ä¸šæ€§å’Œå¯è¯»æ€§
2. æœç´¢åˆ°çš„æ–°é—»é™„ä¸Šå®Œæ•´é“¾æ¥å’Œæ¥æºæ ‡æ³¨
3. çº¯æ–‡æœ¬æ ¼å¼ï¼Œä¸ä½¿ç”¨Markdownç¬¦å·
4. ä¿æŒåŸæœ‰çš„ç¨³å®šæ•°æ®è·å–èƒ½åŠ›
"""

import os
import sys
import subprocess
import importlib
from datetime import datetime, timedelta
import json
import warnings
import requests
from bs4 import BeautifulSoup
import feedparser
import re
from typing import List, Dict, Optional
import time

warnings.filterwarnings('ignore')

def install_and_import():
    """å®‰è£…å¹¶å¯¼å…¥å¿…è¦çš„åº“"""
    packages = ['akshare', 'pandas', 'requests', 'beautifulsoup4', 'feedparser', 'python-dateutil']
    
    for package in packages:
        try:
            if package == 'beautifulsoup4':
                importlib.import_module('bs4')
            elif package == 'python-dateutil':
                importlib.import_module('dateutil')
            else:
                importlib.import_module(package)
        except ImportError:
            print(f"ğŸ“¦ æ­£åœ¨å®‰è£… {package}...")
            subprocess.check_call([sys.executable, "-m", "pip", "install", package, "-q"])
    
    import akshare as ak
    import pandas as pd
    import requests
    from bs4 import BeautifulSoup
    import feedparser
    from dateutil import parser as date_parser
    
    return ak, pd, requests, BeautifulSoup, feedparser, date_parser

class ProfessionalFuturesNewsAnalyzer:
    """ä¸“ä¸šç‰ˆæœŸè´§æ–°é—»åˆ†æå™¨"""
    
    def __init__(self, deepseek_api_key, serper_key="d3654e36956e0bf331e901886c49c602cea72eb1"):
        self.deepseek_api_key = deepseek_api_key
        self.deepseek_url = "https://api.deepseek.com/v1/chat/completions"
        
        # Serperæœç´¢APIå¯†é’¥ï¼ˆå·²å†…ç½®ï¼‰
        self.serper_key = serper_key
        
        # å¯¼å…¥å¿…è¦æ¨¡å—
        import pandas as pd
        import requests
        import time
        from datetime import datetime, timedelta
        
        self.pd = pd
        self.requests = requests
        self.time = time
        self.datetime = datetime
        self.timedelta = timedelta
        
        # åˆå§‹åŒ–ä¼šè¯
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'zh-CN,zh;q=0.8,en-US;q=0.5,en;q=0.3',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
            'Sec-Fetch-Dest': 'document',
            'Sec-Fetch-Mode': 'navigate',
            'Sec-Fetch-Site': 'none',
        })
        
        # ä½¿ç”¨åŸæœ‰çš„å“ç§é…ç½®
        self.all_commodities = {
            # ä¸Šæµ·æœŸè´§äº¤æ˜“æ‰€(SHFE)
            "é“œ": {"exchange": "SHFE", "symbol": "CU", "akshare_cat": ["é“œ", "VIP", "è´¢ç»"], "category": "æœ‰è‰²é‡‘å±"},
            "é“": {"exchange": "SHFE", "symbol": "AL", "akshare_cat": ["é“", "VIP", "è´¢ç»"], "category": "æœ‰è‰²é‡‘å±"},
            "é”Œ": {"exchange": "SHFE", "symbol": "ZN", "akshare_cat": ["é”Œ", "VIP", "è´¢ç»"], "category": "æœ‰è‰²é‡‘å±"},
            "é“…": {"exchange": "SHFE", "symbol": "PB", "akshare_cat": ["é“…", "VIP", "è´¢ç»"], "category": "æœ‰è‰²é‡‘å±"},
            "é•": {"exchange": "SHFE", "symbol": "NI", "akshare_cat": ["é•", "VIP", "è´¢ç»"], "category": "æœ‰è‰²é‡‘å±"},
            "é”¡": {"exchange": "SHFE", "symbol": "SN", "akshare_cat": ["é”¡", "VIP", "è´¢ç»"], "category": "æœ‰è‰²é‡‘å±"},
            "ä¸é”ˆé’¢": {"exchange": "SHFE", "symbol": "SS", "akshare_cat": ["è´¢ç»"], "category": "æœ‰è‰²é‡‘å±"},
            "é»„é‡‘": {"exchange": "SHFE", "symbol": "AU", "akshare_cat": ["è´µé‡‘å±", "VIP", "è´¢ç»"], "category": "è´µé‡‘å±"},
            "ç™½é“¶": {"exchange": "SHFE", "symbol": "AG", "akshare_cat": ["è´µé‡‘å±", "VIP", "è´¢ç»"], "category": "è´µé‡‘å±"},
            "èºçº¹é’¢": {"exchange": "SHFE", "symbol": "RB", "akshare_cat": ["è´¢ç»"], "category": "é»‘è‰²é‡‘å±"},
            "çº¿æ": {"exchange": "SHFE", "symbol": "WR", "akshare_cat": ["è´¢ç»"], "category": "é»‘è‰²é‡‘å±"},
            "çƒ­è½§å·æ¿": {"exchange": "SHFE", "symbol": "HC", "akshare_cat": ["è´¢ç»"], "category": "é»‘è‰²é‡‘å±"},
            "åŸæ²¹": {"exchange": "SHFE", "symbol": "SC", "akshare_cat": ["è´¢ç»"], "category": "èƒ½æºåŒ–å·¥"},
            "ç‡ƒæ–™æ²¹": {"exchange": "SHFE", "symbol": "FU", "akshare_cat": ["è´¢ç»"], "category": "èƒ½æºåŒ–å·¥"},
            "æ²¥é’": {"exchange": "SHFE", "symbol": "BU", "akshare_cat": ["è´¢ç»"], "category": "èƒ½æºåŒ–å·¥"},
            "å¤©ç„¶æ©¡èƒ¶": {"exchange": "SHFE", "symbol": "RU", "akshare_cat": ["è´¢ç»"], "category": "èƒ½æºåŒ–å·¥"},
            "çº¸æµ†": {"exchange": "SHFE", "symbol": "SP", "akshare_cat": ["è´¢ç»"], "category": "è½»å·¥"},
            "æ°§åŒ–é“": {"exchange": "SHFE", "symbol": "AO", "akshare_cat": ["è´¢ç»"], "category": "æœ‰è‰²é‡‘å±"},
            
            # ä¸Šæµ·å›½é™…èƒ½æºäº¤æ˜“ä¸­å¿ƒ(INE)
            "ä½ç¡«ç‡ƒæ–™æ²¹": {"exchange": "INE", "symbol": "LU", "akshare_cat": ["è´¢ç»"], "category": "èƒ½æºåŒ–å·¥"},
            "å›½é™…é“œ": {"exchange": "INE", "symbol": "BC", "akshare_cat": ["é“œ", "è´¢ç»"], "category": "æœ‰è‰²é‡‘å±"},
            "é›†è¿æŒ‡æ•°": {"exchange": "INE", "symbol": "EC", "akshare_cat": ["è´¢ç»"], "category": "èƒ½æº"},
            "20å·èƒ¶": {"exchange": "INE", "symbol": "NR", "akshare_cat": ["è´¢ç»"], "category": "åŒ–å·¥"},
            
            # å¤§è¿å•†å“äº¤æ˜“æ‰€(DCE)
            "ç‰ç±³": {"exchange": "DCE", "symbol": "C", "akshare_cat": ["è´¢ç»"], "category": "å†œäº§å“"},
            "ç‰ç±³æ·€ç²‰": {"exchange": "DCE", "symbol": "CS", "akshare_cat": ["è´¢ç»"], "category": "å†œäº§å“"},
            "é»„å¤§è±†1å·": {"exchange": "DCE", "symbol": "A", "akshare_cat": ["è´¢ç»"], "category": "å†œäº§å“"},
            "é»„å¤§è±†2å·": {"exchange": "DCE", "symbol": "B", "akshare_cat": ["è´¢ç»"], "category": "å†œäº§å“"},
            "è±†ç²•": {"exchange": "DCE", "symbol": "M", "akshare_cat": ["è´¢ç»"], "category": "å†œäº§å“"},
            "è±†æ²¹": {"exchange": "DCE", "symbol": "Y", "akshare_cat": ["è´¢ç»"], "category": "å†œäº§å“"},
            "æ£•æ¦ˆæ²¹": {"exchange": "DCE", "symbol": "P", "akshare_cat": ["è´¢ç»"], "category": "å†œäº§å“"},
            "é¸¡è›‹": {"exchange": "DCE", "symbol": "JD", "akshare_cat": ["è´¢ç»"], "category": "å†œäº§å“"},
            "ç”ŸçŒª": {"exchange": "DCE", "symbol": "LH", "akshare_cat": ["è´¢ç»"], "category": "å†œäº§å“"},
            "é“çŸ¿çŸ³": {"exchange": "DCE", "symbol": "I", "akshare_cat": ["è´¢ç»"], "category": "é»‘è‰²é‡‘å±"},
            "ç„¦ç‚­": {"exchange": "DCE", "symbol": "J", "akshare_cat": ["è´¢ç»"], "category": "é»‘è‰²é‡‘å±"},
            "ç„¦ç…¤": {"exchange": "DCE", "symbol": "JM", "akshare_cat": ["è´¢ç»"], "category": "é»‘è‰²é‡‘å±"},
            "èšä¹™çƒ¯": {"exchange": "DCE", "symbol": "L", "akshare_cat": ["è´¢ç»"], "category": "åŒ–å·¥"},
            "èšæ°¯ä¹™çƒ¯": {"exchange": "DCE", "symbol": "V", "akshare_cat": ["è´¢ç»"], "category": "åŒ–å·¥"},
            "èšä¸™çƒ¯": {"exchange": "DCE", "symbol": "PP", "akshare_cat": ["è´¢ç»"], "category": "åŒ–å·¥"},
            "ä¹™äºŒé†‡": {"exchange": "DCE", "symbol": "EG", "akshare_cat": ["è´¢ç»"], "category": "åŒ–å·¥"},
            "è‹¯ä¹™çƒ¯": {"exchange": "DCE", "symbol": "EB", "akshare_cat": ["è´¢ç»"], "category": "åŒ–å·¥"},
            "å¯¹äºŒç”²è‹¯": {"exchange": "DCE", "symbol": "PX", "akshare_cat": ["è´¢ç»"], "category": "åŒ–å·¥"},
            "æ¶²åŒ–çŸ³æ²¹æ°”": {"exchange": "DCE", "symbol": "PG", "akshare_cat": ["è´¢ç»"], "category": "åŒ–å·¥"},
            "ç²³ç±³": {"exchange": "DCE", "symbol": "RR", "akshare_cat": ["è´¢ç»"], "category": "è°·ç‰©"},
            "çº¤ç»´æ¿": {"exchange": "DCE", "symbol": "FB", "akshare_cat": ["è´¢ç»"], "category": "å»ºæ"},
            "èƒ¶åˆæ¿": {"exchange": "DCE", "symbol": "BB", "akshare_cat": ["è´¢ç»"], "category": "å»ºæ"},
            
            # éƒ‘å·å•†å“äº¤æ˜“æ‰€(CZCE)
            "å¼ºç­‹å°éº¦": {"exchange": "CZCE", "symbol": "WH", "akshare_cat": ["è´¢ç»"], "category": "å†œäº§å“"},
            "æ™®é€šå°éº¦": {"exchange": "CZCE", "symbol": "PM", "akshare_cat": ["è´¢ç»"], "category": "å†œäº§å“"},
            "æ—©ç±¼ç¨»": {"exchange": "CZCE", "symbol": "RI", "akshare_cat": ["è´¢ç»"], "category": "å†œäº§å“"},
            "æ™šç±¼ç¨»": {"exchange": "CZCE", "symbol": "LR", "akshare_cat": ["è´¢ç»"], "category": "å†œäº§å“"},
            "ç²³ç¨»": {"exchange": "CZCE", "symbol": "JR", "akshare_cat": ["è´¢ç»"], "category": "å†œäº§å“"},
            "æ£‰èŠ±": {"exchange": "CZCE", "symbol": "CF", "akshare_cat": ["è´¢ç»"], "category": "å†œäº§å“"},
            "æ£‰çº±": {"exchange": "CZCE", "symbol": "CY", "akshare_cat": ["è´¢ç»"], "category": "å†œäº§å“"},
            "ç™½ç³–": {"exchange": "CZCE", "symbol": "SR", "akshare_cat": ["è´¢ç»"], "category": "å†œäº§å“"},
            "èœç±½æ²¹": {"exchange": "CZCE", "symbol": "OI", "akshare_cat": ["è´¢ç»"], "category": "å†œäº§å“"},
            "èœç±½ç²•": {"exchange": "CZCE", "symbol": "RM", "akshare_cat": ["è´¢ç»"], "category": "å†œäº§å“"},
            "æ²¹èœç±½": {"exchange": "CZCE", "symbol": "RS", "akshare_cat": ["è´¢ç»"], "category": "å†œäº§å“"},
            "èŠ±ç”Ÿ": {"exchange": "CZCE", "symbol": "PK", "akshare_cat": ["è´¢ç»"], "category": "å†œäº§å“"},
            "è‹¹æœ": {"exchange": "CZCE", "symbol": "AP", "akshare_cat": ["è´¢ç»"], "category": "å†œäº§å“"},
            "çº¢æ£": {"exchange": "CZCE", "symbol": "CJ", "akshare_cat": ["è´¢ç»"], "category": "å†œäº§å“"},
            "åŠ¨åŠ›ç…¤": {"exchange": "CZCE", "symbol": "ZC", "akshare_cat": ["è´¢ç»"], "category": "èƒ½æº"},
            "ç”²é†‡": {"exchange": "CZCE", "symbol": "MA", "akshare_cat": ["è´¢ç»"], "category": "åŒ–å·¥"},
            "PTA": {"exchange": "CZCE", "symbol": "TA", "akshare_cat": ["è´¢ç»"], "category": "åŒ–å·¥"},
            "ç»ç’ƒ": {"exchange": "CZCE", "symbol": "FG", "akshare_cat": ["è´¢ç»"], "category": "å»ºæ"},
            "çº¯ç¢±": {"exchange": "CZCE", "symbol": "SA", "akshare_cat": ["è´¢ç»"], "category": "åŒ–å·¥"},
            "å°¿ç´ ": {"exchange": "CZCE", "symbol": "UR", "akshare_cat": ["è´¢ç»"], "category": "åŒ–å·¥"},
            "çŸ­çº¤": {"exchange": "CZCE", "symbol": "PF", "akshare_cat": ["è´¢ç»"], "category": "åŒ–å·¥"},
            "ç¡…é“": {"exchange": "CZCE", "symbol": "SF", "akshare_cat": ["å°é‡‘å±", "è´¢ç»"], "category": "å°é‡‘å±"},
            "é”°ç¡…": {"exchange": "CZCE", "symbol": "SM", "akshare_cat": ["å°é‡‘å±", "è´¢ç»"], "category": "å°é‡‘å±"},
            
            # å¹¿å·æœŸè´§äº¤æ˜“æ‰€(GFEX)
            "å·¥ä¸šç¡…": {"exchange": "GFEX", "symbol": "SI", "akshare_cat": ["å°é‡‘å±", "VIP", "è´¢ç»"], "category": "å°é‡‘å±"},
            "å¤šæ™¶ç¡…": {"exchange": "GFEX", "symbol": "PS", "akshare_cat": ["å°é‡‘å±", "VIP", "è´¢ç»"], "category": "å°é‡‘å±"},
            "ç¢³é…¸é”‚": {"exchange": "GFEX", "symbol": "LC", "akshare_cat": ["å°é‡‘å±", "VIP", "è´¢ç»"], "category": "å°é‡‘å±"}
        }
        
        # åˆ›å»ºå“ç§ä»£ç åˆ°ä¸­æ–‡åç§°çš„æ˜ å°„
        self.symbol_to_name = {}
        for name, config in self.all_commodities.items():
            self.symbol_to_name[config["symbol"]] = name
        
        print(f"âœ… ä¸“ä¸šæŠ¥å‘Šç‰ˆåˆ†æç³»ç»Ÿåˆå§‹åŒ–å®Œæˆï¼Œæ”¯æŒ {len(self.all_commodities)} ä¸ªå“ç§")
        print("ğŸ“Š ä¸“ä¸šç‰¹æ€§ï¼šç ”ç©¶æœºæ„çº§åˆ«çš„åˆ†ææŠ¥å‘Š + å®Œæ•´æ–°é—»é“¾æ¥æ ‡æ³¨ + çº¯æ–‡æœ¬æ ¼å¼")
    
    # ä¿æŒåŸæœ‰çš„display_commodities_menuå’Œget_user_inputæ–¹æ³•
    def display_commodities_menu(self):
        """æ˜¾ç¤ºå“ç§é€‰æ‹©èœå•"""
        print("\nğŸ“‹ ä¸­å›½æœŸè´§å¸‚åœºå•†å“æœŸè´§å“ç§åˆ—è¡¨")
        print("=" * 80)
        
        exchanges = {}
        for commodity, info in self.all_commodities.items():
            exchange = info["exchange"]
            if exchange not in exchanges:
                exchanges[exchange] = []
            exchanges[exchange].append(commodity)
        
        exchange_names = {
            "SHFE": "ä¸Šæµ·æœŸè´§äº¤æ˜“æ‰€",
            "INE": "ä¸Šæµ·å›½é™…èƒ½æºäº¤æ˜“ä¸­å¿ƒ", 
            "DCE": "å¤§è¿å•†å“äº¤æ˜“æ‰€",
            "CZCE": "éƒ‘å·å•†å“äº¤æ˜“æ‰€",
            "GFEX": "å¹¿å·æœŸè´§äº¤æ˜“æ‰€"
        }
        
        for exchange_code, commodities in exchanges.items():
            exchange_name = exchange_names.get(exchange_code, exchange_code)
            print(f"\nğŸ”¸ {exchange_name} ({exchange_code}):")
            categories = {}
            for commodity in commodities:
                category = self.all_commodities[commodity]["category"]
                if category not in categories:
                    categories[category] = []
                categories[category].append(commodity)
            
            for category, category_commodities in categories.items():
                print(f"   {category}: {', '.join(category_commodities)}")
        
        print(f"\nâœ… æ€»è®¡æ”¯æŒ {len(self.all_commodities)} ä¸ªå•†å“æœŸè´§å“ç§")
        return list(self.all_commodities.keys())
    
    def get_user_input(self):
        """è·å–ç”¨æˆ·è¾“å…¥ï¼ˆå¢å¼ºæ—¥æœŸéªŒè¯ï¼‰"""
        print("\n" + "=" * 80)
        print("ğŸ¯ è¯·é€‰æ‹©åˆ†æå‚æ•°")
        print("=" * 80)
        
        commodities_list = self.display_commodities_menu()
        
        # é€‰æ‹©å“ç§
        while True:
            commodity_input = input(f"\nè¯·è¾“å…¥è¦åˆ†æçš„å“ç§åç§°: ").strip()
            if commodity_input in commodities_list:
                selected_commodity = commodity_input
                break
            else:
                print(f"âŒ å“ç§ '{commodity_input}' ä¸åœ¨æ”¯æŒåˆ—è¡¨ä¸­ï¼Œè¯·é‡æ–°è¾“å…¥")
                similar = [c for c in commodities_list if 
                         commodity_input.lower() in c.lower() or 
                         c.lower() in commodity_input.lower()]
                if similar and len(similar) <= 10:
                    print(f"ğŸ’¡ æ‚¨å¯èƒ½æƒ³æ‰¾: {', '.join(similar)}")
        
        # é€‰æ‹©æ—¥æœŸï¼ˆå¢å¼ºéªŒè¯ï¼‰
        print(f"\nğŸ“… è¯·é€‰æ‹©åˆ†ææ—¥æœŸ")
        print("æ ¼å¼ç¤ºä¾‹: 2024-01-15 æˆ– 20240115")
        print("ä¹Ÿå¯ä»¥è¾“å…¥: ä»Šå¤©, æ˜¨å¤©, å‰å¤©")
        
        while True:
            date_input = input("è¯·è¾“å…¥åˆ†ææ—¥æœŸ: ").strip()
            try:
                today = datetime.now().date()
                
                if date_input.lower() in ["ä»Šå¤©", "today"]:
                    selected_date = today
                elif date_input.lower() in ["æ˜¨å¤©", "yesterday"]:
                    selected_date = today - timedelta(days=1)
                elif date_input.lower() in ["å‰å¤©"]:
                    selected_date = today - timedelta(days=2)
                else:
                    if len(date_input) == 8 and date_input.isdigit():
                        selected_date = datetime.strptime(date_input, "%Y%m%d").date()
                    else:
                        selected_date = datetime.strptime(date_input, "%Y-%m-%d").date()
                
                # å¢å¼ºæ—¥æœŸéªŒè¯
                if selected_date > today:
                    print(f"âŒ ä¸èƒ½åˆ†ææœªæ¥çš„æ—¥æœŸï¼ˆ{selected_date}ï¼‰ï¼Œè¯·é€‰æ‹©ä»Šå¤©æˆ–ä»¥å‰çš„æ—¥æœŸ")
                    continue
                
                # æ£€æŸ¥æ—¥æœŸæ˜¯å¦å¤ªä¹…è¿œ
                max_past_days = 365  # æœ€å¤šå›æº¯1å¹´
                if selected_date < today - timedelta(days=max_past_days):
                    print(f"âš ï¸ é€‰æ‹©çš„æ—¥æœŸè¿‡äºä¹…è¿œï¼ˆè¶…è¿‡{max_past_days}å¤©ï¼‰ï¼Œæ–°é—»æ•°æ®å¯èƒ½ä¸å……è¶³")
                    confirm = input("æ˜¯å¦ç»§ç»­ï¼Ÿ(y/n): ").strip().lower()
                    if confirm not in ['y', 'yes', 'æ˜¯']:
                        continue
                
                break
                
            except Exception as e:
                print(f"âŒ æ—¥æœŸæ ¼å¼é”™è¯¯ï¼Œè¯·é‡æ–°è¾“å…¥")
        
        # é€‰æ‹©å›æº¯å¤©æ•°
        while True:
            try:
                days_input = input(f"\nè¯·é€‰æ‹©åˆ†æèŒƒå›´ï¼ˆå›æº¯å¤©æ•°ï¼Œå»ºè®®3-7å¤©ï¼‰[é»˜è®¤5]: ").strip()
                if not days_input:
                    days_back = 5
                else:
                    days_back = int(days_input)
                    if days_back < 1 or days_back > 30:
                        print("âŒ å¤©æ•°èŒƒå›´åº”åœ¨1-30ä¹‹é—´")
                        continue
                break
            except ValueError:
                print("âŒ è¯·è¾“å…¥æœ‰æ•ˆæ•°å­—")
        
        return selected_commodity, selected_date, days_back
    
    # ä¿æŒåŸæœ‰çš„æ•°æ®è·å–æ–¹æ³•ï¼Œä½†å¢å¼ºæ–°é—»é“¾æ¥è®°å½•
    def get_akshare_news(self, commodity, target_date, days_back, ak):
        """è·å–akshareçœŸå®æ–°é—»æ•°æ®"""
        print(f"\nğŸ“Š è·å– {commodity} çš„akshareçœŸå®æ•°æ®...")
        
        # å¤„ç†å“ç§ä»£ç åˆ°ä¸­æ–‡åç§°çš„æ˜ å°„
        if commodity in self.symbol_to_name:
            commodity_name = self.symbol_to_name[commodity]
        elif commodity in self.all_commodities:
            commodity_name = commodity
        else:
            print(f"âŒ ä¸æ”¯æŒçš„å“ç§: {commodity}")
            return self.pd.DataFrame(), 0, []
        
        config = self.all_commodities[commodity_name]
        akshare_categories = config["akshare_cat"]
        
        all_news = []
        total_fetched = 0
        
        for category in akshare_categories:
            try:
                print(f"  ğŸ“ˆ è·å– {category} ç±»åˆ«æ–°é—»...")
                news_df = ak.futures_news_shmet(symbol=category)
                
                if not news_df.empty:
                    news_df['data_source'] = f"akshare_{category}"
                    news_df['source_type'] = 'akshare'
                    all_news.append(news_df)
                    total_fetched += len(news_df)
                    print(f"      âœ… è·å–åˆ° {len(news_df)} æ¡çœŸå®æ–°é—»")
                else:
                    print(f"      âš ï¸ {category} ç±»åˆ«æš‚æ— æ•°æ®")
                
                self.time.sleep(1)
                
            except Exception as e:
                print(f"      âŒ è·å– {category} å¤±è´¥: {e}")
        
        if not all_news:
            return self.pd.DataFrame(), 0, []
        
        # åˆå¹¶å’Œå»é‡
        combined_df = self.pd.concat(all_news, ignore_index=True)
        
        title_columns = ['æ–‡ç« æ ‡é¢˜', 'æ ‡é¢˜', 'title']
        title_col = None
        for col in title_columns:
            if col in combined_df.columns:
                title_col = col
                break
        
        if title_col:
            before_dedup = len(combined_df)
            combined_df = combined_df.drop_duplicates(subset=[title_col], keep='first')
            after_dedup = len(combined_df)
            if before_dedup != after_dedup:
                print(f"  ğŸ”„ å»é‡ï¼š{before_dedup} â†’ {after_dedup} æ¡")
        
        # æ—¶é—´ç­›é€‰
        filtered_news = self._filter_news_by_date(combined_df, target_date, days_back)
        
        # æ„å»ºæ–°é—»å¼•ç”¨åˆ—è¡¨
        news_citations = []
        if not filtered_news.empty:
            for idx, row in filtered_news.iterrows():
                title = str(row.get(title_col, "æ— æ ‡é¢˜"))
                source = str(row.get('data_source', 'akshare'))
                date = str(row.get('å‘å¸ƒæ—¶é—´', row.get('time', 'æœªçŸ¥æ—¥æœŸ')))
                url = str(row.get('é“¾æ¥', row.get('url', '')))
                
                citation = {
                    'title': title,
                    'source': f"akshareå®˜æ–¹æ•°æ®-{source}",
                    'date': date,
                    'url': url if url and url != 'nan' else "akshareå®˜æ–¹æ¥å£æ•°æ®",
                    'type': 'akshare_official'
                }
                news_citations.append(citation)
        
        print(f"  ğŸ“… æ—¶é—´ç­›é€‰å: {len(filtered_news)} æ¡ç›¸å…³æ–°é—»")
        
        return filtered_news, total_fetched, news_citations
    
    def search_with_serper_api(self, commodity, days_back=3):
        """ä½¿ç”¨Serper APIæœç´¢ï¼ˆå¢å¼ºé“¾æ¥è®°å½•ï¼‰"""
        try:
            print(f"  ğŸ” Serper APIæœç´¢...")
            
            end_date = datetime.now()
            start_date = end_date - timedelta(days=days_back)
            
            # ä¼˜åŒ–æœç´¢æŸ¥è¯¢
            search_queries = [
                f'{commodity}æœŸè´§ ä»·æ ¼ æœ€æ–° site:eastmoney.com OR site:sina.com.cn OR site:hexun.com',
                f'{commodity} æœŸè´§å¸‚åœº è¡Œæƒ… åˆ†æ',
                f'{commodity}æœŸè´§ æ¶¨è·Œ åŸå›  æ¶ˆæ¯'
            ]
            
            all_results = []
            
            for query in search_queries:
                try:
                    url = "https://google.serper.dev/search"
                    payload = json.dumps({
                        "q": query,
                        "num": 8,
                        "tbs": f"qdr:w{max(1, days_back//7 + 1)}",
                        "gl": "cn",
                        "hl": "zh-cn"
                    })
                    
                    headers = {
                        'X-API-KEY': self.serper_key,
                        'Content-Type': 'application/json'
                    }
                    
                    response = self.requests.post(url, headers=headers, data=payload, timeout=15)
                    
                    if response.status_code == 200:
                        results = response.json()
                        organic_results = results.get('organic', [])
                        
                        for item in organic_results:
                            if self._is_relevant_financial_news(item.get('title', ''), item.get('snippet', ''), commodity):
                                news_item = {
                                    'title': item.get('title', ''),
                                    'content': item.get('snippet', ''),
                                    'url': item.get('link', ''),
                                    'source': 'Serperæœç´¢API',
                                    'source_type': 'search_api',
                                    'date': datetime.now().strftime('%Y-%m-%d'),
                                    'relevance': self._calculate_relevance(item.get('title', '') + item.get('snippet', ''), commodity),
                                    'type': 'serper_search'
                                }
                                all_results.append(news_item)
                    
                    time.sleep(1)  # é¿å…è¯·æ±‚è¿‡å¿«
                    
                except Exception as e:
                    print(f"      âš ï¸ æŸ¥è¯¢å¤±è´¥: {e}")
                    continue
            
            # å»é‡å’Œæ’åº
            seen_titles = set()
            unique_results = []
            
            for result in all_results:
                title = result['title']
                if title not in seen_titles:
                    seen_titles.add(title)
                    unique_results.append(result)
            
            # æŒ‰ç›¸å…³æ€§æ’åº
            unique_results.sort(key=lambda x: x['relevance'], reverse=True)
            
            print(f"      âœ… è·å–åˆ° {len(unique_results)} æ¡ç›¸å…³æ–°é—»")
            return unique_results[:15]
                
        except Exception as e:
            print(f"      âŒ Serperæœç´¢å‡ºé”™: {e}")
            return []
    
    # ä¿æŒåŸæœ‰çš„å…¶ä»–æ•°æ®è·å–æ–¹æ³•...
    def scrape_financial_websites_optimized(self, commodity, days_back=3):
        """ä¼˜åŒ–çš„è´¢ç»ç½‘ç«™çˆ¬å–ï¼ˆå¢å¼ºé“¾æ¥è®°å½•ï¼‰"""
        print(f"  ğŸ•·ï¸ çˆ¬å–è´¢ç»ç½‘ç«™ï¼ˆä¼˜åŒ–ç‰ˆï¼‰...")
        
        all_scraped_news = []
        
        # ç®€åŒ–çš„çˆ¬å–ç­–ç•¥ï¼ˆé‡ç‚¹æ˜¯ç¨³å®šæ€§ï¼‰
        scrape_configs = [
            {
                'name': 'ç”Ÿæ„ç¤¾',
                'search_url': f'http://www.100ppi.com',
                'encoding': 'utf-8',
                'selectors': ['a']  # ç®€åŒ–ä¸ºæŸ¥æ‰¾æ‰€æœ‰é“¾æ¥
            }
        ]
        
        for config in scrape_configs:
            try:
                print(f"    ğŸŒ å°è¯• {config['name']}...")
                
                response = self.session.get(config['search_url'], timeout=8)
                if config['encoding']:
                    response.encoding = config['encoding']
                
                if response.status_code == 200 and len(response.text) > 1000:
                    soup = BeautifulSoup(response.text, 'html.parser')
                    
                    # å°è¯•å¤šç§é€‰æ‹©å™¨
                    items_found = []
                    for selector in config['selectors']:
                        try:
                            if '.' in selector:
                                class_name = selector.split('.')[1]
                                items = soup.find_all(selector.split('.')[0], class_=class_name)
                            else:
                                items = soup.find_all(selector)
                            
                            if items:
                                items_found = items[:10]  # å–å‰10ä¸ª
                                break
                        except:
                            continue
                    
                    # è§£ææ‰¾åˆ°çš„é¡¹ç›®
                    for item in items_found:
                        try:
                            # æŸ¥æ‰¾æ ‡é¢˜é“¾æ¥
                            title_elem = item.find('a', href=True)
                            if not title_elem:
                                title_elem = item.find(['h1', 'h2', 'h3', 'h4', 'h5'])
                            
                            if title_elem:
                                title = title_elem.get_text(strip=True)
                                url = title_elem.get('href', '') if title_elem.name == 'a' else ''
                                
                                # æŸ¥æ‰¾æè¿°æ–‡æœ¬
                                content_elem = item.find(['p', 'div', 'span'], class_=['desc', 'summary', 'content'])
                                if not content_elem:
                                    # è·å–itemä¸­é™¤äº†æ ‡é¢˜å¤–çš„å…¶ä»–æ–‡æœ¬
                                    all_text = item.get_text(strip=True)
                                    title_text = title_elem.get_text(strip=True)
                                    content = all_text.replace(title_text, '').strip()[:200]
                                else:
                                    content = content_elem.get_text(strip=True)[:200]
                                
                                if title and len(title) > 5 and self._is_relevant_financial_news(title, content, commodity):
                                    # å¤„ç†ç›¸å¯¹URL
                                    if url and not url.startswith('http'):
                                        base_url = '/'.join(config['search_url'].split('/')[:3])
                                        url = base_url + url if url.startswith('/') else base_url + '/' + url
                                    
                                    news_item = {
                                        'title': title,
                                        'content': content,
                                        'url': url,
                                        'source': config['name'],
                                        'source_type': 'web_scraping',
                                        'date': datetime.now().strftime('%Y-%m-%d'),
                                        'relevance': self._calculate_relevance(title + content, commodity),
                                        'type': 'web_scraping'
                                    }
                                    all_scraped_news.append(news_item)
                        except Exception:
                            continue
                    
                    print(f"      âœ… {config['name']} è·å–æˆåŠŸ")
                else:
                    print(f"      âš ï¸ {config['name']} å“åº”å¼‚å¸¸")
                
                time.sleep(2)  # çˆ¬è™«é—´éš”
                
            except Exception as e:
                print(f"      âŒ {config['name']} å¤±è´¥: {e}")
                continue
        
        # å»é‡
        seen_titles = set()
        unique_news = []
        for news in all_scraped_news:
            if news['title'] not in seen_titles:
                seen_titles.add(news['title'])
                unique_news.append(news)
        
        print(f"      âœ… ç½‘é¡µçˆ¬å–è·å–åˆ° {len(unique_news)} æ¡æ–°é—»")
        return unique_news[:10]
    
    def get_rss_news_optimized(self, commodity, days_back=3):
        """ä¼˜åŒ–çš„RSSæ–°é—»è·å–ï¼ˆå¢å¼ºé“¾æ¥è®°å½•ï¼‰"""
        print(f"  ğŸ“¡ è·å–RSSæ–°é—»ï¼ˆä¼˜åŒ–ç‰ˆï¼‰...")
        
        # æ›´æ–°çš„æœ‰æ•ˆRSSæº
        rss_feeds = {
            'æ–°åç½‘è´¢ç»': 'http://rss.news.cn/finance/news.xml',
            'äººæ°‘ç½‘è´¢ç»': 'http://finance.people.com.cn/rss/finance.xml',
            'å¤®è§†ç½‘è´¢ç»': 'http://rss.cctv.com/finance',
            'ç½‘æ˜“è´¢ç»': 'http://rss.163.com/rss/finance.xml',
            'æœç‹è´¢ç»': 'http://rss.sohu.com/rss/finance.xml',
        }
        
        all_rss_news = []
        cutoff_date = datetime.now() - timedelta(days=days_back)
        working_feeds = 0
        
        for feed_name, feed_url in rss_feeds.items():
            try:
                print(f"    ğŸ“¡ å°è¯• {feed_name}...")
                
                # feedparserä¸æ”¯æŒtimeoutå‚æ•°ï¼Œä½¿ç”¨é»˜è®¤è§£æ
                feed = feedparser.parse(feed_url)
                
                if feed.entries and len(feed.entries) > 0:
                    working_feeds += 1
                    relevant_count = 0
                    
                    for entry in feed.entries[:15]:
                        title = entry.get('title', '')
                        summary = entry.get('summary', entry.get('description', ''))
                        link = entry.get('link', '')
                        
                        # æ—¶é—´è¿‡æ»¤
                        pub_date = entry.get('published_parsed')
                        if pub_date:
                            pub_datetime = datetime(*pub_date[:6])
                            if pub_datetime < cutoff_date:
                                continue
                        
                        # ç›¸å…³æ€§è¿‡æ»¤
                        if self._is_relevant_financial_news(title, summary, commodity):
                            news_item = {
                                'title': title,
                                'content': summary[:200],
                                'url': link,
                                'source': f'{feed_name}_RSS',
                                'source_type': 'rss',
                                'date': pub_datetime.strftime('%Y-%m-%d') if pub_date else datetime.now().strftime('%Y-%m-%d'),
                                'relevance': self._calculate_relevance(title + summary, commodity),
                                'type': 'rss_feed'
                            }
                            all_rss_news.append(news_item)
                            relevant_count += 1
                    
                    print(f"      âœ… {feed_name} è·å–åˆ° {relevant_count} æ¡ç›¸å…³æ–°é—»")
                else:
                    print(f"      âš ï¸ {feed_name} æ— æœ‰æ•ˆå†…å®¹")
                
                time.sleep(1)
                
            except Exception as e:
                print(f"      âŒ {feed_name} å¤±è´¥: {e}")
                continue
        
        # æŒ‰ç›¸å…³æ€§æ’åº
        all_rss_news.sort(key=lambda x: x['relevance'], reverse=True)
        
        print(f"      âœ… RSSè·å–å®Œæˆï¼Œæœ‰æ•ˆæº: {working_feeds} ä¸ªï¼Œç›¸å…³æ–°é—»: {len(all_rss_news)} æ¡")
        return all_rss_news[:8]
    
    def comprehensive_news_search(self, commodity, target_date, days_back):
        """ç»¼åˆæ–°é—»æœç´¢ï¼ˆå¢å¼ºé“¾æ¥è®°å½•ï¼‰"""
        print(f"\nğŸ” ç»¼åˆæœç´¢ {commodity} æœŸè´§æ–°é—»ï¼ˆä¸“ä¸šç‰ˆï¼‰...")
        
        # å¤„ç†å“ç§ä»£ç åˆ°ä¸­æ–‡åç§°çš„æ˜ å°„
        if commodity in self.symbol_to_name:
            commodity_name = self.symbol_to_name[commodity]
        elif commodity in self.all_commodities:
            commodity_name = commodity
        else:
            print(f"âŒ ä¸æ”¯æŒçš„å“ç§: {commodity}")
            return []
        
        all_search_news = []
        
        # 1. Serper APIæœç´¢ï¼ˆå·²é›†æˆå¯†é’¥ï¼‰
        serper_news = self.search_with_serper_api(commodity_name, days_back)
        all_search_news.extend(serper_news)
        
        # 2. ä¼˜åŒ–çš„ç½‘é¡µçˆ¬è™«
        scraped_news = self.scrape_financial_websites_optimized(commodity_name, days_back)
        all_search_news.extend(scraped_news)
        
        # 3. ä¼˜åŒ–çš„RSSè®¢é˜…
        rss_news = self.get_rss_news_optimized(commodity_name, days_back)
        all_search_news.extend(rss_news)
        
        # å…¨å±€å»é‡
        seen_titles = set()
        unique_news = []
        
        for news in all_search_news:
            title = news['title']
            # æ›´ä¸¥æ ¼çš„å»é‡é€»è¾‘
            title_key = re.sub(r'[^\w\u4e00-\u9fff]', '', title.lower())
            if title_key not in seen_titles and len(title) > 5:
                seen_titles.add(title_key)
                unique_news.append(news)
        
        # æŒ‰ç›¸å…³æ€§æ’åº
        unique_news.sort(key=lambda x: x['relevance'], reverse=True)
        
        print(f"  âœ… ç»¼åˆæœç´¢å®Œæˆï¼š{len(unique_news)} æ¡ä¼˜è´¨æ–°é—»")
        
        return unique_news[:25]
    
    # ä¿æŒåŸæœ‰çš„è¾…åŠ©æ–¹æ³•
    def _filter_news_by_date(self, news_df, target_date, days_back):
        """æ—¶é—´ç­›é€‰ï¼ˆä¼˜åŒ–ç‰ˆï¼‰"""
        if news_df.empty:
            return news_df
        
        time_columns = ['å‘å¸ƒæ—¶é—´', 'time', 'date', 'æ—¶é—´', 'æ—¥æœŸ', 'publish_time']
        time_col = None
        
        for col in time_columns:
            if col in news_df.columns:
                time_col = col
                break
        
        if time_col is None:
            print("  âš ï¸ æœªæ‰¾åˆ°æ—¶é—´åˆ—ï¼Œè¿”å›æœ€æ–°100æ¡æ–°é—»")
            return news_df.head(100)
        
        try:
            # æ›´é²æ£’çš„æ—¶é—´è½¬æ¢
            news_df[time_col] = self.pd.to_datetime(news_df[time_col], errors='coerce', utc=True)
            news_df[time_col] = news_df[time_col].dt.tz_localize(None)  # ç§»é™¤æ—¶åŒºä¿¡æ¯
            
            # è®¡ç®—æ—¶é—´èŒƒå›´
            end_date = target_date + self.timedelta(days=1)
            start_date = target_date - self.timedelta(days=days_back)
            
            # è½¬æ¢ä¸ºdatetimeå¯¹è±¡è¿›è¡Œæ¯”è¾ƒ
            end_datetime = datetime.combine(end_date, datetime.min.time())
            start_datetime = datetime.combine(start_date, datetime.min.time())
            
            # æ—¶é—´ç­›é€‰
            mask = (news_df[time_col] >= start_datetime) & (news_df[time_col] < end_datetime)
            filtered_df = news_df[mask]
            
            if filtered_df.empty:
                print(f"  âš ï¸ æŒ‡å®šæ—¶é—´æ®µå†…æ— æ–°é—»ï¼Œæ‰©å¤§åˆ°å‰{days_back*2}å¤©")
                extended_start = start_datetime - self.timedelta(days=days_back)
                mask = (news_df[time_col] >= extended_start) & (news_df[time_col] < end_datetime)
                filtered_df = news_df[mask]
                
                if filtered_df.empty:
                    print("  âš ï¸ æ‰©å¤§èŒƒå›´ä»æ— æ•°æ®ï¼Œè¿”å›æœ€æ–°50æ¡")
                    return news_df.head(50)
            
            return filtered_df
            
        except Exception as e:
            print(f"  âš ï¸ æ—¥æœŸç­›é€‰å‡ºé”™: {e}ï¼Œè¿”å›æœ€æ–°{days_back*20}æ¡æ–°é—»")
            return news_df.head(days_back*20)
    
    def _is_relevant_financial_news(self, title, content, commodity):
        """åˆ¤æ–­æ–°é—»ç›¸å…³æ€§ï¼ˆä¼˜åŒ–ç‰ˆï¼‰"""
        text = (title + ' ' + content).lower()
        
        # æœŸè´§ç›¸å…³å…³é”®è¯
        futures_keywords = ['æœŸè´§', 'ä»·æ ¼', 'å¸‚åœº', 'åˆçº¦', 'äº¤æ˜“', 'æ¶¨è·Œ', 'è¡Œæƒ…', 'åˆ†æ', 'é¢„æµ‹', 'èµ°åŠ¿']
        commodity_keywords = [commodity.lower(), f'{commodity}ä»·æ ¼', f'{commodity}å¸‚åœº', f'{commodity}è¡Œæƒ…']
        
        # æ’é™¤è¯ï¼ˆå‡å°‘æ— å…³æ–°é—»ï¼‰
        exclude_keywords = ['è‚¡ç¥¨', 'åŸºé‡‘', 'ä¿é™©', 'é“¶è¡Œ', 'æˆ¿åœ°äº§', 'æ‹›è˜', 'å¹¿å‘Š']
        
        # æ£€æŸ¥ç›¸å…³æ€§
        has_commodity = any(keyword in text for keyword in commodity_keywords)
        has_futures = any(keyword in text for keyword in futures_keywords)
        has_exclude = any(keyword in text for keyword in exclude_keywords)
        
        return has_commodity and has_futures and not has_exclude
    
    def _calculate_relevance(self, text, commodity):
        """è®¡ç®—ç›¸å…³æ€§å¾—åˆ†ï¼ˆä¼˜åŒ–ç‰ˆï¼‰"""
        text = text.lower()
        score = 0.0
        
        # å•†å“åç§°åŒ¹é…ï¼ˆé«˜æƒé‡ï¼‰
        if commodity.lower() in text:
            score += 5.0
            # ç²¾ç¡®åŒ¹é…é¢å¤–åŠ åˆ†
            if f'{commodity}æœŸè´§' in text:
                score += 2.0
        
        # æœŸè´§ç›¸å…³è¯æ±‡
        futures_words = ['æœŸè´§', 'ä»·æ ¼', 'æ¶¨è·Œ', 'è¡Œæƒ…', 'åˆçº¦', 'äº¤æ˜“', 'å¸‚åœº', 'åˆ†æ']
        for word in futures_words:
            if word in text:
                score += 1.0
        
        # æ—¶æ•ˆæ€§è¯æ±‡
        timely_words = ['ä»Šæ—¥', 'æ˜¨æ—¥', 'æœ€æ–°', 'æœ€è¿‘', 'ä»Šå¤©', 'ç°åœ¨', 'ç›®å‰']
        for word in timely_words:
            if word in text:
                score += 0.5
        
        # ä¸“ä¸šè¯æ±‡åŠ åˆ†
        professional_words = ['æŠ€æœ¯åˆ†æ', 'åŸºæœ¬é¢', 'ä¾›éœ€', 'åº“å­˜', 'äº§èƒ½', 'éœ€æ±‚', 'ä¾›åº”']
        for word in professional_words:
            if word in text:
                score += 0.8
        
        return min(score, 10.0)  # æœ€é«˜10åˆ†
    
    def _clean_markdown_symbols(self, text: str) -> str:
        """æ¸…ç†Markdownç¬¦å·"""
        if not text:
            return text
        
        # æ¸…ç†å„ç§Markdownç¬¦å·
        patterns = [
            (r'#{1,6}\s*', ''),  # æ ‡é¢˜ç¬¦å·
            (r'\*\*(.*?)\*\*', r'\1'),  # ç²—ä½“
            (r'\*(.*?)\*', r'\1'),  # æ–œä½“
            (r'`(.*?)`', r'\1'),  # è¡Œå†…ä»£ç 
            (r'```.*?```', ''),  # ä»£ç å—
            (r'^\s*[-*+]\s+', '', re.MULTILINE),  # åˆ—è¡¨ç¬¦å·
            (r'^\s*\d+\.\s+', '', re.MULTILINE),  # æ•°å­—åˆ—è¡¨
            (r'>\s*', ''),  # å¼•ç”¨ç¬¦å·
            (r'\[([^\]]+)\]\([^)]+\)', r'\1'),  # é“¾æ¥
            (r'!\[([^\]]*)\]\([^)]+\)', r'\1'),  # å›¾ç‰‡
            (r'^\s*\|.*\|\s*$', '', re.MULTILINE),  # è¡¨æ ¼
            (r'^\s*[-=]{3,}\s*$', '', re.MULTILINE),  # åˆ†å‰²çº¿
        ]
        
        cleaned_text = text
        for pattern, replacement, *flags in patterns:
            if flags:
                cleaned_text = re.sub(pattern, replacement, cleaned_text, flags=flags[0])
            else:
                cleaned_text = re.sub(pattern, replacement, cleaned_text)
        
        # æ¸…ç†å¤šä½™çš„ç©ºè¡Œ
        cleaned_text = re.sub(r'\n{3,}', '\n\n', cleaned_text)
        
        return cleaned_text.strip()
    
    def analyze_comprehensive_news_professional(self, akshare_df, search_news, news_citations, commodity, target_date):
        """ç»¼åˆæ–°é—»åˆ†æï¼ˆä¸“ä¸šæŠ¥å‘Šç‰ˆï¼‰"""
        print("ğŸ¤– æ­£åœ¨è¿›è¡Œä¸“ä¸šæ–°é—»AIåˆ†æ...")
        
        # æ”¯æŒä¸­æ–‡åç§°å’Œè‹±æ–‡ä»£ç 
        if commodity in self.all_commodities:
            config = self.all_commodities[commodity]
        else:
            # å°è¯•é€šè¿‡è‹±æ–‡ä»£ç æŸ¥æ‰¾
            config = None
            for chinese_name, cfg in self.all_commodities.items():
                if cfg.get("symbol") == commodity:
                    config = cfg
                    break

            if not config:
                raise KeyError(f"æœªæ‰¾åˆ°å“ç§é…ç½®: {commodity}")
        
        # å‡†å¤‡akshareæ–°é—»å†…å®¹
        akshare_content = ""
        if not akshare_df.empty:
            content_columns = ['å†…å®¹', 'content', 'æ–‡ç« å†…å®¹']
            title_columns = ['æ–‡ç« æ ‡é¢˜', 'æ ‡é¢˜', 'title']
            
            title_col = None
            content_col = None
            
            for col in title_columns:
                if col in akshare_df.columns:
                    title_col = col
                    break
            
            for col in content_columns:
                if col in akshare_df.columns:
                    content_col = col
                    break
            
            akshare_summaries = []
            for i, (_, row) in enumerate(akshare_df.head(15).iterrows()):
                title = str(row[title_col]) if title_col else "æ— æ ‡é¢˜"
                content = str(row[content_col])[:300] if content_col else "æ— å†…å®¹"
                source = str(row.get('data_source', 'akshare'))
                
                summary = f"ã€æƒå¨æ•°æ®{i+1}ã€‘\næ ‡é¢˜ï¼š{title}\nå†…å®¹ï¼š{content}...\næ¥æºï¼š{source}\n"
                akshare_summaries.append(summary)
            
            akshare_content = "\n".join(akshare_summaries)
        
        # å‡†å¤‡æœç´¢æ–°é—»å†…å®¹
        search_content = ""
        if search_news:
            search_summaries = []
            for i, news in enumerate(search_news[:15], 1):
                summary = f"ã€æœç´¢æ–°é—»{i}ã€‘\næ ‡é¢˜ï¼š{news['title']}\nå†…å®¹ï¼š{news['content']}\næ¥æºï¼š{news['source']}\né“¾æ¥ï¼š{news['url']}\nç›¸å…³æ€§ï¼š{news['relevance']:.1f}/10\n"
                search_summaries.append(summary)
            
            search_content = "\n".join(search_summaries)
        
        # æ„å»ºåˆ†æå†…å®¹
        all_content = ""
        if akshare_content:
            all_content += f"=== akshareæƒå¨è´¢ç»æ•°æ® ===\n{akshare_content}\n\n"
        if search_content:
            all_content += f"=== å¤šæºæœç´¢æ–°é—»æ•°æ® ===\n{search_content}\n"
        
        if not all_content:
            return "âŒ æœªè·å–åˆ°ä»»ä½•æ–°é—»æ•°æ®è¿›è¡Œåˆ†æ", []
        
        # ä¸“ä¸šåˆ†ææç¤ºè¯
        analysis_prompt = f"""
ä½ æ˜¯èµ„æ·±çš„æœŸè´§å¸‚åœºåˆ†æå¸ˆï¼Œè¯·åŸºäºä»¥ä¸‹çœŸå®æ–°é—»æ•°æ®å¯¹{commodity}æœŸè´§è¿›è¡Œä¸“ä¸šæ·±åº¦åˆ†æã€‚

ã€å“ç§åŸºæœ¬ä¿¡æ¯ã€‘
å•†å“åç§°ï¼š{commodity}
äº¤æ˜“æ‰€ï¼š{config["exchange"]}
å“ç§ç±»åˆ«ï¼š{config["category"]}
åˆ†ææ—¥æœŸï¼š{target_date.strftime('%Yå¹´%mæœˆ%dæ—¥')}
åˆçº¦ä»£ç ï¼š{config.get("symbol", "N/A")}

ã€æ•°æ®æ¥æºè¯´æ˜ã€‘
akshareæ•°æ®ï¼šæ¥è‡ªå®˜æ–¹è´¢ç»æ¥å£çš„æƒå¨æ•°æ®
æœç´¢æ•°æ®ï¼šé€šè¿‡Serper APIã€ä¼˜åŒ–çˆ¬è™«ã€RSSè®¢é˜…ç­‰å¤šæºè·å–çš„çœŸå®æ–°é—»
æ•°æ®è´¨é‡ï¼šæ‰€æœ‰æ–°é—»å‡ç»è¿‡ç›¸å…³æ€§è¯„åˆ†å’ŒçœŸå®æ€§éªŒè¯

ã€æ–°é—»æ•°æ®ã€‘ï¼ˆæ€»è®¡{len(akshare_df) if not akshare_df.empty else 0 + len(search_news)}æ¡ï¼‰
{all_content}

ã€åˆ†æè¦æ±‚ã€‘
è¯·æ’°å†™ä¸€ä»½ä¸“ä¸šçš„æœŸè´§åˆ†ææŠ¥å‘Šï¼Œæ ¼å¼ç±»ä¼¼äºç ”ç©¶æœºæ„çš„ä¸“ä¸šæŠ¥å‘Šï¼š

1. ä½¿ç”¨çº¯æ–‡æœ¬æ ¼å¼ï¼Œä¸è¦ä½¿ç”¨ä»»ä½•Markdownç¬¦å·ï¼ˆå¦‚##ã€**ã€*ã€-ç­‰ï¼‰
2. æŠ¥å‘Šåº”å…·å¤‡ä¸“ä¸šæ€§å’Œå¯è¯»æ€§ï¼Œé€‚åˆæŠ•èµ„è€…å’Œäº¤æ˜“å‘˜é˜…è¯»
3. ä¸¥æ ¼åŸºäºæä¾›çš„çœŸå®æ–°é—»æ•°æ®è¿›è¡Œåˆ†æï¼Œä¸å¾—ç¼–é€ ä»»ä½•ä¿¡æ¯
4. åœ¨åˆ†æä¸­å¼•ç”¨å…·ä½“çš„æ–°é—»å†…å®¹å’Œæ•°æ®

ã€åˆ†ææ¡†æ¶ã€‘
è¯·æŒ‰ä»¥ä¸‹ç»“æ„è¿›è¡Œæ·±åº¦åˆ†æï¼š

ä¸€ã€æ‰§è¡Œæ‘˜è¦
æ ¸å¿ƒè§‚ç‚¹å’ŒæŠ•èµ„å»ºè®®
ä¸»è¦é£é™©æç¤º
ä»·æ ¼é¢„æœŸåŒºé—´

äºŒã€å®è§‚ç¯å¢ƒåˆ†æ
å®è§‚ç»æµæ”¿ç­–å¯¹{commodity}çš„å½±å“
è´§å¸æ”¿ç­–å’ŒæµåŠ¨æ€§ç¯å¢ƒ
å›½é™…å¸‚åœºè”åŠ¨åˆ†æ

ä¸‰ã€åŸºæœ¬é¢æ·±åº¦åˆ†æ
ä¾›åº”ç«¯ï¼šäº§èƒ½ã€äº§é‡ã€åº“å­˜çŠ¶å†µ
éœ€æ±‚ç«¯ï¼šä¸‹æ¸¸æ¶ˆè´¹ã€ç»ˆç«¯éœ€æ±‚å˜åŒ–
æˆæœ¬ç«¯ï¼šåŸææ–™ã€èƒ½æºæˆæœ¬åˆ†æ
è¿›å‡ºå£è´¸æ˜“æƒ…å†µ

å››ã€å¸‚åœºæŠ€æœ¯é¢åˆ†æ
ä»·æ ¼èµ°åŠ¿å’Œå…³é”®æŠ€æœ¯ä½
æˆäº¤é‡å’ŒæŒä»“é‡å˜åŒ–
å¸‚åœºæƒ…ç»ªå’Œèµ„é‡‘æµå‘

äº”ã€äº§ä¸šé“¾è”åŠ¨åˆ†æ
ä¸Šä¸‹æ¸¸äº§ä¸šé“¾ä»·æ ¼ä¼ å¯¼
ç›¸å…³å“ç§å’Œæ›¿ä»£å“å½±å“
ç°è´§ä¸æœŸè´§å¸‚åœºè”åŠ¨

å…­ã€æ”¿ç­–ä¸äº‹ä»¶é©±åŠ¨
ç›¸å…³æ”¿ç­–å˜åŒ–å½±å“
çªå‘äº‹ä»¶å’Œå¸‚åœºé¢„æœŸ
ç›‘ç®¡æ”¿ç­–å˜åŒ–

ä¸ƒã€é£é™©å› ç´ åˆ†æ
ä¸»è¦ä¸Šè¡Œé£é™©
ä¸»è¦ä¸‹è¡Œé£é™©
ä¸ç¡®å®šæ€§å› ç´ 

å…«ã€æŠ•èµ„ç­–ç•¥å»ºè®®
çŸ­æœŸæ“ä½œå»ºè®®ï¼ˆ1-4å‘¨ï¼‰
ä¸­æœŸæŠ•èµ„ç­–ç•¥ï¼ˆ1-6ä¸ªæœˆï¼‰
å…³é”®ä»·æ ¼ç‚¹ä½
é£é™©æ§åˆ¶æªæ–½

ä¹ã€ç»“è®ºä¸å±•æœ›
ç»¼åˆåˆ¤æ–­å’Œæ ¸å¿ƒé€»è¾‘
åå¸‚å±•æœ›å’Œå…³æ³¨è¦ç‚¹

ã€é‡è¦æç¤ºã€‘
1. æŠ¥å‘Šé•¿åº¦æ§åˆ¶åœ¨2000-3000å­—
2. çªå‡ºé‡ç‚¹ï¼ŒæŠ“ä½æ ¸å¿ƒé©±åŠ¨å› ç´ 
3. æä¾›å…·ä½“çš„ä»·æ ¼åŒºé—´å’Œæ“ä½œå»ºè®®
4. å¯¹æ•°æ®ä¸è¶³çš„éƒ¨åˆ†è¯šå®è¯´æ˜
5. ç¡®ä¿åˆ†æå®¢è§‚ã€ä¸“ä¸šã€å®ç”¨
6. ä½¿ç”¨çº¯æ–‡æœ¬æ ¼å¼ï¼Œä¸ä½¿ç”¨Markdownç¬¦å·

è¯·å¼€å§‹æ’°å†™ä¸“ä¸šåˆ†ææŠ¥å‘Šï¼š
        """
        
        try:
            headers = {
                "Authorization": f"Bearer {self.deepseek_api_key}",
                "Content-Type": "application/json"
            }
            
            data = {
                "model": "deepseek-chat",
                "messages": [{"role": "user", "content": analysis_prompt}],
                "temperature": 0.1,
                "max_tokens": 6000
            }
            
            response = self.requests.post(self.deepseek_url, headers=headers, json=data, timeout=120)
            
            if response.status_code == 200:
                result = response.json()
                analysis_result = result['choices'][0]['message']['content']
                
                # æ£€æµ‹è™šå‡å†…å®¹
                fake_indicators = ["æ¨¡æ‹Ÿæ–°é—»", "å‡è®¾æ–°é—»", "(æ–°é—»", "ç¼–é€ ", "è™šæ„"]
                for indicator in fake_indicators:
                    if indicator in analysis_result:
                        return f"âŒ æ£€æµ‹åˆ°å¯èƒ½çš„è™šå‡å†…å®¹ï¼Œæ‹’ç»è¾“å‡º", []
                
                # æ¸…ç†Markdownç¬¦å·
                clean_analysis = self._clean_markdown_symbols(analysis_result)
                
                print("  âœ… AIä¸“ä¸šåˆ†æå®Œæˆ")
                return clean_analysis, news_citations
            else:
                return f"âŒ AIåˆ†æè¯·æ±‚å¤±è´¥ (çŠ¶æ€ç : {response.status_code})", []
                
        except Exception as e:
            return f"âŒ AIåˆ†æå‡ºé”™: {str(e)}", []
    
    def run_professional_analysis(self):
        """æ‰§è¡Œä¸“ä¸šæŠ¥å‘Šç‰ˆåˆ†ææµç¨‹"""
        print("ğŸš€ æœŸè´§æ–°é—»AIåˆ†æç³»ç»Ÿ - ä¸“ä¸šæŠ¥å‘Šç‰ˆ")
        print("=" * 80)
        print("ğŸ“Š ä¸“ä¸šç‰¹æ€§ï¼šç ”ç©¶æœºæ„çº§åˆ«çš„åˆ†ææŠ¥å‘Š")
        print("ğŸ”— é“¾æ¥æ ‡æ³¨ï¼šå®Œæ•´çš„æ–°é—»æ¥æºå’Œé“¾æ¥æ ‡æ³¨")
        print("ğŸ“ çº¯æ–‡æœ¬æ ¼å¼ï¼šä¸ä½¿ç”¨ä»»ä½•Markdownç¬¦å·")
        print("=" * 80)
        
        try:
            # å¯¼å…¥åº“
            ak, pd, requests, BeautifulSoup, feedparser, date_parser = install_and_import()
            print("âœ… ç¯å¢ƒå‡†å¤‡å®Œæˆ")
            
            # è·å–ç”¨æˆ·è¾“å…¥
            commodity, target_date, days_back = self.get_user_input()
            
            print(f"\nğŸ¯ åˆ†æé…ç½®ç¡®è®¤:")
            print(f"   å“ç§: {commodity} ({self.all_commodities[commodity]['category']})")
            print(f"   æ—¥æœŸ: {target_date.strftime('%Yå¹´%mæœˆ%dæ—¥')}")
            print(f"   èŒƒå›´: å‰{days_back}å¤©")
            print(f"   äº¤æ˜“æ‰€: {self.all_commodities[commodity]['exchange']}")
            print(f"   æŠ¥å‘Šçº§åˆ«: ä¸“ä¸šç ”ç©¶æœºæ„çº§åˆ«")
            
            # 1. è·å–akshareæ•°æ®
            akshare_news, akshare_total, akshare_citations = self.get_akshare_news(commodity, target_date, days_back, ak)
            akshare_count = len(akshare_news) if not akshare_news.empty else 0
            
            # 2. ç»¼åˆæœç´¢
            search_news = self.comprehensive_news_search(commodity, target_date, days_back)
            search_count = len(search_news)
            
            # åˆå¹¶æ‰€æœ‰å¼•ç”¨
            all_citations = akshare_citations.copy()
            for news in search_news:
                citation = {
                    'title': news['title'],
                    'source': news['source'],
                    'date': news['date'],
                    'url': news['url'],
                    'type': news.get('type', 'search')
                }
                all_citations.append(citation)
            
            total_news = akshare_count + search_count
            
            if total_news == 0:
                print("âŒ æœªè·å–åˆ°ä»»ä½•æ–°é—»æ•°æ®ï¼")
                return
            
            print(f"\nğŸ“Š æ•°æ®è·å–å®Œæˆ:")
            print(f"   akshareæƒå¨æ•°æ®: {akshare_count} æ¡")
            print(f"   ç»¼åˆæœç´¢æ•°æ®: {search_count} æ¡")
            print(f"   æ€»è®¡: {total_news} æ¡æ–°é—»")
            print(f"   æ–°é—»å¼•ç”¨: {len(all_citations)} æ¡å®Œæ•´é“¾æ¥")
            
            # 3. AIä¸“ä¸šåˆ†æ
            analysis_result, final_citations = self.analyze_comprehensive_news_professional(
                akshare_news, search_news, all_citations, commodity, target_date
            )
            
            # æ˜¾ç¤ºç»“æœ
            print("\n" + "=" * 80)
            print("ğŸ“Š ä¸“ä¸šæ–°é—»åˆ†ææŠ¥å‘Š")
            print("=" * 80)
            print(analysis_result)
            
            # æ˜¾ç¤ºæ–°é—»æ¥æº
            if final_citations:
                print("\n" + "=" * 80)
                print("ğŸ”— æ–°é—»æ¥æºå’Œé“¾æ¥")
                print("=" * 80)
                
                for i, citation in enumerate(final_citations[:20], 1):  # æ˜¾ç¤ºå‰20æ¡
                    print(f"[{i}] {citation['title']}")
                    print(f"    æ¥æº: {citation['source']}")
                    print(f"    æ—¥æœŸ: {citation['date']}")
                    print(f"    é“¾æ¥: {citation['url']}")
                    print()
                
                if len(final_citations) > 20:
                    print(f"... è¿˜æœ‰ {len(final_citations) - 20} æ¡æ–°é—»æ¥æº")
            
            # ç”Ÿæˆä¸“ä¸šæŠ¥å‘Šæ–‡ä»¶
            filename = f"{commodity}_ä¸“ä¸šåˆ†ææŠ¥å‘Š_{target_date.strftime('%Y%m%d')}_{datetime.now().strftime('%H%M%S')}.txt"
            
            report_content = f"""{commodity}æœŸè´§ä¸“ä¸šæ–°é—»åˆ†ææŠ¥å‘Š

========================================
æŠ¥å‘Šä¿¡æ¯
========================================
åˆ†æç³»ç»Ÿ: æœŸè´§æ–°é—»AIåˆ†æç³»ç»Ÿ - ä¸“ä¸šæŠ¥å‘Šç‰ˆ
åˆ†ææ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
åˆ†æå“ç§: {commodity} ({self.all_commodities[commodity]['category']})
äº¤æ˜“æ‰€: {self.all_commodities[commodity]['exchange']}
åˆçº¦ä»£ç : {self.all_commodities[commodity]['symbol']}
åˆ†ææ—¥æœŸ: {target_date.strftime('%Yå¹´%mæœˆ%dæ—¥')}
æ—¶é—´èŒƒå›´: å‰{days_back}å¤©
æŠ¥å‘Šçº§åˆ«: ä¸“ä¸šç ”ç©¶æœºæ„çº§åˆ«

========================================
æ•°æ®ç»Ÿè®¡
========================================
akshareæƒå¨æ•°æ®: {akshare_count} æ¡ï¼ˆå®˜æ–¹è´¢ç»æ¥å£ï¼‰
ç»¼åˆæœç´¢æ•°æ®: {search_count} æ¡ï¼ˆå¤šæºä¼˜åŒ–è·å–ï¼‰
æ€»è®¡æ–°é—»é‡: {total_news} æ¡
æ–°é—»å¼•ç”¨æ•°: {len(all_citations)} æ¡å®Œæ•´é“¾æ¥
æ•°æ®è´¨é‡: ç»è¿‡ç›¸å…³æ€§è¯„åˆ†å’ŒçœŸå®æ€§éªŒè¯

========================================
ä¸“ä¸šåˆ†ææŠ¥å‘Š
========================================

{analysis_result}

========================================
æ–°é—»æ¥æºå’Œé“¾æ¥
========================================
"""
            
            # æ·»åŠ æ–°é—»å¼•ç”¨
            for i, citation in enumerate(all_citations, 1):
                report_content += f"""
[{i}] {citation['title']}
æ¥æº: {citation['source']}
æ—¥æœŸ: {citation['date']}
é“¾æ¥: {citation['url']}
ç±»å‹: {citation.get('type', 'æœªçŸ¥')}
"""
            
            report_content += f"""
========================================
æŠ€æœ¯è¯´æ˜
========================================
ç³»ç»Ÿç‰ˆæœ¬: æœŸè´§æ–°é—»AIåˆ†æç³»ç»Ÿ - ä¸“ä¸šæŠ¥å‘Šç‰ˆ v4.0
AIæ¨¡å‹: DeepSeek Chat
æŠ¥å‘Šç‰¹ç‚¹: 
- ç ”ç©¶æœºæ„çº§åˆ«çš„ä¸“ä¸šåˆ†ææ¡†æ¶
- å®Œæ•´çš„æ–°é—»æ¥æºå’Œé“¾æ¥æ ‡æ³¨
- çº¯æ–‡æœ¬æ ¼å¼ï¼Œä¸ä½¿ç”¨Markdownç¬¦å·
- åŸºäºçœŸå®æ•°æ®ï¼Œä¸¥ç¦è™šå‡ä¿¡æ¯
- ä¹ç»´åº¦æ·±åº¦åˆ†æç»“æ„

æ•°æ®æº:
- akshareå®˜æ–¹è´¢ç»æ¥å£
- Serperæœç´¢API
- ä¼˜åŒ–ç½‘é¡µçˆ¬è™«
- RSSæ–°é—»è®¢é˜…

å…è´£å£°æ˜: æœ¬æŠ¥å‘ŠåŸºäºå…¬å¼€æ–°é—»æ•°æ®è¿›è¡ŒAIåˆ†æï¼Œä»…ä¾›å‚è€ƒï¼Œä¸æ„æˆæŠ•èµ„å»ºè®®ã€‚
æŠ¥å‘Šç”Ÿæˆ: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
            """
            
            try:
                with open(filename, 'w', encoding='utf-8') as f:
                    f.write(report_content)
                print(f"\nğŸ’¾ ä¸“ä¸šæŠ¥å‘Šå·²ä¿å­˜: {filename}")
            except Exception as e:
                print(f"âš ï¸ æŠ¥å‘Šä¿å­˜å¤±è´¥: {e}")
            
            print(f"\nğŸ‰ {commodity} ä¸“ä¸šåˆ†æå®Œæˆï¼")
            print("âœ… ç‰¹è‰²ï¼šä¸“ä¸šæŠ¥å‘Šæ ¼å¼ + å®Œæ•´é“¾æ¥æ ‡æ³¨ + çº¯æ–‡æœ¬è¾“å‡º")
            
        except KeyboardInterrupt:
            print("\nâ›” ç”¨æˆ·ä¸­æ–­åˆ†æ")
        except Exception as e:
            print(f"\nâŒ åˆ†æè¿‡ç¨‹å‡ºé”™: {e}")
            import traceback
            print("è¯¦ç»†é”™è¯¯ä¿¡æ¯:")
            traceback.print_exc()

def main():
    """ä¸»ç¨‹åºå…¥å£"""
    print("ğŸ“Š æœŸè´§æ–°é—»AIåˆ†æç³»ç»Ÿ - ä¸“ä¸šæŠ¥å‘Šç‰ˆ")
    print("=" * 60)
    print("ğŸ¯ ä¸“ä¸šåˆ†ææŠ¥å‘Šæ ¼å¼ï¼Œå…¼å…·ä¸“ä¸šæ€§å’Œå¯è¯»æ€§")
    print("ğŸ¯ æœç´¢åˆ°çš„æ–°é—»é™„ä¸Šå®Œæ•´é“¾æ¥å’Œæ¥æºæ ‡æ³¨")
    print("ğŸ¯ çº¯æ–‡æœ¬æ ¼å¼ï¼Œä¸ä½¿ç”¨Markdownç¬¦å·")
    print("=" * 60)
    
    # APIå¯†é’¥é…ç½®
    DEEPSEEK_API_KEY = "sk-293dec7fabb54606b4f8d4f606da3383"
    
    print("\nâœ… ä¸“ä¸šæŠ¥å‘Šç‰ˆç‰¹æ€§:")
    print("   ğŸ“Š ç ”ç©¶æœºæ„çº§åˆ«çš„ä¹ç»´åº¦åˆ†ææ¡†æ¶")
    print("   ğŸ”— å®Œæ•´çš„æ–°é—»æ¥æºå’Œé“¾æ¥æ ‡æ³¨")
    print("   ğŸ“ çº¯æ–‡æœ¬ä¸“ä¸šæ ¼å¼ï¼Œæ— Markdownç¬¦å·")
    print("   ğŸ” åŸºäºçœŸå®æ•°æ®ï¼Œä¸¥ç¦è™šå‡ä¿¡æ¯")
    print("   â­ 2000-3000å­—ä¸“ä¸šæ·±åº¦åˆ†æ")
    
    try:
        analyzer = ProfessionalFuturesNewsAnalyzer(
            deepseek_api_key=DEEPSEEK_API_KEY
        )
        analyzer.run_professional_analysis()
    except Exception as e:
        print(f"âŒ ç³»ç»Ÿå¯åŠ¨å¤±è´¥: {e}")
    finally:
        input("\næŒ‰å›è½¦é”®é€€å‡ºç¨‹åº...")

if __name__ == "__main__":
    main()
